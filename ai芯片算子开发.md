# AIèŠ¯ç‰‡ç®—å­å¼€å‘æ ¸å¿ƒæŠ€æœ¯æ·±åº¦ç ”ç©¶

## ğŸ¯ ç ”ç©¶ç›®æ ‡
æ·±å…¥ç ”ç©¶å››å¤§æ ¸å¿ƒå¼€æºé¡¹ç›®ï¼ŒæŒæ¡å·¥ä¸šçº§ç®—å­å¼€å‘çš„ç²¾é«“ï¼š
1. **ONNXRuntimeæºç ** - ç†è§£å·¥ä¸šçº§ç®—å­ä¼˜åŒ–
2. **Triton** - æŒæ¡ç°ä»£GPUç¼–ç¨‹æ€ç»´
3. **PyTorch ATen** - ç†è§£åŠ¨æ€å›¾ç®—å­å®ç°
4. **TVM** - å­¦ä¹ ç¼–è¯‘å™¨ä¼˜åŒ–æŠ€æœ¯

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šONNXRuntimeæºç æ·±åº¦åˆ†æ

### 1.1 ONNXRuntimeæ¶æ„æ ¸å¿ƒç†è§£

#### æ ¸å¿ƒæ¶æ„ç»„ä»¶
```cpp
// ONNXRuntimeçš„æ ¸å¿ƒæ¶æ„å‰–æ
namespace onnxruntime {

// 1. SessionState - ç®¡ç†æ•´ä¸ªæ¨ç†ä¼šè¯
class SessionState {
private:
    std::unique_ptr<Graph> graph_;                    // è®¡ç®—å›¾
    std::vector<std::unique_ptr<ExecutionProvider>> execution_providers_;  // æ‰§è¡Œæä¾›è€…
    KernelRegistryManager kernel_registry_manager_;  // å†…æ ¸æ³¨å†Œç®¡ç†å™¨
    std::unique_ptr<DataTransferManager> data_transfer_mgr_;  // æ•°æ®ä¼ è¾“ç®¡ç†
    
public:
    // æ ¸å¿ƒæ–¹æ³•ï¼šåˆ›å»ºæ‰§è¡Œè®¡åˆ’
    Status CreateExecutionPlan(const Graph& graph,
                              const std::vector<const NodeArg*>& outer_scope_node_args,
                              const ExecutionOptions& execution_options,
                              std::unique_ptr<SequentialExecutionPlan>& execution_plan);
    
    // è·å–å†…æ ¸åˆ›å»ºä¿¡æ¯
    Status GetKernelCreateInfo(const Node& node, 
                              std::unique_ptr<OpKernelInfo>& kernel_create_info);
};

// 2. ExecutionProvider - æ‰§è¡Œæä¾›è€…åŸºç±»
class IExecutionProvider {
public:
    virtual ~IExecutionProvider() = default;
    
    // æ ¸å¿ƒæ¥å£ï¼šè·å–æ”¯æŒçš„å†…æ ¸æ³¨å†Œ
    virtual std::vector<std::unique_ptr<KernelDef>> GetKernelRegistry() const = 0;
    
    // åˆ†é…å†…å­˜
    virtual std::unique_ptr<onnxruntime::IAllocator> CreatePreferredAllocators() = 0;
    
    // åˆ›å»ºè®¡ç®—æµ
    virtual std::unique_ptr<profiling::EpProfiler> GetProfiler() = 0;
    
    // å›¾çº§åˆ«çš„ä¼˜åŒ–å˜æ¢
    virtual common::Status OnRunStart() { return Status::OK(); }
    virtual common::Status OnRunEnd(bool sync_stream) { return Status::OK(); }
};

// 3. CUDAæ‰§è¡Œæä¾›è€…å®ç°åˆ†æ
class CUDAExecutionProvider : public IExecutionProvider {
private:
    CUDAExecutionProviderInfo info_;
    cudaStream_t stream_;
    cublasHandle_t cublas_handle_;
    cudnnHandle_t cudnn_handle_;
    
public:
    CUDAExecutionProvider(const CUDAExecutionProviderInfo& info) : info_(info) {
        // åˆå§‹åŒ–CUDAèµ„æº
        CUDA_CALL(cudaStreamCreate(&stream_));
        CUBLAS_CALL(cublasCreate(&cublas_handle_));
        CUDNN_CALL(cudnnCreate(&cudnn_handle_));
        
        // è®¾ç½®æµ
        CUBLAS_CALL(cublasSetStream(cublas_handle_, stream_));
        CUDNN_CALL(cudnnSetStream(cudnn_handle_, stream_));
    }
    
    // è·å–CUDAå†…æ ¸æ³¨å†Œ
    std::vector<std::unique_ptr<KernelDef>> GetKernelRegistry() const override {
        std::vector<std::unique_ptr<KernelDef>> kernel_defs;
        
        // æ³¨å†ŒGEMMå†…æ ¸
        kernel_defs.push_back(
            KernelDefBuilder().SetName("MatMul")
                             .Domain(kOnnxDomain)
                             .SinceVersion(1)
                             .Provider(kCudaExecutionProvider)
                             .TypeConstraint("T", DataTypeImpl::GetTensorType<float>())
                             .Build());
        
        // æ³¨å†Œå·ç§¯å†…æ ¸
        kernel_defs.push_back(
            KernelDefBuilder().SetName("Conv")
                             .Domain(kOnnxDomain)
                             .SinceVersion(1)
                             .Provider(kCudaExecutionProvider)
                             .TypeConstraint("T", {DataTypeImpl::GetTensorType<float>(),
                                                 DataTypeImpl::GetTensorType<MLFloat16>()})
                             .Build());
        
        return kernel_defs;
    }
};
}
```

#### ç®—å­å†…æ ¸å®ç°æ·±åº¦åˆ†æ
```cpp
// ONNXRuntimeçš„OpKernelå®ç°æ¨¡å¼æ·±åº¦åˆ†æ

// 1. OpKernelåŸºç±»è®¾è®¡
class OpKernel {
protected:
    OpKernelInfo info_;
    
public:
    explicit OpKernel(const OpKernelInfo& info) : info_(info) {}
    virtual ~OpKernel() = default;
    
    // æ ¸å¿ƒè®¡ç®—æ¥å£
    virtual Status Compute(OpKernelContext* context) const = 0;
    
    // è·å–ç®—å­ä¿¡æ¯
    const OpKernelInfo& Info() const { return info_; }
};

// 2. CUDA MatMulå†…æ ¸å®ç°åˆ†æ
template <typename T>
class MatMul final : public CudaKernel {
private:
    bool trans_A_;
    bool trans_B_;
    float alpha_;
    float beta_;
    
public:
    MatMul(const OpKernelInfo& info) : CudaKernel(info) {
        // è§£æå±æ€§
        int64_t trans_A_int = info.GetAttrOrDefault<int64_t>("transA", 0);
        int64_t trans_B_int = info.GetAttrOrDefault<int64_t>("transB", 0);
        trans_A_ = trans_A_int != 0;
        trans_B_ = trans_B_int != 0;
        alpha_ = info.GetAttrOrDefault<float>("alpha", 1.0f);
        beta_ = info.GetAttrOrDefault<float>("beta", 0.0f);
    }
    
    Status Compute(OpKernelContext* context) const override {
        // è·å–è¾“å…¥å¼ é‡
        const Tensor* A = context->Input<Tensor>(0);
        const Tensor* B = context->Input<Tensor>(1);
        
        // éªŒè¯è¾“å…¥ç»´åº¦
        if (A->Shape().NumDimensions() != 2 || B->Shape().NumDimensions() != 2) {
            return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, "MatMul requires 2D tensors");
        }
        
        // è®¡ç®—è¾“å‡ºç»´åº¦
        auto A_shape = A->Shape();
        auto B_shape = B->Shape();
        
        int64_t M = trans_A_ ? A_shape[1] : A_shape[0];
        int64_t K_A = trans_A_ ? A_shape[0] : A_shape[1];
        int64_t K_B = trans_B_ ? B_shape[1] : B_shape[0];
        int64_t N = trans_B_ ? B_shape[0] : B_shape[1];
        
        if (K_A != K_B) {
            return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, 
                                 "MatMul inner dimensions must match");
        }
        
        // åˆ›å»ºè¾“å‡ºå¼ é‡
        TensorShape output_shape({M, N});
        Tensor* Y = context->Output(0, output_shape);
        
        // è°ƒç”¨ä¼˜åŒ–çš„CUDAå®ç°
        return ComputeGemm(A, B, Y, trans_A_, trans_B_, alpha_, beta_, context);
    }
    
private:
    Status ComputeGemm(const Tensor* A, const Tensor* B, Tensor* Y,
                      bool trans_A, bool trans_B, float alpha, float beta,
                      OpKernelContext* context) const {
        
        // è·å–CUDAæµå’ŒcuBLASå¥æŸ„
        auto& cuda_ep = static_cast<const CUDAExecutionProvider&>(Info().GetExecutionProvider());
        cudaStream_t stream = cuda_ep.GetStream();
        cublasHandle_t cublas_handle = cuda_ep.GetCublasHandle();
        
        // è®¾ç½®cuBLASæµ
        CUBLAS_RETURN_IF_ERROR(cublasSetStream(cublas_handle, stream));
        
        // è·å–æ•°æ®æŒ‡é’ˆ
        const T* a_data = A->Data<T>();
        const T* b_data = B->Data<T>();
        T* y_data = Y->MutableData<T>();
        
        // è·å–ç»´åº¦
        auto A_shape = A->Shape();
        auto B_shape = B->Shape();
        
        int64_t M = trans_A ? A_shape[1] : A_shape[0];
        int64_t K = trans_A ? A_shape[0] : A_shape[1];
        int64_t N = trans_B ? B_shape[0] : B_shape[1];
        
        // è®¡ç®—leading dimensions
        int64_t lda = trans_A ? M : K;
        int64_t ldb = trans_B ? K : N;
        int64_t ldc = N;
        
        // è°ƒç”¨cuBLAS GEMM
        cublasOperation_t op_A = trans_A ? CUBLAS_OP_T : CUBLAS_OP_N;
        cublasOperation_t op_B = trans_B ? CUBLAS_OP_T : CUBLAS_OP_N;
        
        if constexpr (std::is_same_v<T, float>) {
            CUBLAS_RETURN_IF_ERROR(cublasSgemm(
                cublas_handle, op_B, op_A,  // æ³¨æ„ï¼šcuBLASä½¿ç”¨åˆ—ä¸»åº
                N, M, K,
                &alpha,
                b_data, ldb,
                a_data, lda,
                &beta,
                y_data, ldc));
        } else if constexpr (std::is_same_v<T, MLFloat16>) {
            __half alpha_half = __float2half(alpha);
            __half beta_half = __float2half(beta);
            
            CUBLAS_RETURN_IF_ERROR(cublasHgemm(
                cublas_handle, op_B, op_A,
                N, M, K,
                &alpha_half,
                reinterpret_cast<const __half*>(b_data), ldb,
                reinterpret_cast<const __half*>(a_data), lda,
                &beta_half,
                reinterpret_cast<__half*>(y_data), ldc));
        }
        
        return Status::OK();
    }
};

// 3. å†…æ ¸æ³¨å†Œå®åˆ†æ
#define ONNX_OPERATOR_KERNEL_EX(name, domain, since_version, provider, builder, ...)  \
  ONNX_OPERATOR_KERNEL_CLASS_NAME(provider, domain, since_version, name)              \
  ONNX_OPERATOR_KERNEL_BUILD_INFO(provider, domain, since_version, name, builder);   \
  ONNX_OPERATOR_KERNEL_CREATE_INFO(provider, domain, since_version, name, __VA_ARGS__)

// ä½¿ç”¨ç¤ºä¾‹
ONNX_OPERATOR_KERNEL_EX(
    MatMul,                    // ç®—å­åç§°
    kOnnxDomain,              // åŸŸ
    1,                        // ç‰ˆæœ¬
    kCudaExecutionProvider,   // æä¾›è€…
    KernelDefBuilder()        // æ„å»ºå™¨
        .TypeConstraint("T", DataTypeImpl::GetTensorType<float>()),
    MatMul<float>);           // å®ç°ç±»
```

### 1.2 ONNXRuntimeå›¾ä¼˜åŒ–æ¡†æ¶æ·±å…¥

#### å›¾ä¼˜åŒ–å˜æ¢å®ç°
```cpp
// ONNXRuntimeçš„å›¾ä¼˜åŒ–æ¡†æ¶æ·±åº¦åˆ†æ

// 1. GraphTransformeråŸºç±»
class GraphTransformer {
protected:
    std::string name_;
    TransformerLevel level_;
    
public:
    GraphTransformer(const std::string& name, TransformerLevel level)
        : name_(name), level_(level) {}
    
    virtual ~GraphTransformer() = default;
    
    // æ ¸å¿ƒå˜æ¢æ¥å£
    virtual Status ApplyTransform(Graph& graph, bool& modified, 
                                 const NodeIndex& node_index,
                                 const std::logging::Logger& logger) const = 0;
    
    const std::string& Name() const { return name_; }
    TransformerLevel Level() const { return level_; }
};

// 2. ç®—å­èåˆå˜æ¢å™¨å®ç°
class FusionTransformer : public GraphTransformer {
private:
    std::vector<std::unique_ptr<NodeFusion>> fusion_rules_;
    
public:
    FusionTransformer(const std::string& name) 
        : GraphTransformer(name, TransformerLevel::Level2) {
        
        // æ³¨å†Œèåˆè§„åˆ™
        RegisterFusionRules();
    }
    
    Status ApplyTransform(Graph& graph, bool& modified, 
                         const NodeIndex& node_index,
                         const std::logging::Logger& logger) const override {
        
        modified = false;
        auto& nodes = graph.Nodes();
        
        // éå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œå¯»æ‰¾èåˆæœºä¼š
        for (auto& node : nodes) {
            if (node.GetExecutionProviderType() != kCudaExecutionProvider) {
                continue;
            }
            
            // åº”ç”¨èåˆè§„åˆ™
            for (const auto& fusion_rule : fusion_rules_) {
                bool rule_modified = false;
                Status status = fusion_rule->Apply(graph, node, rule_modified, logger);
                
                if (!status.IsOK()) {
                    return status;
                }
                
                if (rule_modified) {
                    modified = true;
                    break;  // ä¸€æ¬¡åªåº”ç”¨ä¸€ä¸ªè§„åˆ™
                }
            }
        }
        
        return Status::OK();
    }
    
private:
    void RegisterFusionRules() {
        // Conv + BatchNorm + ReLUèåˆ
        fusion_rules_.push_back(std::make_unique<ConvBatchNormReLUFusion>());
        
        // MatMul + Addèåˆ  
        fusion_rules_.push_back(std::make_unique<MatMulAddFusion>());
        
        // æ³¨æ„åŠ›æœºåˆ¶èåˆ
        fusion_rules_.push_back(std::make_unique<AttentionFusion>());
    }
};

// 3. Conv+BN+ReLUèåˆè§„åˆ™å®ç°
class ConvBatchNormReLUFusion : public NodeFusion {
public:
    Status Apply(Graph& graph, Node& conv_node, bool& modified,
                const std::logging::Logger& logger) const override {
        
        modified = false;
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯ConvèŠ‚ç‚¹
        if (conv_node.OpType() != "Conv") {
            return Status::OK();
        }
        
        // æŸ¥æ‰¾BatchNormalizationèŠ‚ç‚¹
        Node* bn_node = nullptr;
        if (!FindSingleConsumer(graph, conv_node, "BatchNormalization", bn_node)) {
            return Status::OK();
        }
        
        // æŸ¥æ‰¾ReLUèŠ‚ç‚¹
        Node* relu_node = nullptr;
        if (!FindSingleConsumer(graph, *bn_node, "Relu", relu_node)) {
            return Status::OK();
        }
        
        // æ‰§è¡Œèåˆ
        return FuseConvBatchNormReLU(graph, conv_node, *bn_node, *relu_node, modified, logger);
    }
    
private:
    Status FuseConvBatchNormReLU(Graph& graph, Node& conv_node, Node& bn_node, Node& relu_node,
                                bool& modified, const std::logging::Logger& logger) const {
        
        // è·å–BatchNormå‚æ•°
        const NodeArg* scale_arg = bn_node.InputDefs()[1];
        const NodeArg* bias_arg = bn_node.InputDefs()[2];
        const NodeArg* mean_arg = bn_node.InputDefs()[3];
        const NodeArg* var_arg = bn_node.InputDefs()[4];
        
        // é¢„è®¡ç®—èåˆå‚æ•°
        auto fused_weights = PrecomputeFusedWeights(
            conv_node.InputDefs()[1],  // åŸå§‹æƒé‡
            scale_arg, bias_arg, mean_arg, var_arg);
        
        // åˆ›å»ºèåˆèŠ‚ç‚¹
        std::vector<NodeArg*> fused_inputs = {
            const_cast<NodeArg*>(conv_node.InputDefs()[0]),  // è¾“å…¥
            fused_weights.first,   // èåˆæƒé‡
            fused_weights.second   // èåˆåç½®
        };
        
        std::vector<NodeArg*> fused_outputs = {
            const_cast<NodeArg*>(relu_node.OutputDefs()[0])  // æœ€ç»ˆè¾“å‡º
        };
        
        // æ·»åŠ èåˆèŠ‚ç‚¹
        Node& fused_node = graph.AddNode(
            "FusedConvBatchNormReLU_" + conv_node.Name(),
            "FusedConvBatchNormReLU",
            "Fused Conv + BatchNorm + ReLU operation",
            fused_inputs,
            fused_outputs,
            &conv_node.GetAttributes(),  // ä½¿ç”¨Convçš„å±æ€§
            kCudaExecutionProvider);
        
        // ç§»é™¤åŸå§‹èŠ‚ç‚¹
        graph.RemoveNode(relu_node.Index());
        graph.RemoveNode(bn_node.Index());
        graph.RemoveNode(conv_node.Index());
        
        modified = true;
        
        LOGS(logger, INFO) << "Fused Conv + BatchNorm + ReLU: " 
                          << conv_node.Name() << " -> " << fused_node.Name();
        
        return Status::OK();
    }
    
    std::pair<NodeArg*, NodeArg*> PrecomputeFusedWeights(
        const NodeArg* conv_weight,
        const NodeArg* bn_scale, const NodeArg* bn_bias,
        const NodeArg* bn_mean, const NodeArg* bn_var) const {
        
        // è¿™é‡Œåº”è¯¥å®ç°æƒé‡èåˆçš„æ•°å­¦è®¡ç®—
        // èåˆå…¬å¼: 
        // new_weight = conv_weight * (bn_scale / sqrt(bn_var + epsilon))
        // new_bias = bn_bias - bn_mean * bn_scale / sqrt(bn_var + epsilon)
        
        // å®é™…å®ç°ä¼šæ¶‰åŠå¼ é‡è®¡ç®—ï¼Œè¿™é‡Œç®€åŒ–è¡¨ç¤º
        NodeArg* fused_weight = nullptr;  // è®¡ç®—å¾—åˆ°çš„èåˆæƒé‡
        NodeArg* fused_bias = nullptr;    // è®¡ç®—å¾—åˆ°çš„èåˆåç½®
        
        return {fused_weight, fused_bias};
    }
    
    bool FindSingleConsumer(const Graph& graph, const Node& producer, 
                           const std::string& consumer_op_type, Node*& consumer) const {
        
        const auto& output_edges = producer.GetRelationships().output_edges;
        
        // æ£€æŸ¥æ˜¯å¦åªæœ‰ä¸€ä¸ªæ¶ˆè´¹è€…
        if (output_edges.size() != 1) {
            return false;
        }
        
        Node& potential_consumer = *graph.GetNode(output_edges[0].GetNode().Index());
        
        // æ£€æŸ¥æ“ä½œç±»å‹
        if (potential_consumer.OpType() != consumer_op_type) {
            return false;
        }
        
        consumer = &potential_consumer;
        return true;
    }
};
```

### 1.3 ONNXRuntimeå†…å­˜ç®¡ç†å’Œæ€§èƒ½ä¼˜åŒ–

#### å†…å­˜æ± å’Œåˆ†é…å™¨å®ç°
```cpp
// ONNXRuntimeçš„å†…å­˜ç®¡ç†æ·±åº¦åˆ†æ

// 1. å†…å­˜åˆ†é…å™¨æ¥å£
class IAllocator {
public:
    virtual ~IAllocator() = default;
    
    // æ ¸å¿ƒåˆ†é…æ¥å£
    virtual void* Alloc(size_t size) = 0;
    virtual void Free(void* p) = 0;
    
    // è·å–åˆ†é…å™¨ä¿¡æ¯
    virtual const OrtAllocatorInfo& Info() const = 0;
    
    // é«˜çº§æ¥å£
    virtual void* Reserve(size_t size) { return Alloc(size); }
    virtual void GetStats(AllocatorStats* stats) {}
};

// 2. CUDAå†…å­˜åˆ†é…å™¨å®ç°
class CUDAAllocator : public IAllocator {
private:
    OrtAllocatorInfo info_;
    cudaStream_t stream_;
    
    // å†…å­˜æ± ç®¡ç†
    struct MemoryPool {
        std::map<size_t, std::vector<void*>> free_blocks_;  // æŒ‰å¤§å°åˆ†ç»„çš„ç©ºé—²å—
        std::unordered_map<void*, size_t> allocated_blocks_; // å·²åˆ†é…å—çš„å¤§å°è®°å½•
        std::mutex mutex_;
        size_t total_allocated_ = 0;
        size_t peak_allocated_ = 0;
    };
    
    mutable MemoryPool pool_;
    
public:
    CUDAAllocator(int device_id, cudaStream_t stream)
        : info_(OrtAllocatorInfo("Cuda", OrtAllocatorType::OrtDeviceAllocator, 
                               OrtDevice(OrtDevice::GPU, OrtDevice::MemType::DEFAULT, device_id))),
          stream_(stream) {}
    
    void* Alloc(size_t size) override {
        std::lock_guard<std::mutex> lock(pool_.mutex_);
        
        // å¯¹é½åˆ°256å­—èŠ‚è¾¹ç•Œï¼ˆé’ˆå¯¹Tensor Coreä¼˜åŒ–ï¼‰
        size_t aligned_size = (size + 255) & ~255;
        
        // å°è¯•ä»å†…å­˜æ± è·å–
        auto it = pool_.free_blocks_.find(aligned_size);
        if (it != pool_.free_blocks_.end() && !it->second.empty()) {
            void* ptr = it->second.back();
            it->second.pop_back();
            pool_.allocated_blocks_[ptr] = aligned_size;
            return ptr;
        }
        
        // åˆ†é…æ–°å†…å­˜
        void* ptr = nullptr;
        cudaError_t result = cudaMalloc(&ptr, aligned_size);
        
        if (result != cudaSuccess) {
            // å†…å­˜ä¸è¶³ï¼Œå°è¯•æ¸…ç†å†…å­˜æ± 
            CleanupMemoryPool();
            result = cudaMalloc(&ptr, aligned_size);
            
            if (result != cudaSuccess) {
                throw std::bad_alloc();
            }
        }
        
        pool_.allocated_blocks_[ptr] = aligned_size;
        pool_.total_allocated_ += aligned_size;
        pool_.peak_allocated_ = std::max(pool_.peak_allocated_, pool_.total_allocated_);
        
        return ptr;
    }
    
    void Free(void* ptr) override {
        if (!ptr) return;
        
        std::lock_guard<std::mutex> lock(pool_.mutex_);
        
        auto it = pool_.allocated_blocks_.find(ptr);
        if (it == pool_.allocated_blocks_.end()) {
            return;  // ä¸æ˜¯æˆ‘ä»¬åˆ†é…çš„å†…å­˜
        }
        
        size_t size = it->second;
        pool_.allocated_blocks_.erase(it);
        pool_.total_allocated_ -= size;
        
        // è¿”å›åˆ°å†…å­˜æ± è€Œä¸æ˜¯ç«‹å³é‡Šæ”¾
        pool_.free_blocks_[size].push_back(ptr);
        
        // å¦‚æœç©ºé—²å—è¿‡å¤šï¼Œé‡Šæ”¾ä¸€äº›
        if (pool_.free_blocks_[size].size() > 8) {
            void* to_free = pool_.free_blocks_[size].front();
            pool_.free_blocks_[size].erase(pool_.free_blocks_[size].begin());
            cudaFree(to_free);
        }
    }
    
    void GetStats(AllocatorStats* stats) override {
        std::lock_guard<std::mutex> lock(pool_.mutex_);
        stats->bytes_in_use = pool_.total_allocated_;
        stats->peak_bytes_in_use = pool_.peak_allocated_;
        
        stats->num_allocs = pool_.allocated_blocks_.size();
        stats->num_frees = 0;  // ç®€åŒ–å®ç°
        
        for (const auto& [size, blocks] : pool_.free_blocks_) {
            stats->num_frees += blocks.size();
        }
    }
    
private:
    void CleanupMemoryPool() {
        // é‡Šæ”¾æ‰€æœ‰ç©ºé—²å—
        for (auto& [size, blocks] : pool_.free_blocks_) {
            for (void* ptr : blocks) {
                cudaFree(ptr);
            }
            blocks.clear();
        }
    }
    
    const OrtAllocatorInfo& Info() const override {
        return info_;
    }
};

// 3. å¼ é‡å†…å­˜å¸ƒå±€ä¼˜åŒ–
class TensorLayoutOptimizer {
public:
    // åˆ†ææœ€ä¼˜å†…å­˜å¸ƒå±€
    static std::string OptimalLayout(const TensorShape& shape, 
                                   const std::string& op_type,
                                   const std::string& device_type) {
        
        if (device_type == "cuda") {
            return OptimalCudaLayout(shape, op_type);
        } else if (device_type == "cpu") {
            return OptimalCpuLayout(shape, op_type);
        }
        
        return "NCHW";  // é»˜è®¤å¸ƒå±€
    }
    
private:
    static std::string OptimalCudaLayout(const TensorShape& shape, 
                                       const std::string& op_type) {
        
        // å¯¹äºå·ç§¯æ“ä½œ
        if (op_type == "Conv") {
            int64_t channels = shape[1];
            
            // å¦‚æœé€šé“æ•°æ˜¯32çš„å€æ•°ï¼Œä½¿ç”¨NCHW32å¸ƒå±€ä¼˜åŒ–Tensor Core
            if (channels % 32 == 0) {
                return "NCHW32";
            }
            // å¦‚æœé€šé“æ•°æ˜¯4çš„å€æ•°ï¼Œä½¿ç”¨NCHW4å¸ƒå±€
            else if (channels % 4 == 0) {
                return "NCHW4";
            }
            // å¦åˆ™ä½¿ç”¨æ ‡å‡†NCHW
            else {
                return "NCHW";
            }
        }
        
        // å¯¹äºçŸ©é˜µä¹˜æ³•ï¼Œè¡Œä¸»åºé€šå¸¸æ›´ä¼˜
        if (op_type == "MatMul") {
            return "RowMajor";
        }
        
        return "NCHW";
    }
    
    static std::string OptimalCpuLayout(const TensorShape& shape, 
                                      const std::string& op_type) {
        
        // CPUä¸ŠNHWCå¸ƒå±€é€šå¸¸å¯¹ç¼“å­˜æ›´å‹å¥½
        if (op_type == "Conv") {
            return "NHWC";
        }
        
        return "NCHW";
    }
};
```

é€šè¿‡æ·±å…¥åˆ†æONNXRuntimeçš„æ¶æ„è®¾è®¡ã€ç®—å­å®ç°å’Œä¼˜åŒ–ç­–ç•¥ï¼Œæˆ‘ä»¬å¯ä»¥å­¦åˆ°å·¥ä¸šçº§ç®—å­å¼€å‘çš„ç²¾é«“ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬ç»§ç»­æ·±å…¥ç ”ç©¶å…¶ä»–ä¸‰ä¸ªæ ¸å¿ƒé¡¹ç›®ã€‚

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šTritonç°ä»£GPUç¼–ç¨‹æ·±åº¦ç ”ç©¶

### 2.1 Tritonè¯­è¨€æ ¸å¿ƒç†å¿µä¸è®¾è®¡å“²å­¦

#### Tritonçš„é©å‘½æ€§æ„ä¹‰
```python
"""
Tritonæ˜¯OpenAIå¼€å‘çš„Python DSLï¼Œæ—¨åœ¨ç®€åŒ–GPUå†…æ ¸ç¼–ç¨‹
æ ¸å¿ƒç†å¿µï¼šè®©GPUç¼–ç¨‹åƒNumPyä¸€æ ·ç®€å•ï¼ŒåŒæ—¶ä¿æŒæ¥è¿‘CUDAçš„æ€§èƒ½

å…³é”®åˆ›æ–°ï¼š
1. é«˜çº§æŠ½è±¡ï¼šPythonè¯­æ³•ï¼Œè‡ªåŠ¨å†…å­˜ç®¡ç†
2. ç¼–è¯‘æ—¶ä¼˜åŒ–ï¼šç”Ÿæˆé«˜æ•ˆçš„PTXä»£ç 
3. å—ç¼–ç¨‹æ¨¡å‹ï¼šè‡ªåŠ¨å¤„ç†çº¿ç¨‹åä½œ
4. å†…å­˜åˆå¹¶ï¼šè‡ªåŠ¨ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
"""

import triton
import triton.language as tl
import torch

# Tritonçš„åŸºç¡€ç¼–ç¨‹æ¨¡å‹åˆ†æ
@triton.jit
def vector_add_kernel(
    x_ptr,  # è¾“å…¥å¼ é‡xçš„æŒ‡é’ˆ
    y_ptr,  # è¾“å…¥å¼ é‡yçš„æŒ‡é’ˆ  
    output_ptr,  # è¾“å‡ºå¼ é‡çš„æŒ‡é’ˆ
    n_elements,  # å¼ é‡å…ƒç´ æ€»æ•°
    BLOCK_SIZE: tl.constexpr,  # ç¼–è¯‘æ—¶å¸¸é‡ï¼Œå—å¤§å°
):
    """
    Tritonå‘é‡åŠ æ³•å†…æ ¸ - å±•ç¤ºåŸºç¡€ç¼–ç¨‹æ¨¡å‹
    
    å…³é”®æ¦‚å¿µï¼š
    1. @triton.jit - JITç¼–è¯‘è£…é¥°å™¨
    2. tl.constexpr - ç¼–è¯‘æ—¶å¸¸é‡ï¼Œç”¨äºä¼˜åŒ–
    3. æŒ‡é’ˆæ“ä½œ - ç›´æ¥å†…å­˜è®¿é—®
    4. å—å¹¶è¡Œæ¨¡å‹ - è‡ªåŠ¨çº¿ç¨‹è°ƒåº¦
    """
    
    # è·å–å½“å‰ç¨‹åºçš„å—IDï¼ˆç±»ä¼¼CUDAçš„blockIdxï¼‰
    pid = tl.program_id(axis=0)
    
    # è®¡ç®—å½“å‰å—å¤„ç†çš„å…ƒç´ èŒƒå›´
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    
    # åˆ›å»ºæ©ç é˜²æ­¢å†…å­˜è¶Šç•Œè®¿é—®
    mask = offsets < n_elements
    
    # åŠ è½½æ•°æ®ï¼šè‡ªåŠ¨å‘é‡åŒ–å’Œå†…å­˜åˆå¹¶
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    
    # å‘é‡åŒ–è®¡ç®—
    output = x + y
    
    # å­˜å‚¨ç»“æœï¼šè‡ªåŠ¨ä¼˜åŒ–å†™å…¥æ¨¡å¼
    tl.store(output_ptr + offsets, output, mask=mask)

# å¯åŠ¨å‡½æ•°ï¼šå±•ç¤ºTritonä¸PyTorchçš„é›†æˆ
def vector_add_triton(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    output = torch.empty_like(x)
    
    n_elements = x.numel()
    
    # é€‰æ‹©æœ€ä¼˜å—å¤§å°ï¼ˆé‡è¦çš„æ€§èƒ½è°ƒä¼˜å‚æ•°ï¼‰
    BLOCK_SIZE = 1024
    
    # è®¡ç®—ç½‘æ ¼å¤§å°
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    
    # å¯åŠ¨å†…æ ¸
    vector_add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=BLOCK_SIZE)
    
    return output
```

### 2.2 Tritoné«˜æ€§èƒ½çŸ©é˜µä¹˜æ³•å®ç°æ·±åº¦è§£æ

#### åˆ†å—çŸ©é˜µä¹˜æ³•çš„Tritonå®ç°
```python
@triton.jit
def matmul_kernel(
    # è¾“å…¥çŸ©é˜µæŒ‡é’ˆ
    a_ptr, b_ptr, c_ptr,
    # çŸ©é˜µç»´åº¦
    M, N, K,
    # çŸ©é˜µçš„æ­¥é•¿ï¼ˆstrideï¼‰ä¿¡æ¯
    stride_am, stride_ak,  # AçŸ©é˜µçš„è¡Œæ­¥é•¿å’Œåˆ—æ­¥é•¿
    stride_bk, stride_bn,  # BçŸ©é˜µçš„è¡Œæ­¥é•¿å’Œåˆ—æ­¥é•¿  
    stride_cm, stride_cn,  # CçŸ©é˜µçš„è¡Œæ­¥é•¿å’Œåˆ—æ­¥é•¿
    # ç¼–è¯‘æ—¶å¸¸é‡ï¼šåˆ†å—å¤§å°
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr, 
    BLOCK_SIZE_K: tl.constexpr,
):
    """
    é«˜æ€§èƒ½åˆ†å—çŸ©é˜µä¹˜æ³•å†…æ ¸
    
    å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼š
    1. åˆ†å—è®¡ç®—ï¼šå‡å°‘å…¨å±€å†…å­˜è®¿é—®
    2. å…±äº«å†…å­˜ä¼˜åŒ–ï¼šé‡ç”¨æ•°æ®
    3. å‘é‡åŒ–æ“ä½œï¼šåˆ©ç”¨GPUå¹¶è¡Œæ€§
    4. å†…å­˜åˆå¹¶ï¼šä¼˜åŒ–å†…å­˜å¸¦å®½
    """
    
    # è·å–ç¨‹åºIDå’Œè®¡ç®—å—åæ ‡
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    
    # 2Då—åˆ†è§£ï¼šå°†1Dç¨‹åºIDæ˜ å°„åˆ°2Då—åæ ‡
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    
    # è®¡ç®—å½“å‰å—çš„å…¨å±€ç´¢å¼•åç§»
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    
    # åˆå§‹åŒ–ç´¯åŠ å™¨ï¼ˆé‡è¦ï¼šä½¿ç”¨FP32ç´¯åŠ ä¿è¯æ•°å€¼ç²¾åº¦ï¼‰
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    # åˆ†å—è®¡ç®—å¾ªç¯ï¼šæ²¿Kç»´åº¦è¿›è¡Œåˆ†å—
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # è®¡ç®—AçŸ©é˜µå—çš„å†…å­˜åœ°å€
        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
        
        # è®¡ç®—BçŸ©é˜µå—çš„å†…å­˜åœ°å€  
        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)
        
        # åŠ è½½AçŸ©é˜µå—ï¼šè‡ªåŠ¨å¤„ç†è¾¹ç•Œæƒ…å†µ
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        
        # åŠ è½½BçŸ©é˜µå—
        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        
        # çŸ©é˜µä¹˜æ³•ç´¯åŠ ï¼šTritonè‡ªåŠ¨ä¼˜åŒ–ä¸ºé«˜æ•ˆçš„ç‚¹ç§¯æ“ä½œ
        accumulator += tl.dot(a, b)
        
        # æ›´æ–°Kç»´åº¦åç§»
        offs_k += BLOCK_SIZE_K
    
    # ç±»å‹è½¬æ¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
    c = accumulator.to(tl.float16)
    
    # è®¡ç®—è¾“å‡ºçŸ©é˜µçš„å†…å­˜åœ°å€
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    
    # åˆ›å»ºè¾“å‡ºæ©ç 
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    
    # å­˜å‚¨ç»“æœï¼šè‡ªåŠ¨ä¼˜åŒ–å†™å…¥æ¨¡å¼
    tl.store(c_ptrs, c, mask=c_mask)

# è‡ªåŠ¨è°ƒä¼˜æ¡†æ¶ï¼šå¯»æ‰¾æœ€ä¼˜å—å¤§å°é…ç½®
@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),
    ],
    key=['M', 'N', 'K'],  # è°ƒä¼˜å‚æ•°ï¼šæ ¹æ®çŸ©é˜µå¤§å°é€‰æ‹©æœ€ä¼˜é…ç½®
)
@triton.jit
def matmul_kernel_optimized(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
    # ä¸ä¸Šé¢ç›¸åŒçš„å®ç°ï¼Œä½†ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é…ç½®
    # ... (å†…æ ¸å®ç°ç›¸åŒ)
    pass

def matmul_triton(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """
    TritonçŸ©é˜µä¹˜æ³•çš„å®Œæ•´å®ç°
    
    æ€§èƒ½ç‰¹ç‚¹ï¼š
    1. æ¥è¿‘cuBLASçš„æ€§èƒ½ï¼ˆ80-95%ï¼‰
    2. æ›´çµæ´»çš„å®ç°å’Œè°ƒä¼˜
    3. æ˜“äºç†è§£å’Œä¿®æ”¹
    """
    
    # æ£€æŸ¥è¾“å…¥ç»´åº¦
    assert a.shape[1] == b.shape[0], "çŸ©é˜µç»´åº¦ä¸åŒ¹é…"
    assert a.is_contiguous(), "çŸ©é˜µAå¿…é¡»æ˜¯è¿ç»­çš„"
    assert b.is_contiguous(), "çŸ©é˜µBå¿…é¡»æ˜¯è¿ç»­çš„"
    
    M, K = a.shape
    K, N = b.shape
    
    # åˆ†é…è¾“å‡ºå¼ é‡
    c = torch.empty((M, N), device=a.device, dtype=a.dtype)
    
    # è®¡ç®—æ­¥é•¿
    stride_am, stride_ak = a.stride()
    stride_bk, stride_bn = b.stride()
    stride_cm, stride_cn = c.stride()
    
    # å¯åŠ¨å†…æ ¸ï¼šTritonè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é…ç½®
    def grid(META):
        return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)
    
    matmul_kernel_optimized[grid](
        a, b, c,
        M, N, K,
        stride_am, stride_ak,
        stride_bk, stride_bn,
        stride_cm, stride_cn,
    )
    
    return c
```

### 2.3 Tritoné«˜çº§ä¼˜åŒ–æŠ€æœ¯ï¼šFlash Attentionå®ç°

#### Flash Attentionçš„Tritonå®ç°
```python
@triton.jit
def flash_attention_kernel(
    Q, K, V, Out,
    L, M,  # ç”¨äºæ•°å€¼ç¨³å®šæ€§çš„è¾…åŠ©æ•°ç»„
    TMP,   # ä¸´æ—¶å­˜å‚¨
    softmax_scale,
    stride_qz, stride_qh, stride_qm, stride_qk,
    stride_kz, stride_kh, stride_kn, stride_kk,
    stride_vz, stride_vh, stride_vn, stride_vk,
    stride_oz, stride_oh, stride_om, stride_ok,
    Z, H, N_CTX,
    BLOCK_M: tl.constexpr,
    BLOCK_DMODEL: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    Flash Attentionå†…æ ¸ï¼šå†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å®ç°
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. åˆ†å—è®¡ç®—é¿å…å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ
    2. åœ¨çº¿softmaxç®—æ³•ä¿è¯æ•°å€¼ç¨³å®šæ€§
    3. é‡è®¡ç®—ç­–ç•¥å‡å°‘å†…å­˜ä½¿ç”¨
    
    ç®—æ³•å¤æ‚åº¦ï¼š
    - æ—¶é—´å¤æ‚åº¦ï¼šO(NÂ²)ï¼ˆä¸æ ‡å‡†æ³¨æ„åŠ›ç›¸åŒï¼‰
    - ç©ºé—´å¤æ‚åº¦ï¼šO(N)ï¼ˆæ ‡å‡†æ³¨æ„åŠ›ä¸ºO(NÂ²)ï¼‰
    """
    
    # è·å–ç¨‹åºID
    start_m = tl.program_id(0)
    off_hz = tl.program_id(1)
    
    # è®¡ç®—å½“å‰å¤´çš„åç§»
    off_z = off_hz // H
    off_h = off_hz % H
    
    # è®¡ç®—Qå—çš„ç´¢å¼•
    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh
    Q_block_ptr = Q + qvk_offset
    K_block_ptr = K + qvk_offset
    V_block_ptr = V + qvk_offset
    
    # åˆå§‹åŒ–æŒ‡é’ˆ
    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = tl.arange(0, BLOCK_N)
    offs_d = tl.arange(0, BLOCK_DMODEL)
    
    # åŠ è½½Qå—
    q_ptrs = Q_block_ptr + (offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk)
    q = tl.load(q_ptrs)
    
    # åˆå§‹åŒ–åœ¨çº¿softmaxçš„ç»Ÿè®¡é‡
    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")  # æœ€å¤§å€¼
    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)                # å½’ä¸€åŒ–å› å­
    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)  # ç´¯åŠ å™¨
    
    # éå†æ‰€æœ‰K/Vå—
    for start_n in range(0, N_CTX, BLOCK_N):
        start_n = tl.multiple_of(start_n, BLOCK_N)
        
        # åŠ è½½Kå—
        k_ptrs = K_block_ptr + (offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk)
        k = tl.load(k_ptrs)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šQ @ K^T
        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk += tl.dot(q, k)
        qk *= softmax_scale
        
        # åº”ç”¨å› æœæ©ç ï¼ˆç”¨äºè‡ªå›å½’æ¨¡å‹ï¼‰
        if start_n == 0:
            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float("-inf"))
        
        # åœ¨çº¿softmaxæ›´æ–°
        # 1. è®¡ç®—æ–°çš„æœ€å¤§å€¼
        m_ij = tl.max(qk, 1)
        m_i_new = tl.maximum(m_i, m_ij)
        
        # 2. è®¡ç®—softmaxæƒé‡
        alpha = tl.math.exp2(m_i - m_i_new)
        beta = tl.math.exp2(m_ij - m_i_new)
        
        # 3. æ›´æ–°å½’ä¸€åŒ–å› å­
        l_i_new = alpha * l_i + beta * tl.sum(tl.math.exp2(qk - m_ij[:, None]), 1)
        
        # 4. é‡æ–°ç¼©æ”¾ç´¯åŠ å™¨
        acc_scale = l_i / l_i_new * alpha
        acc = acc * acc_scale[:, None]
        
        # 5. åŠ è½½Vå—å¹¶ç´¯åŠ 
        v_ptrs = V_block_ptr + (offs_n[:, None] * stride_vn + offs_d[None, :] * stride_vk)
        v = tl.load(v_ptrs)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        p = tl.math.exp2(qk - m_ij[:, None])
        
        # ç´¯åŠ åˆ°è¾“å‡º
        acc += tl.dot(p.to(v.dtype), v)
        
        # æ›´æ–°ç»Ÿè®¡é‡
        l_i = l_i_new
        m_i = m_i_new
    
    # æœ€ç»ˆå½’ä¸€åŒ–
    acc = acc / l_i[:, None]
    
    # å­˜å‚¨è¾“å‡º
    off_o = off_z * stride_oz + off_h * stride_oh
    out_ptrs = Out + off_o + (offs_m[:, None] * stride_om + offs_d[None, :] * stride_ok)
    tl.store(out_ptrs, acc.to(Out.dtype.element_ty))

def flash_attention_triton(q, k, v, causal=False, sm_scale=None):
    """
    Flash Attentionçš„å®Œæ•´Tritonå®ç°
    
    ä¼˜åŠ¿ï¼š
    1. å†…å­˜ä½¿ç”¨é‡ä»O(NÂ²)é™åˆ°O(N)
    2. åœ¨é•¿åºåˆ—ä¸Šæ˜¾è‘—æé€Ÿ
    3. æ•°å€¼ç¨³å®šæ€§å¥½
    """
    
    BLOCK = 128
    
    # è‡ªåŠ¨è®¡ç®—softmaxç¼©æ”¾å› å­
    if sm_scale is None:
        sm_scale = 1.0 / math.sqrt(q.shape[-1])
    
    batch, heads, seqlen, d_model = q.shape
    
    # åˆ†é…è¾“å‡ºå’Œè¾…åŠ©å¼ é‡
    o = torch.empty_like(q)
    l = torch.empty((batch, heads, seqlen), device=q.device, dtype=torch.float32)
    m = torch.empty((batch, heads, seqlen), device=q.device, dtype=torch.float32)
    
    grid = (triton.cdiv(seqlen, BLOCK), batch * heads)
    
    flash_attention_kernel[grid](
        q, k, v, o,
        l, m,
        None,  # TMP
        sm_scale,
        q.stride(0), q.stride(1), q.stride(2), q.stride(3),
        k.stride(0), k.stride(1), k.stride(2), k.stride(3),
        v.stride(0), v.stride(1), v.stride(2), v.stride(3),
        o.stride(0), o.stride(1), o.stride(2), o.stride(3),
        batch, heads, seqlen,
        BLOCK_M=BLOCK,
        BLOCK_DMODEL=d_model,
        BLOCK_N=BLOCK,
        num_warps=4 if d_model <= 64 else 8,
        num_stages=1,
    )
    
    return o
```

### 2.4 Tritonç¼–è¯‘å™¨ä¼˜åŒ–å’Œæ€§èƒ½è°ƒä¼˜

#### Tritonç¼–è¯‘å™¨åç«¯åˆ†æ
```python
"""
Tritonç¼–è¯‘å™¨æ¶æ„æ·±åº¦è§£æ

ç¼–è¯‘æµç¨‹ï¼š
1. Python AST â†’ Triton IR
2. Triton IRä¼˜åŒ–
3. LLVM IRç”Ÿæˆ
4. PTXä»£ç ç”Ÿæˆ
5. JITç¼–è¯‘å’Œç¼“å­˜

å…³é”®ä¼˜åŒ–ï¼š
1. å†…å­˜åˆå¹¶ä¼˜åŒ–
2. å¾ªç¯å±•å¼€
3. æŒ‡ä»¤è°ƒåº¦
4. å¯„å­˜å™¨åˆ†é…
"""

class TritonCompilerAnalysis:
    def analyze_compilation_process(self):
        """
        åˆ†æTritonçš„ç¼–è¯‘è¿‡ç¨‹å’Œä¼˜åŒ–ç­–ç•¥
        """
        
        print("=== Tritonç¼–è¯‘å™¨ä¼˜åŒ–åˆ†æ ===")
        
        # 1. å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
        self.analyze_memory_coalescing()
        
        # 2. å¾ªç¯ä¼˜åŒ–
        self.analyze_loop_optimization()
        
        # 3. æŒ‡ä»¤çº§å¹¶è¡Œä¼˜åŒ–
        self.analyze_instruction_optimization()
        
        # 4. è‡ªåŠ¨è°ƒä¼˜æœºåˆ¶
        self.analyze_autotuning()
    
    def analyze_memory_coalescing(self):
        """
        åˆ†æTritonçš„å†…å­˜åˆå¹¶ä¼˜åŒ–
        """
        print("\n1. å†…å­˜åˆå¹¶ä¼˜åŒ–:")
        print("   - è‡ªåŠ¨æ£€æµ‹è¿ç»­å†…å­˜è®¿é—®æ¨¡å¼")
        print("   - ç”Ÿæˆå‘é‡åŒ–load/storeæŒ‡ä»¤")
        print("   - ä¼˜åŒ–å…¨å±€å†…å­˜å¸¦å®½åˆ©ç”¨ç‡")
        
        # ç¤ºä¾‹ï¼šä¼˜åŒ–å‰åçš„å¯¹æ¯”
        print("\n   ä¼˜åŒ–ç¤ºä¾‹:")
        print("   æ ‡é‡è®¿é—®: load %r1, [%ptr + %tid]")
        print("   å‘é‡è®¿é—®: load.v4 %r1-r4, [%ptr + %tid*4]")
    
    def analyze_loop_optimization(self):
        """
        åˆ†æå¾ªç¯ä¼˜åŒ–ç­–ç•¥
        """
        print("\n2. å¾ªç¯ä¼˜åŒ–:")
        print("   - å¾ªç¯å±•å¼€å‡å°‘åˆ†æ”¯å¼€é”€")
        print("   - è½¯ä»¶æµæ°´çº¿æé«˜å¹¶è¡Œåº¦")
        print("   - å¾ªç¯èåˆå‡å°‘å†…å­˜è®¿é—®")
        
    def analyze_instruction_optimization(self):
        """
        åˆ†ææŒ‡ä»¤çº§ä¼˜åŒ–
        """
        print("\n3. æŒ‡ä»¤çº§ä¼˜åŒ–:")
        print("   - è‡ªåŠ¨ä½¿ç”¨Tensor CoreæŒ‡ä»¤")
        print("   - æŒ‡ä»¤è°ƒåº¦éšè—å»¶è¿Ÿ")
        print("   - å¯„å­˜å™¨å‹åŠ›ç®¡ç†")

# æ€§èƒ½è°ƒä¼˜æœ€ä½³å®è·µ
class TritonPerformanceTuning:
    def __init__(self):
        self.best_practices = {
            "block_size": "é€‰æ‹©åˆé€‚çš„å—å¤§å°ï¼Œé€šå¸¸ä¸º32çš„å€æ•°",
            "memory_access": "ä¿è¯å†…å­˜è®¿é—®çš„åˆå¹¶æ€§",
            "register_usage": "é¿å…å¯„å­˜å™¨æº¢å‡º",
            "occupancy": "å¹³è¡¡çº¿ç¨‹å—å¤§å°å’Œå¯„å­˜å™¨ä½¿ç”¨",
            "autotuning": "ä½¿ç”¨è‡ªåŠ¨è°ƒä¼˜æ‰¾åˆ°æœ€ä¼˜é…ç½®"
        }
    
    def tune_block_size(self, kernel_func, input_shapes):
        """
        è‡ªåŠ¨è°ƒä¼˜å—å¤§å°
        """
        configs = []
        
        # ç”Ÿæˆå€™é€‰é…ç½®
        for block_m in [32, 64, 128, 256]:
            for block_n in [32, 64, 128, 256]:
                for block_k in [16, 32, 64]:
                    configs.append(
                        triton.Config(
                            {'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k},
                            num_stages=self.calculate_stages(block_m, block_n, block_k),
                            num_warps=self.calculate_warps(block_m, block_n)
                        )
                    )
        
        return configs
    
    def calculate_stages(self, block_m, block_n, block_k):
        """
        è®¡ç®—æœ€ä¼˜çš„æµæ°´çº¿é˜¶æ®µæ•°
        """
        # åŸºäºå—å¤§å°å’Œå†…å­˜ä½¿ç”¨é‡è®¡ç®—
        memory_usage = block_m * block_n * 4  # å‡è®¾FP32
        
        if memory_usage < 32 * 1024:  # 32KB
            return 4
        elif memory_usage < 64 * 1024:  # 64KB  
            return 3
        else:
            return 2
    
    def calculate_warps(self, block_m, block_n):
        """
        è®¡ç®—æœ€ä¼˜çš„warpæ•°é‡
        """
        threads = (block_m * block_n) // 32  # æ¯ä¸ªwarp 32ä¸ªçº¿ç¨‹
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        return min(max(threads // 32, 1), 8)

# Tritonæ€§èƒ½åŸºå‡†æµ‹è¯•
def benchmark_triton_vs_torch():
    """
    Triton vs PyTorchæ€§èƒ½å¯¹æ¯”
    """
    import time
    
    # æµ‹è¯•çŸ©é˜µä¹˜æ³•æ€§èƒ½
    sizes = [(1024, 1024, 1024), (2048, 2048, 2048), (4096, 4096, 4096)]
    
    for M, K, N in sizes:
        print(f"\nçŸ©é˜µå¤§å°: {M}x{K} @ {K}x{N}")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        a = torch.randn(M, K, device='cuda', dtype=torch.float16)
        b = torch.randn(K, N, device='cuda', dtype=torch.float16)
        
        # PyTorchåŸºå‡†
        torch.cuda.synchronize()
        start = time.time()
        for _ in range(100):
            c_torch = torch.matmul(a, b)
        torch.cuda.synchronize()
        torch_time = (time.time() - start) / 100
        
        # TritonåŸºå‡†
        torch.cuda.synchronize()
        start = time.time()
        for _ in range(100):
            c_triton = matmul_triton(a, b)
        torch.cuda.synchronize()
        triton_time = (time.time() - start) / 100
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        flops = 2 * M * K * N
        torch_tflops = flops / torch_time / 1e12
        triton_tflops = flops / triton_time / 1e12
        
        print(f"PyTorch: {torch_time*1000:.2f}ms, {torch_tflops:.2f} TFLOPS")
        print(f"Triton:  {triton_time*1000:.2f}ms, {triton_tflops:.2f} TFLOPS")
        print(f"åŠ é€Ÿæ¯”: {torch_time/triton_time:.2f}x")
        
        # éªŒè¯æ­£ç¡®æ€§
        max_diff = torch.max(torch.abs(c_torch - c_triton)).item()
        print(f"æœ€å¤§è¯¯å·®: {max_diff:.6f}")
```

### 2.5 Tritonåœ¨å®é™…é¡¹ç›®ä¸­çš„åº”ç”¨

#### è‡ªå®šä¹‰ç®—å­å¼€å‘å®æˆ˜
```python
# å®æˆ˜é¡¹ç›®ï¼šä½¿ç”¨Tritonå®ç°é«˜æ€§èƒ½LayerNorm
@triton.jit
def layer_norm_kernel(
    X,  # è¾“å…¥å¼ é‡
    Y,  # è¾“å‡ºå¼ é‡  
    W,  # æƒé‡
    B,  # åç½®
    Mean,  # å‡å€¼ï¼ˆè¾“å‡ºï¼‰
    Rstd,  # æ ‡å‡†å·®å€’æ•°ï¼ˆè¾“å‡ºï¼‰
    stride_x_row,
    stride_y_row,
    N,  # ç‰¹å¾ç»´åº¦
    eps,  # æ•°å€¼ç¨³å®šæ€§å‚æ•°
    BLOCK_SIZE: tl.constexpr,
):
    """
    é«˜æ€§èƒ½LayerNormå®ç°
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. åœ¨çº¿ç®—æ³•è®¡ç®—å‡å€¼å’Œæ–¹å·®
    2. å‘é‡åŒ–æ“ä½œ
    3. å†…å­˜åˆå¹¶è®¿é—®
    """
    
    # è·å–è¡Œç´¢å¼•
    row_idx = tl.program_id(0)
    
    # è®¡ç®—å½“å‰è¡Œçš„æŒ‡é’ˆ
    X += row_idx * stride_x_row
    Y += row_idx * stride_y_row
    
    # è®¡ç®—åç§»
    cols = tl.arange(0, BLOCK_SIZE)
    mask = cols < N
    
    # ç¬¬ä¸€éï¼šè®¡ç®—å‡å€¼
    x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)
    mean = tl.sum(x, axis=0) / N
    
    # ç¬¬äºŒéï¼šè®¡ç®—æ–¹å·®
    x_centered = x - mean
    var = tl.sum(x_centered * x_centered, axis=0) / N
    rstd = 1.0 / tl.sqrt(var + eps)
    
    # å½’ä¸€åŒ–å’Œä»¿å°„å˜æ¢
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    b = tl.load(B + cols, mask=mask).to(tl.float32)
    
    y = x_centered * rstd * w + b
    
    # å­˜å‚¨ç»“æœ
    tl.store(Y + cols, y, mask=mask)
    
    # å­˜å‚¨ç»Ÿè®¡é‡
    if tl.program_id(0) == 0:
        tl.store(Mean + row_idx, mean)
        tl.store(Rstd + row_idx, rstd)

# å®Œæ•´çš„LayerNormå®ç°
def layer_norm_triton(x, weight, bias, eps=1e-5):
    """
    Triton LayerNormå®ç°
    """
    M, N = x.shape
    
    # åˆ†é…è¾“å‡ºå¼ é‡
    y = torch.empty_like(x)
    mean = torch.empty(M, device=x.device, dtype=torch.float32)
    rstd = torch.empty(M, device=x.device, dtype=torch.float32)
    
    # é€‰æ‹©å—å¤§å°
    BLOCK_SIZE = triton.next_power_of_2(N)
    
    # å¯åŠ¨å†…æ ¸
    layer_norm_kernel[(M,)](
        x, y, weight, bias, mean, rstd,
        x.stride(0), y.stride(0),
        N, eps,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return y, mean, rstd

# Tritonå­¦ä¹ è·¯å¾„å’Œå®è·µå»ºè®®
class TritonLearningPath:
    def __init__(self):
        self.learning_stages = {
            "å…¥é—¨": ["åŸºç¡€è¯­æ³•", "ç®€å•å†…æ ¸", "å†…å­˜æ¨¡å‹"],
            "è¿›é˜¶": ["æ€§èƒ½ä¼˜åŒ–", "è‡ªåŠ¨è°ƒä¼˜", "å¤æ‚ç®—å­"],
            "é«˜çº§": ["ç¼–è¯‘å™¨ç†è§£", "è‡ªå®šä¹‰ä¼˜åŒ–", "é¡¹ç›®é›†æˆ"]
        }
    
    def recommend_projects(self):
        """
        æ¨èTritonå­¦ä¹ é¡¹ç›®
        """
        projects = [
            {
                "name": "åŸºç¡€ç®—å­å®ç°",
                "description": "å®ç°å‘é‡è¿ç®—ã€çŸ©é˜µä¹˜æ³•ç­‰åŸºç¡€ç®—å­",
                "difficulty": "åˆçº§",
                "skills": ["Tritonè¯­æ³•", "GPUå¹¶è¡Œç¼–ç¨‹", "æ€§èƒ½æµ‹è¯•"]
            },
            {
                "name": "Flash Attention",
                "description": "å®ç°å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶",
                "difficulty": "ä¸­çº§", 
                "skills": ["åˆ†å—ç®—æ³•", "åœ¨çº¿ç®—æ³•", "æ•°å€¼ç¨³å®šæ€§"]
            },
            {
                "name": "è‡ªå®šä¹‰æ·±åº¦å­¦ä¹ å±‚",
                "description": "å®ç°LayerNormã€GroupNormç­‰æ ‡å‡†åŒ–å±‚",
                "difficulty": "ä¸­çº§",
                "skills": ["ç»Ÿè®¡è®¡ç®—", "æ•°å€¼ä¼˜åŒ–", "PyTorché›†æˆ"]
            },
            {
                "name": "é‡åŒ–ç®—å­",
                "description": "å®ç°INT8/FP16é‡åŒ–è®¡ç®—å†…æ ¸",
                "difficulty": "é«˜çº§",
                "skills": ["æ··åˆç²¾åº¦", "æ•°å€¼ç²¾åº¦", "ç¡¬ä»¶ä¼˜åŒ–"]
            }
        ]
        
        return projects
```

é€šè¿‡æ·±å…¥å­¦ä¹ Tritonï¼Œæˆ‘ä»¬æŒæ¡äº†ç°ä»£GPUç¼–ç¨‹çš„ç²¾é«“ï¼šç®€æ´çš„è¯­æ³•ã€å¼ºå¤§çš„æ€§èƒ½ã€çµæ´»çš„ä¼˜åŒ–ã€‚è¿™ä¸ºæˆ‘ä»¬åœ¨AIèŠ¯ç‰‡ç®—å­å¼€å‘ä¸­æä¾›äº†é‡è¦çš„å‚è€ƒå’Œå·¥å…·ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬ç»§ç»­ç ”ç©¶PyTorch ATençš„åŠ¨æ€å›¾ç®—å­å®ç°ã€‚

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šPyTorch ATenåŠ¨æ€å›¾ç®—å­å®ç°æ·±åº¦åˆ†æ

### 3.1 ATenå¼ é‡åº“æ¶æ„æ ¸å¿ƒè§£æ

#### ATençš„è®¾è®¡ç†å¿µå’Œæ¶æ„
```cpp
// PyTorch ATenå¼ é‡åº“æ ¸å¿ƒæ¶æ„åˆ†æ

namespace at {

// 1. Tensorç±»ï¼šPyTorchçš„æ ¸å¿ƒæ•°æ®ç»“æ„
class TORCH_API Tensor {
private:
    c10::intrusive_ptr<TensorImpl> impl_;  // å®é™…æ•°æ®å®ç°çš„æ™ºèƒ½æŒ‡é’ˆ
    
public:
    // æ„é€ å‡½æ•°
    Tensor() : impl_(c10::make_intrusive<TensorImpl>(
        c10::DispatchKeySet{}, 
        caffe2::TypeMeta::Make<float>(), 
        c10::nullopt)) {}
    
    explicit Tensor(c10::intrusive_ptr<TensorImpl> tensor_impl) 
        : impl_(std::move(tensor_impl)) {}
    
    // æ ¸å¿ƒæ–¹æ³•ï¼šç®—å­è°ƒç”¨çš„ç»Ÿä¸€å…¥å£
    template <typename... Args>
    auto call_op(const c10::OperatorHandle& op, Args&&... args) const {
        return op.call(args...);
    }
    
    // åŠ¨æ€åˆ†å‘ï¼šæ ¹æ®è®¾å¤‡å’Œæ•°æ®ç±»å‹é€‰æ‹©å®ç°
    Tensor add(const Tensor& other, const Scalar& alpha = 1) const {
        return at::add(*this, other, alpha);
    }
    
    Tensor matmul(const Tensor& other) const {
        return at::matmul(*this, other);
    }
    
    // å†…å­˜ç®¡ç†
    bool is_contiguous() const { return impl_->is_contiguous(); }
    Tensor contiguous() const { return impl_->is_contiguous() ? *this : __contiguous(); }
    
    // è®¾å¤‡ç®¡ç†
    Device device() const { return impl_->device(); }
    Tensor to(Device device) const { return __to_device(device); }
    
    // å½¢çŠ¶ä¿¡æ¯
    IntArrayRef sizes() const { return impl_->sizes(); }
    IntArrayRef strides() const { return impl_->strides(); }
    int64_t ndimension() const { return impl_->dim(); }
    
private:
    Tensor __contiguous() const;
    Tensor __to_device(Device device) const;
};

// 2. TensorImplï¼šå¼ é‡çš„å…·ä½“å®ç°
class TORCH_API TensorImpl : public c10::intrusive_ptr_target {
private:
    // æ ¸å¿ƒæˆå‘˜å˜é‡
    Storage storage_;                    // æ•°æ®å­˜å‚¨
    int64_t storage_offset_ = 0;        // å­˜å‚¨åç§»
    SmallVector<int64_t, 5> sizes_;     // å¼ é‡ç»´åº¦
    SmallVector<int64_t, 5> strides_;   // æ­¥é•¿ä¿¡æ¯
    int64_t numel_ = 1;                 // å…ƒç´ æ€»æ•°
    caffe2::TypeMeta dtype_;            // æ•°æ®ç±»å‹
    c10::optional<c10::Device> device_; // è®¾å¤‡ä¿¡æ¯
    c10::DispatchKeySet key_set_;       // åˆ†å‘é”®é›†åˆ
    
public:
    TensorImpl(c10::DispatchKeySet key_set, const caffe2::TypeMeta& data_type, 
              c10::optional<c10::Device> device_opt)
        : key_set_(key_set), dtype_(data_type), device_(device_opt) {}
    
    // æ ¸å¿ƒåˆ†å‘é€»è¾‘
    c10::DispatchKeySet key_set() const { return key_set_; }
    
    // æ•°æ®è®¿é—®
    template <typename T>
    T* data_ptr() const {
        return static_cast<T*>(storage_.data()) + storage_offset_;
    }
    
    // ç»´åº¦æ“ä½œ
    void resize_(IntArrayRef size) {
        sizes_ = size.vec();
        refresh_numel();
        refresh_contiguous();
    }
    
    // å†…å­˜å¸ƒå±€
    bool is_contiguous() const {
        return compute_contiguous();
    }
    
private:
    void refresh_numel() {
        numel_ = 1;
        for (auto s : sizes_) numel_ *= s;
    }
    
    bool compute_contiguous() const {
        if (sizes_.empty()) return true;
        
        int64_t expected_stride = 1;
        for (int64_t i = sizes_.size() - 1; i >= 0; i--) {
            if (sizes_[i] != 1 && strides_[i] != expected_stride) {
                return false;
            }
            expected_stride *= sizes_[i];
        }
        return true;
    }
};

// 3. åŠ¨æ€åˆ†å‘ç³»ç»Ÿæ ¸å¿ƒ
class TORCH_API Dispatcher {
private:
    // ç®—å­æ³¨å†Œè¡¨ï¼šå­˜å‚¨æ‰€æœ‰å·²æ³¨å†Œçš„ç®—å­
    std::unordered_map<c10::OperatorName, std::unique_ptr<OperatorEntry>> operators_;
    std::mutex mutex_;
    
public:
    static Dispatcher& singleton() {
        static Dispatcher instance;
        return instance;
    }
    
    // ç®—å­æ³¨å†Œ
    c10::OperatorHandle registerOperator(c10::OperatorName op_name,
                                        c10::FunctionSchema schema,
                                        std::function<void(OperatorKernel*)> kernel_func) {
        std::lock_guard<std::mutex> lock(mutex_);
        
        auto op_entry = std::make_unique<OperatorEntry>(std::move(schema));
        op_entry->registerKernel(kernel_func);
        
        auto handle = c10::OperatorHandle(op_entry.get());
        operators_[op_name] = std::move(op_entry);
        
        return handle;
    }
    
    // æ ¸å¿ƒåˆ†å‘é€»è¾‘ï¼šæ ¹æ®è¾“å…¥å¼ é‡çš„è®¾å¤‡å’Œç±»å‹é€‰æ‹©å†…æ ¸
    void call(const c10::OperatorHandle& op, Stack* stack) const {
        // 1. æå–åˆ†å‘é”®
        auto dispatch_key_set = computeDispatchKeySet(*stack);
        
        // 2. é€‰æ‹©æœ€é«˜ä¼˜å…ˆçº§çš„é”®
        auto dispatch_key = dispatch_key_set.highestPriorityTypeId();
        
        // 3. æŸ¥æ‰¾å¯¹åº”çš„å†…æ ¸
        const auto& kernel = op.lookup(dispatch_key);
        
        // 4. è°ƒç”¨å†…æ ¸
        kernel.call(stack);
    }
    
private:
    c10::DispatchKeySet computeDispatchKeySet(const Stack& stack) const {
        c10::DispatchKeySet key_set;
        
        // éå†æ‰€æœ‰å¼ é‡å‚æ•°ï¼Œåˆå¹¶å®ƒä»¬çš„åˆ†å‘é”®
        for (const auto& arg : stack) {
            if (arg.isTensor()) {
                const auto& tensor = arg.toTensor();
                key_set = key_set | tensor.key_set();
            }
        }
        
        return key_set;
    }
};
}

// ATenç®—å­å®ç°çš„å…¸å‹æ¨¡å¼
namespace at {
namespace native {

// CPUå®ç°ç¤ºä¾‹ï¼šå‘é‡åŠ æ³•
Tensor add_cpu(const Tensor& self, const Tensor& other, const Scalar& alpha) {
    // 1. è¾“å…¥éªŒè¯å’Œå¹¿æ’­
    auto result = at::empty_like(self);
    auto iter = TensorIterator::comparison_op(result, self, other);
    
    // 2. è°ƒç”¨ä¼˜åŒ–çš„CPUå†…æ ¸
    add_stub(iter.device_type(), iter, alpha);
    
    return result;
}

// CUDAå®ç°ç¤ºä¾‹ï¼šå‘é‡åŠ æ³•
Tensor add_cuda(const Tensor& self, const Tensor& other, const Scalar& alpha) {
    auto result = at::empty_like(self);
    auto iter = TensorIterator::comparison_op(result, self, other);
    
    // è°ƒç”¨CUDAå†…æ ¸
    add_stub(iter.device_type(), iter, alpha);
    
    return result;
}

// ç®—å­æ³¨å†Œå®ï¼šå°†å®ç°æ³¨å†Œåˆ°åˆ†å‘ç³»ç»Ÿ
TORCH_LIBRARY_IMPL(aten, CPU, m) {
    m.impl("add", TORCH_FN(add_cpu));
}

TORCH_LIBRARY_IMPL(aten, CUDA, m) {
    m.impl("add", TORCH_FN(add_cuda));
}

}}
```

### 3.2 PyTorchè‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿæ·±åº¦åˆ†æ

#### Autogradå¼•æ“å®ç°æœºåˆ¶
```cpp
// PyTorchè‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿæ ¸å¿ƒå®ç°

namespace torch { namespace autograd {

// 1. Variableï¼šæ”¯æŒè‡ªåŠ¨å¾®åˆ†çš„å¼ é‡
class TORCH_API Variable {
private:
    at::Tensor data_;                          // å®é™…æ•°æ®
    std::shared_ptr<Node> grad_fn_;           // æ¢¯åº¦å‡½æ•°èŠ‚ç‚¹
    at::Tensor grad_;                         // æ¢¯åº¦å¼ é‡
    bool requires_grad_ = false;              // æ˜¯å¦éœ€è¦æ¢¯åº¦
    bool is_leaf_ = true;                     // æ˜¯å¦ä¸ºå¶å­èŠ‚ç‚¹
    
public:
    Variable(at::Tensor data, bool requires_grad = false)
        : data_(std::move(data)), requires_grad_(requires_grad) {}
    
    // è‡ªåŠ¨å¾®åˆ†æ ¸å¿ƒæ¥å£
    void backward(
        const at::Tensor& gradient = {},
        bool retain_graph = false,
        bool create_graph = false) const {
        
        // å¯åŠ¨åå‘ä¼ æ’­
        Engine::get_default_engine().execute(
            {grad_fn_}, {gradient.defined() ? gradient : at::ones_like(data_)},
            retain_graph, create_graph, {});
    }
    
    // è®¾ç½®æ¢¯åº¦å‡½æ•°
    void set_grad_fn(std::shared_ptr<Node> grad_fn) {
        grad_fn_ = std::move(grad_fn);
        is_leaf_ = false;
    }
    
    // æ¢¯åº¦ç´¯ç§¯
    void set_grad(const at::Tensor& new_grad) {
        if (grad_.defined()) {
            grad_ = grad_ + new_grad;  // ç´¯ç§¯æ¢¯åº¦
        } else {
            grad_ = new_grad;
        }
    }
};

// 2. è®¡ç®—å›¾èŠ‚ç‚¹ï¼šè¡¨ç¤ºä¸€ä¸ªæ“ä½œ
class TORCH_API Node {
protected:
    std::vector<Edge> next_edges_;        // åç»­è¾¹ï¼ˆè¾“å…¥çš„æ¢¯åº¦å‡½æ•°ï¼‰
    PyObject* pyobj_ = nullptr;           // Pythonå¯¹è±¡å¼•ç”¨
    
public:
    virtual ~Node() = default;
    
    // æ ¸å¿ƒæ¥å£ï¼šè®¡ç®—æ¢¯åº¦
    virtual variable_list apply(variable_list&& grads) = 0;
    
    // æ·»åŠ è¾“å…¥è¾¹
    void add_input_metadata(const Variable& input) {
        if (input.requires_grad()) {
            next_edges_.emplace_back(input.grad_fn(), input.output_nr());
        } else {
            next_edges_.emplace_back();
        }
    }
    
    // è·å–åç»­è¾¹
    const std::vector<Edge>& next_edges() const { return next_edges_; }
};

// 3. å…·ä½“ç®—å­çš„æ¢¯åº¦å‡½æ•°å®ç°
class TORCH_API AddBackward : public Node {
private:
    at::ScalarType self_scalar_type;
    at::ScalarType other_scalar_type;
    Scalar alpha;
    
public:
    AddBackward(at::ScalarType self_scalar_type_, 
               at::ScalarType other_scalar_type_,
               const Scalar& alpha_)
        : self_scalar_type(self_scalar_type_)
        , other_scalar_type(other_scalar_type_)
        , alpha(alpha_) {}
    
    variable_list apply(variable_list&& grads) override {
        auto& grad = grads[0];
        
        variable_list grad_inputs(2);
        
        // å¯¹ç¬¬ä¸€ä¸ªè¾“å…¥çš„æ¢¯åº¦ï¼šç›´æ¥ä¼ é€’
        if (should_compute_output(0)) {
            grad_inputs[0] = grad;
        }
        
        // å¯¹ç¬¬äºŒä¸ªè¾“å…¥çš„æ¢¯åº¦ï¼šä¹˜ä»¥alpha
        if (should_compute_output(1)) {
            if (alpha.equal(1)) {
                grad_inputs[1] = grad;
            } else {
                grad_inputs[1] = grad * alpha;
            }
        }
        
        return grad_inputs;
    }
};

// 4. è‡ªåŠ¨å¾®åˆ†å¼•æ“
class TORCH_API Engine {
private:
    // æ‰§è¡Œé˜Ÿåˆ—ï¼šå­˜å‚¨å¾…æ‰§è¡Œçš„æ¢¯åº¦å‡½æ•°
    std::queue<ReadyQueue> ready_queues_;
    std::mutex mutex_;
    
    // çº¿ç¨‹æ± ï¼šå¹¶è¡Œæ‰§è¡Œæ¢¯åº¦è®¡ç®—
    ThreadPool thread_pool_;
    
public:
    static Engine& get_default_engine() {
        static Engine engine;
        return engine;
    }
    
    // æ ¸å¿ƒæ‰§è¡Œæ¥å£
    variable_list execute(
        const std::vector<std::shared_ptr<Node>>& roots,
        const variable_list& inputs,
        bool keep_graph,
        bool create_graph,
        const std::vector<Variable>& outputs) {
        
        // 1. æ„å»ºæ‰§è¡Œå›¾
        auto exec_info = make_exec_info(roots, inputs, outputs);
        
        // 2. æ‹“æ‰‘æ’åº
        auto sorted_nodes = topological_sort(exec_info.graph);
        
        // 3. åå‘æ‰§è¡Œ
        return execute_graph(sorted_nodes, exec_info, keep_graph, create_graph);
    }
    
private:
    variable_list execute_graph(
        const std::vector<std::shared_ptr<Node>>& sorted_nodes,
        const ExecInfo& exec_info,
        bool keep_graph,
        bool create_graph) {
        
        // æ¢¯åº¦ç¼“å†²åŒº
        std::unordered_map<std::shared_ptr<Node>, variable_list> gradients;
        
        // åå‘éå†è®¡ç®—å›¾
        for (auto it = sorted_nodes.rbegin(); it != sorted_nodes.rend(); ++it) {
            auto& node = *it;
            
            // è·å–å½“å‰èŠ‚ç‚¹çš„è¾“å…¥æ¢¯åº¦
            auto input_grads = collect_input_gradients(node, gradients);
            
            // æ‰§è¡Œæ¢¯åº¦è®¡ç®—
            auto output_grads = node->apply(std::move(input_grads));
            
            // åˆ†å‘æ¢¯åº¦åˆ°ä¸‹ä¸€å±‚
            distribute_gradients(node, output_grads, gradients);
        }
        
        // è¿”å›æ ¹èŠ‚ç‚¹çš„æ¢¯åº¦
        return collect_root_gradients(exec_info.roots, gradients);
    }
    
    void distribute_gradients(
        const std::shared_ptr<Node>& node,
        const variable_list& grads,
        std::unordered_map<std::shared_ptr<Node>, variable_list>& gradient_map) {
        
        const auto& next_edges = node->next_edges();
        
        for (size_t i = 0; i < next_edges.size(); ++i) {
            const auto& edge = next_edges[i];
            
            if (edge.function) {
                // ç´¯ç§¯æ¢¯åº¦
                auto& node_grads = gradient_map[edge.function];
                if (node_grads.empty()) {
                    node_grads.resize(edge.function->num_outputs());
                }
                
                if (node_grads[edge.input_nr].defined()) {
                    node_grads[edge.input_nr] = node_grads[edge.input_nr] + grads[i];
                } else {
                    node_grads[edge.input_nr] = grads[i];
                }
            }
        }
    }
};

// 5. å‡½æ•°åŒ…è£…å™¨ï¼šè¿æ¥å‰å‘å’Œåå‘è®¡ç®—
template<typename Func>
class Function {
public:
    static Variable apply(Variable input, Func forward_func) {
        // å‰å‘è®¡ç®—
        auto result_data = forward_func(input.data());
        auto result = Variable(result_data, input.requires_grad());
        
        // å¦‚æœéœ€è¦æ¢¯åº¦ï¼Œåˆ›å»ºåå‘è®¡ç®—èŠ‚ç‚¹
        if (input.requires_grad()) {
            auto grad_fn = std::make_shared<typename Func::Backward>();
            grad_fn->add_input_metadata(input);
            result.set_grad_fn(grad_fn);
        }
        
        return result;
    }
};

}}

// è‡ªåŠ¨å¾®åˆ†çš„ä½¿ç”¨ç¤ºä¾‹
namespace torch { namespace autograd {

// è‡ªå®šä¹‰å‡½æ•°ç¤ºä¾‹ï¼šå¹³æ–¹è¿ç®—
class SquareFunction : public Function<SquareFunction> {
public:
    static at::Tensor forward(const at::Tensor& input) {
        return input * input;
    }
    
    class Backward : public Node {
    private:
        at::Tensor saved_input;
        
    public:
        Backward(const at::Tensor& input) : saved_input(input) {}
        
        variable_list apply(variable_list&& grads) override {
            // d/dx(xÂ²) = 2x
            auto grad_input = grads[0] * (2 * saved_input);
            return {grad_input};
        }
    };
};

// ä½¿ç”¨ç¤ºä¾‹
void autograd_example() {
    // åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
    auto x = torch::tensor({2.0}, torch::requires_grad(true));
    
    // å‰å‘è®¡ç®—
    auto y = SquareFunction::apply(x);  // y = xÂ²
    auto z = y.sum();                   // z = sum(y)
    
    // åå‘ä¼ æ’­
    z.backward();
    
    // è·å–æ¢¯åº¦ï¼šdz/dx = 2x = 4
    std::cout << "Gradient: " << x.grad() << std::endl;
}

}}
```

### 3.3 ATené«˜æ€§èƒ½å†…æ ¸å®ç°åˆ†æ

#### TensorIteratorå’Œå†…æ ¸åˆ†å‘æœºåˆ¶
```cpp
// ATençš„é«˜æ€§èƒ½å†…æ ¸å®ç°æ¡†æ¶

namespace at {

// 1. TensorIteratorï¼šé«˜æ•ˆçš„å¤šå¼ é‡è¿­ä»£å™¨
class TORCH_API TensorIterator {
private:
    SmallVector<OperandInfo, 4> operands_;    // æ“ä½œæ•°ä¿¡æ¯
    SmallVector<char*, 4> data_ptrs_;         // æ•°æ®æŒ‡é’ˆ
    SmallVector<int64_t, 8> strides_;         // æ­¥é•¿ä¿¡æ¯
    int64_t numel_ = 0;                       // å…ƒç´ æ€»æ•°
    int ndim_ = 0;                            // ç»´åº¦æ•°
    bool is_reduction_ = false;               // æ˜¯å¦ä¸ºå½’çº¦æ“ä½œ
    ScalarType common_dtype_ = ScalarType::Undefined;  // é€šç”¨æ•°æ®ç±»å‹
    
public:
    // æ„å»ºäºŒå…ƒæ“ä½œçš„è¿­ä»£å™¨
    static TensorIterator binary_op(Tensor& out, const Tensor& a, const Tensor& b) {
        return TensorIterator()
            .add_output(out)
            .add_input(a)
            .add_input(b)
            .build();
    }
    
    // æ„å»ºä¸€å…ƒæ“ä½œçš„è¿­ä»£å™¨
    static TensorIterator unary_op(Tensor& out, const Tensor& input) {
        return TensorIterator()
            .add_output(out)
            .add_input(input)
            .build();
    }
    
    // æ·»åŠ è¾“å‡ºå¼ é‡
    TensorIterator& add_output(const Tensor& tensor) {
        operands_.emplace_back(tensor, /*is_output=*/true);
        return *this;
    }
    
    // æ·»åŠ è¾“å…¥å¼ é‡
    TensorIterator& add_input(const Tensor& tensor) {
        operands_.emplace_back(tensor, /*is_output=*/false);
        return *this;
    }
    
    // æ„å»ºè¿­ä»£å™¨ï¼šæ ¸å¿ƒä¼˜åŒ–é€»è¾‘
    TensorIterator& build() {
        // 1. è®¡ç®—è¾“å‡ºç»´åº¦å’Œæ•°æ®ç±»å‹
        compute_types();
        
        // 2. å¹¿æ’­æ“ä½œæ•°åˆ°ç›¸åŒå½¢çŠ¶
        compute_shape();
        
        // 3. ä¼˜åŒ–å†…å­˜å¸ƒå±€
        reorder_dimensions();
        
        // 4. åˆ†é…è¾“å‡ºå†…å­˜
        allocate_outputs();
        
        // 5. æ£€æŸ¥å†…å­˜é‡å 
        compute_mem_overlaps();
        
        return *this;
    }
    
    // é«˜æ•ˆè¿­ä»£ï¼šæ”¯æŒå¤šç§è®¿é—®æ¨¡å¼
    template <typename Func>
    void for_each(Func f, int64_t grain_size = at::internal::GRAIN_SIZE) {
        if (is_contiguous()) {
            // è¿ç»­å†…å­˜ä¼˜åŒ–è·¯å¾„
            for_each_contiguous(f, grain_size);
        } else {
            // é€šç”¨è¿­ä»£è·¯å¾„
            for_each_strided(f, grain_size);
        }
    }
    
private:
    void compute_types() {
        // ç±»å‹æå‡é€»è¾‘ï¼šè®¡ç®—é€šç”¨æ•°æ®ç±»å‹
        common_dtype_ = ScalarType::Undefined;
        
        for (const auto& op : operands_) {
            if (!op.is_output) {
                common_dtype_ = promoteTypes(common_dtype_, op.tensor.scalar_type());
            }
        }
    }
    
    void compute_shape() {
        // å¹¿æ’­è§„åˆ™å®ç°
        SmallVector<int64_t, 8> shape;
        
        // è®¡ç®—æœ€å¤§ç»´åº¦æ•°
        int max_ndim = 0;
        for (const auto& op : operands_) {
            max_ndim = std::max(max_ndim, static_cast<int>(op.tensor.dim()));
        }
        
        // åå‘å¹¿æ’­
        shape.resize(max_ndim, 1);
        for (int i = max_ndim - 1; i >= 0; i--) {
            int64_t target_size = 1;
            
            for (const auto& op : operands_) {
                auto tensor_dim = op.tensor.dim();
                if (tensor_dim > max_ndim - 1 - i) {
                    int64_t size = op.tensor.size(tensor_dim - max_ndim + i);
                    if (size != 1) {
                        if (target_size == 1) {
                            target_size = size;
                        } else if (target_size != size) {
                            TORCH_CHECK(false, "The size of tensor mismatch at dimension ", i);
                        }
                    }
                }
            }
            
            shape[i] = target_size;
        }
        
        ndim_ = max_ndim;
        numel_ = std::accumulate(shape.begin(), shape.end(), 1, std::multiplies<int64_t>());
    }
    
    void reorder_dimensions() {
        // ç»´åº¦é‡æ’ä¼˜åŒ–ï¼šå°†æ­¥é•¿æœ€å¤§çš„ç»´åº¦æ”¾åœ¨æœ€å
        // è¿™æ ·å¯ä»¥æœ€å¤§åŒ–å†…å­˜å±€éƒ¨æ€§
        
        SmallVector<int64_t, 8> perm(ndim_);
        std::iota(perm.begin(), perm.end(), 0);
        
        // æŒ‰æ­¥é•¿æ’åº
        std::sort(perm.begin(), perm.end(), [this](int64_t a, int64_t b) {
            int64_t stride_a = 0, stride_b = 0;
            
            for (const auto& op : operands_) {
                if (!op.is_output) {
                    stride_a += op.tensor.stride(a);
                    stride_b += op.tensor.stride(b);
                }
            }
            
            return stride_a > stride_b;
        });
        
        // åº”ç”¨é‡æ’
        apply_permutation(perm);
    }
    
    template <typename Func>
    void for_each_contiguous(Func f, int64_t grain_size) {
        // è¿ç»­å†…å­˜çš„é«˜æ•ˆå¹¶è¡Œå¤„ç†
        at::parallel_for(0, numel_, grain_size, [&](int64_t begin, int64_t end) {
            for (int64_t i = begin; i < end; i++) {
                // æ”¶é›†æ‰€æœ‰æ“ä½œæ•°çš„æ•°æ®æŒ‡é’ˆ
                SmallVector<void*, 4> ptrs;
                for (size_t op_idx = 0; op_idx < operands_.size(); op_idx++) {
                    ptrs.push_back(static_cast<char*>(data_ptrs_[op_idx]) + 
                                  i * strides_[op_idx]);
                }
                
                // è°ƒç”¨ç”¨æˆ·å‡½æ•°
                f(ptrs.data());
            }
        });
    }
};

// 2. å†…æ ¸åˆ†å‘ç³»ç»Ÿ
class TORCH_API DispatchStub {
private:
    void* fn_CPU = nullptr;
    void* fn_CUDA = nullptr;
    void* fn_HIP = nullptr;
    
public:
    template <typename rT, typename T, typename... Args>
    rT call(DeviceType device_type, Args&&... args) {
        switch (device_type) {
            case DeviceType::CPU:
                return reinterpret_cast<rT(*)(Args...)>(fn_CPU)(std::forward<Args>(args)...);
            case DeviceType::CUDA:
                return reinterpret_cast<rT(*)(Args...)>(fn_CUDA)(std::forward<Args>(args)...);
            default:
                TORCH_CHECK(false, "Unsupported device type");
        }
    }
    
    void set_cuda_dispatch_ptr(void* fn) { fn_CUDA = fn; }
    void set_cpu_dispatch_ptr(void* fn) { fn_CPU = fn; }
};

// å†…æ ¸æ³¨å†Œå®
#define REGISTER_DISPATCH(name, fn) \
    static auto registry_##name = []() { \
        name.set_cpu_dispatch_ptr(reinterpret_cast<void*>(fn<CPUImpl>)); \
        name.set_cuda_dispatch_ptr(reinterpret_cast<void*>(fn<CUDAImpl>)); \
        return true; \
    }();

// 3. é«˜æ€§èƒ½CPUå†…æ ¸å®ç°ç¤ºä¾‹
namespace cpu_kernel {

// å‘é‡åŒ–çš„åŠ æ³•å†…æ ¸
template <typename T>
void add_kernel_impl(TensorIterator& iter, const Scalar& alpha_scalar) {
    using Vec = at::vec::Vectorized<T>;
    T alpha = alpha_scalar.to<T>();
    
    // å‘é‡åŒ–å¤„ç†
    iter.for_each([alpha](char** data) {
        T* out_ptr = reinterpret_cast<T*>(data[0]);
        const T* a_ptr = reinterpret_cast<const T*>(data[1]);
        const T* b_ptr = reinterpret_cast<const T*>(data[2]);
        
        // åŠ è½½å‘é‡
        Vec a_vec = Vec::loadu(a_ptr);
        Vec b_vec = Vec::loadu(b_ptr);
        
        // å‘é‡åŒ–è®¡ç®—ï¼ša + alpha * b
        Vec result = a_vec + Vec(alpha) * b_vec;
        
        // å­˜å‚¨ç»“æœ
        result.store(out_ptr);
    });
}

// çŸ©é˜µä¹˜æ³•å†…æ ¸ï¼šè°ƒç”¨ä¼˜åŒ–çš„BLASåº“
void mm_kernel_impl(const Tensor& result, const Tensor& self, const Tensor& mat2) {
    // è·å–ç»´åº¦
    int64_t m = self.size(0);
    int64_t k = self.size(1);
    int64_t n = mat2.size(1);
    
    // è°ƒç”¨MKLæˆ–OpenBLAS
    AT_DISPATCH_FLOATING_TYPES(self.scalar_type(), "mm_cpu", [&] {
        at::native::cpublas::gemm(
            TransposeType::NoTranspose, TransposeType::NoTranspose,
            n, m, k,
            static_cast<scalar_t>(1),
            mat2.data_ptr<scalar_t>(), n,
            self.data_ptr<scalar_t>(), k,
            static_cast<scalar_t>(0),
            result.data_ptr<scalar_t>(), n);
    });
}

}

// 4. CUDAå†…æ ¸å®ç°ç¤ºä¾‹
namespace cuda_kernel {

// CUDAå‘é‡åŠ æ³•å†…æ ¸
template <typename T>
__global__ void add_kernel_cuda(
    T* __restrict__ out,
    const T* __restrict__ a,
    const T* __restrict__ b,
    T alpha,
    int64_t numel) {
    
    int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    int64_t stride = blockDim.x * gridDim.x;
    
    // å‘é‡åŒ–è®¿é—®
    for (int64_t i = idx; i < numel; i += stride) {
        out[i] = a[i] + alpha * b[i];
    }
}

// å¯åŠ¨CUDAå†…æ ¸çš„åŒ…è£…å‡½æ•°
template <typename T>
void add_kernel_impl(TensorIterator& iter, const Scalar& alpha_scalar) {
    T alpha = alpha_scalar.to<T>();
    
    // è®¡ç®—ç½‘æ ¼é…ç½®
    int64_t numel = iter.numel();
    int64_t block_size = 256;
    int64_t grid_size = (numel + block_size - 1) / block_size;
    
    // é™åˆ¶ç½‘æ ¼å¤§å°
    grid_size = std::min(grid_size, static_cast<int64_t>(65535));
    
    // å¯åŠ¨å†…æ ¸
    auto stream = at::cuda::getCurrentCUDAStream();
    add_kernel_cuda<T><<<grid_size, block_size, 0, stream>>>(
        static_cast<T*>(iter.data_ptr(0)),
        static_cast<const T*>(iter.data_ptr(1)),
        static_cast<const T*>(iter.data_ptr(2)),
        alpha,
        numel);
    
    C10_CUDA_KERNEL_LAUNCH_CHECK();
}

}

// 5. å†…æ ¸æ³¨å†Œå’Œåˆ†å‘
DispatchStub add_stub;

REGISTER_DISPATCH(add_stub, [](TensorIterator& iter, const Scalar& alpha) {
    AT_DISPATCH_ALL_TYPES(iter.dtype(), "add", [&] {
        add_kernel_impl<scalar_t>(iter, alpha);
    });
});

}
```

### 3.4 PyTorchæ‰©å±•æœºåˆ¶æ·±åº¦åˆ†æ

#### è‡ªå®šä¹‰ç®—å­å¼€å‘æ¡†æ¶
```python
# PyTorchè‡ªå®šä¹‰ç®—å­å¼€å‘å®Œæ•´æŒ‡å—

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Function
from torch.utils.cpp_extension import load_inline

# 1. Pythonå®ç°çš„è‡ªå®šä¹‰å‡½æ•°
class SquaredReLU(Function):
    """
    è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°ï¼šSquared ReLU
    forward: f(x) = (max(0, x))Â²
    backward: f'(x) = 2 * max(0, x) * (x > 0)
    """
    
    @staticmethod
    def forward(ctx, input):
        # ä¿å­˜ç”¨äºåå‘ä¼ æ’­çš„å¼ é‡
        ctx.save_for_backward(input)
        
        # å‰å‘è®¡ç®—
        positive_mask = input > 0
        output = torch.where(positive_mask, input ** 2, torch.zeros_like(input))
        
        return output
    
    @staticmethod
    def backward(ctx, grad_output):
        # æ¢å¤ä¿å­˜çš„å¼ é‡
        input, = ctx.saved_tensors
        
        # è®¡ç®—æ¢¯åº¦
        positive_mask = input > 0
        grad_input = torch.where(positive_mask, 2 * input, torch.zeros_like(input))
        
        # é“¾å¼æ³•åˆ™
        grad_input = grad_output * grad_input
        
        return grad_input

# å‡½æ•°å¼æ¥å£
def squared_relu(input):
    return SquaredReLU.apply(input)

# 2. C++æ‰©å±•å®ç°é«˜æ€§èƒ½ç®—å­
cpp_source = """
#include <torch/extension.h>
#include <vector>

// CPUç‰ˆæœ¬å®ç°
torch::Tensor squared_relu_forward_cpu(torch::Tensor input) {
    auto output = torch::zeros_like(input);
    
    // è·å–æ•°æ®æŒ‡é’ˆ
    auto input_data = input.data_ptr<float>();
    auto output_data = output.data_ptr<float>();
    auto numel = input.numel();
    
    // å‘é‡åŒ–å¤„ç†
    #pragma omp parallel for
    for (int64_t i = 0; i < numel; i++) {
        float val = input_data[i];
        output_data[i] = val > 0 ? val * val : 0.0f;
    }
    
    return output;
}

torch::Tensor squared_relu_backward_cpu(torch::Tensor grad_output, torch::Tensor input) {
    auto grad_input = torch::zeros_like(input);
    
    auto grad_output_data = grad_output.data_ptr<float>();
    auto input_data = input.data_ptr<float>();
    auto grad_input_data = grad_input.data_ptr<float>();
    auto numel = input.numel();
    
    #pragma omp parallel for
    for (int64_t i = 0; i < numel; i++) {
        float val = input_data[i];
        grad_input_data[i] = val > 0 ? 2.0f * val * grad_output_data[i] : 0.0f;
    }
    
    return grad_input;
}

// CUDAç‰ˆæœ¬å£°æ˜
torch::Tensor squared_relu_forward_cuda(torch::Tensor input);
torch::Tensor squared_relu_backward_cuda(torch::Tensor grad_output, torch::Tensor input);

// åˆ†å‘å‡½æ•°
torch::Tensor squared_relu_forward(torch::Tensor input) {
    if (input.is_cuda()) {
        return squared_relu_forward_cuda(input);
    } else {
        return squared_relu_forward_cpu(input);
    }
}

torch::Tensor squared_relu_backward(torch::Tensor grad_output, torch::Tensor input) {
    if (input.is_cuda()) {
        return squared_relu_backward_cuda(grad_output, input);
    } else {
        return squared_relu_backward_cpu(grad_output, input);
    }
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("forward", &squared_relu_forward, "Squared ReLU forward");
    m.def("backward", &squared_relu_backward, "Squared ReLU backward");
}
"""

cuda_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// CUDAå†…æ ¸
__global__ void squared_relu_forward_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int64_t numel) {
    
    int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    int64_t stride = blockDim.x * gridDim.x;
    
    for (int64_t i = idx; i < numel; i += stride) {
        float val = input[i];
        output[i] = val > 0.0f ? val * val : 0.0f;
    }
}

__global__ void squared_relu_backward_kernel(
    const float* __restrict__ grad_output,
    const float* __restrict__ input,
    float* __restrict__ grad_input,
    int64_t numel) {
    
    int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    int64_t stride = blockDim.x * gridDim.x;
    
    for (int64_t i = idx; i < numel; i += stride) {
        float val = input[i];
        grad_input[i] = val > 0.0f ? 2.0f * val * grad_output[i] : 0.0f;
    }
}

torch::Tensor squared_relu_forward_cuda(torch::Tensor input) {
    auto output = torch::zeros_like(input);
    
    auto numel = input.numel();
    const int threads = 256;
    const int blocks = (numel + threads - 1) / threads;
    
    squared_relu_forward_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        numel);
    
    return output;
}

torch::Tensor squared_relu_backward_cuda(torch::Tensor grad_output, torch::Tensor input) {
    auto grad_input = torch::zeros_like(input);
    
    auto numel = input.numel();
    const int threads = 256;
    const int blocks = (numel + threads - 1) / threads;
    
    squared_relu_backward_kernel<<<blocks, threads>>>(
        grad_output.data_ptr<float>(),
        input.data_ptr<float>(),
        grad_input.data_ptr<float>(),
        numel);
    
    return grad_input;
}
"""

# 3. ç¼–è¯‘å’Œä½¿ç”¨C++æ‰©å±•
def load_cpp_extension():
    """
    åŠ¨æ€ç¼–è¯‘å¹¶åŠ è½½C++æ‰©å±•
    """
    try:
        # å°è¯•åŠ è½½CUDAç‰ˆæœ¬
        squared_relu_cpp = load_inline(
            name='squared_relu_cpp',
            cpp_sources=[cpp_source],
            cuda_sources=[cuda_source],
            verbose=True,
            extra_cflags=['-O3'],
            extra_cuda_cflags=['-O3', '--use_fast_math']
        )
    except:
        # å›é€€åˆ°CPUç‰ˆæœ¬
        squared_relu_cpp = load_inline(
            name='squared_relu_cpu',
            cpp_sources=[cpp_source],
            verbose=True,
            extra_cflags=['-O3', '-fopenmp']
        )
    
    return squared_relu_cpp

# 4. é«˜çº§è‡ªå®šä¹‰å‡½æ•°ï¼šæ”¯æŒC++åç«¯
class SquaredReLUCpp(Function):
    cpp_module = None
    
    @staticmethod
    def forward(ctx, input):
        if SquaredReLUCpp.cpp_module is None:
            SquaredReLUCpp.cpp_module = load_cpp_extension()
        
        ctx.save_for_backward(input)
        return SquaredReLUCpp.cpp_module.forward(input)
    
    @staticmethod
    def backward(ctx, grad_output):
        if SquaredReLUCpp.cpp_module is None:
            SquaredReLUCpp.cpp_module = load_cpp_extension()
        
        input, = ctx.saved_tensors
        return SquaredReLUCpp.cpp_module.backward(grad_output, input)

# 5. è‡ªå®šä¹‰å±‚å®ç°
class SquaredReLULayer(nn.Module):
    """
    å¯è®­ç»ƒçš„Squared ReLUå±‚
    """
    def __init__(self, use_cpp=True):
        super().__init__()
        self.use_cpp = use_cpp
        
        # å¯å­¦ä¹ å‚æ•°
        self.alpha = nn.Parameter(torch.tensor(1.0))
        self.beta = nn.Parameter(torch.tensor(0.0))
    
    def forward(self, x):
        # åº”ç”¨ç¼©æ”¾å’Œåç§»
        scaled_x = self.alpha * x + self.beta
        
        # é€‰æ‹©å®ç°
        if self.use_cpp:
            return SquaredReLUCpp.apply(scaled_x)
        else:
            return SquaredReLU.apply(scaled_x)

# 6. æ€§èƒ½åŸºå‡†æµ‹è¯•
def benchmark_implementations():
    """
    å¯¹æ¯”ä¸åŒå®ç°çš„æ€§èƒ½
    """
    import time
    
    # æµ‹è¯•æ•°æ®
    sizes = [(1000,), (10000,), (100000,), (1000000,)]
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"Benchmarking on {device}")
    print("Size\t\tPython\t\tC++\t\tSpeedup")
    print("-" * 50)
    
    for size in sizes:
        x = torch.randn(size, device=device, requires_grad=True)
        
        # Pythonå®ç°
        torch.cuda.synchronize() if device.type == 'cuda' else None
        start = time.time()
        for _ in range(100):
            y = squared_relu(x)
            loss = y.sum()
            loss.backward()
        torch.cuda.synchronize() if device.type == 'cuda' else None
        python_time = time.time() - start
        
        # C++å®ç°
        x.grad = None  # æ¸…é›¶æ¢¯åº¦
        torch.cuda.synchronize() if device.type == 'cuda' else None
        start = time.time()
        for _ in range(100):
            y = SquaredReLUCpp.apply(x)
            loss = y.sum()
            loss.backward()
        torch.cuda.synchronize() if device.type == 'cuda' else None
        cpp_time = time.time() - start
        
        speedup = python_time / cpp_time
        print(f"{size[0]}\t\t{python_time:.4f}s\t{cpp_time:.4f}s\t{speedup:.2f}x")

# 7. ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """
    è‡ªå®šä¹‰ç®—å­ä½¿ç”¨ç¤ºä¾‹
    """
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(10, 5, requires_grad=True)
    
    # ä½¿ç”¨è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°
    layer = SquaredReLULayer(use_cpp=True)
    y = layer(x)
    
    # åå‘ä¼ æ’­
    loss = y.sum()
    loss.backward()
    
    print("Input:", x)
    print("Output:", y)
    print("Gradient:", x.grad)
    print("Learnable parameters:", dict(layer.named_parameters()))

if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    example_usage()
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    # benchmark_implementations()
```

### 3.5 PyTorchæ¨¡å‹ä¼˜åŒ–å’Œéƒ¨ç½²

#### JITç¼–è¯‘å’Œå›¾ä¼˜åŒ–
```python
# PyTorch JITç¼–è¯‘å’Œå›¾ä¼˜åŒ–æ·±åº¦åˆ†æ

import torch
import torch.jit as jit
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, List

# 1. TorchScriptåŸºç¡€ï¼šå°†Pythonä»£ç è½¬æ¢ä¸ºå¯ä¼˜åŒ–çš„å›¾è¡¨ç¤º
class OptimizedModel(nn.Module):
    """
    å¯è¢«JITç¼–è¯‘ä¼˜åŒ–çš„æ¨¡å‹ç¤ºä¾‹
    """
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        super().__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, hidden_size)  
        self.linear3 = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ä½¿ç”¨ç±»å‹æ³¨è§£æé«˜JITæ€§èƒ½
        x = F.relu(self.linear1(x))
        x = self.dropout(x)
        x = F.relu(self.linear2(x))
        x = self.dropout(x)
        x = self.linear3(x)
        return x

# 2. å¤šç§JITç¼–è¯‘æ–¹å¼
def demonstrate_jit_compilation():
    """
    æ¼”ç¤ºä¸åŒçš„JITç¼–è¯‘æ–¹æ³•
    """
    model = OptimizedModel(784, 256, 10)
    example_input = torch.randn(32, 784)
    
    # æ–¹æ³•1ï¼štorch.jit.script - é™æ€åˆ†æPythonä»£ç 
    print("=== Script Compilation ===")
    scripted_model = torch.jit.script(model)
    
    # æ–¹æ³•2ï¼štorch.jit.trace - è®°å½•æ‰§è¡Œè½¨è¿¹
    print("=== Trace Compilation ===")
    traced_model = torch.jit.trace(model, example_input)
    
    # æ–¹æ³•3ï¼šæ··åˆæ¨¡å¼ - éƒ¨åˆ†scriptï¼Œéƒ¨åˆ†trace
    print("=== Hybrid Compilation ===")
    
    @torch.jit.script
    def fused_activation(x: torch.Tensor) -> torch.Tensor:
        # èåˆReLU + Dropout
        return F.dropout(F.relu(x), 0.1, training=False)
    
    class HybridModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(784, 256)
            self.linear2 = nn.Linear(256, 10)
        
        def forward(self, x):
            x = self.linear1(x)
            x = fused_activation(x)  # ä½¿ç”¨JITç¼–è¯‘çš„å‡½æ•°
            x = self.linear2(x)
            return x
    
    hybrid_model = torch.jit.script(HybridModel())
    
    return scripted_model, traced_model, hybrid_model

# 3. å›¾ä¼˜åŒ–åˆ†æ
def analyze_graph_optimizations():
    """
    åˆ†æTorchScriptçš„å›¾ä¼˜åŒ–è¿‡ç¨‹
    """
    
    # åˆ›å»ºåŒ…å«ä¼˜åŒ–æœºä¼šçš„æ¨¡å‹
    class OptimizableModel(nn.Module):
        def forward(self, x):
            # å¤šä¸ªè¿ç»­çš„çº¿æ€§å˜æ¢ - å¯ä»¥èåˆ
            x = x + 1
            x = x * 2
            x = x - 0.5
            
            # å¸¸é‡æŠ˜å æœºä¼š
            y = torch.tensor([1.0, 2.0, 3.0])
            z = y * 2 + 1  # å¯ä»¥åœ¨ç¼–è¯‘æ—¶è®¡ç®—
            
            # æ­»ä»£ç æ¶ˆé™¤
            unused = x * 10  # å¦‚æœæœªä½¿ç”¨ï¼Œä¼šè¢«åˆ é™¤
            
            return x + z.sum()
    
    model = OptimizableModel()
    example_input = torch.randn(5)
    
    # è·å–åŸå§‹å›¾
    traced_model = torch.jit.trace(model, example_input)
    print("Original graph:")
    print(traced_model.graph)
    
    # åº”ç”¨ä¼˜åŒ–
    optimized_model = torch.jit.optimize_for_inference(traced_model)
    print("\nOptimized graph:")
    print(optimized_model.graph)
    
    # åˆ†æä¼˜åŒ–æ•ˆæœ
    analyze_optimization_effects(traced_model, optimized_model, example_input)

def analyze_optimization_effects(original_model, optimized_model, input_tensor):
    """
    åˆ†æä¼˜åŒ–æ•ˆæœ
    """
    import time
    
    # é¢„çƒ­
    for _ in range(100):
        _ = original_model(input_tensor)
        _ = optimized_model(input_tensor)
    
    # æ€§èƒ½æµ‹è¯•
    iterations = 1000
    
    # åŸå§‹æ¨¡å‹
    torch.cuda.synchronize() if input_tensor.is_cuda else None
    start = time.time()
    for _ in range(iterations):
        _ = original_model(input_tensor)
    torch.cuda.synchronize() if input_tensor.is_cuda else None
    original_time = time.time() - start
    
    # ä¼˜åŒ–æ¨¡å‹
    torch.cuda.synchronize() if input_tensor.is_cuda else None
    start = time.time()
    for _ in range(iterations):
        _ = optimized_model(input_tensor)
    torch.cuda.synchronize() if input_tensor.is_cuda else None
    optimized_time = time.time() - start
    
    speedup = original_time / optimized_time
    print(f"Original time: {original_time:.4f}s")
    print(f"Optimized time: {optimized_time:.4f}s")
    print(f"Speedup: {speedup:.2f}x")

# 4. é«˜çº§ä¼˜åŒ–æŠ€æœ¯
class AdvancedOptimizations:
    """
    é«˜çº§ä¼˜åŒ–æŠ€æœ¯é›†åˆ
    """
    
    @staticmethod
    def operator_fusion_example():
        """
        ç®—å­èåˆç¤ºä¾‹
        """
        @torch.jit.script
        def fused_conv_bn_relu(x: torch.Tensor, weight: torch.Tensor, 
                              bias: torch.Tensor, bn_weight: torch.Tensor,
                              bn_bias: torch.Tensor, bn_mean: torch.Tensor,
                              bn_var: torch.Tensor) -> torch.Tensor:
            # å·ç§¯
            x = F.conv2d(x, weight, bias)
            
            # æ‰¹å½’ä¸€åŒ–  
            x = F.batch_norm(x, bn_mean, bn_var, bn_weight, bn_bias, training=False)
            
            # ReLUæ¿€æ´»
            x = F.relu(x)
            
            return x
        
        return fused_conv_bn_relu
    
    @staticmethod
    def memory_optimization_example():
        """
        å†…å­˜ä¼˜åŒ–ç¤ºä¾‹
        """
        @torch.jit.script
        def memory_efficient_attention(q: torch.Tensor, k: torch.Tensor, 
                                     v: torch.Tensor) -> torch.Tensor:
            # ä½¿ç”¨checkpointå‡å°‘å†…å­˜ä½¿ç”¨
            def attention_forward(q, k, v):
                scores = torch.matmul(q, k.transpose(-2, -1))
                scores = F.softmax(scores, dim=-1)
                output = torch.matmul(scores, v)
                return output
            
            # åœ¨JITä¸­æ¨¡æ‹Ÿcheckpoint
            return attention_forward(q, k, v)
        
        return memory_efficient_attention
    
    @staticmethod
    def quantization_aware_optimization():
        """
        é‡åŒ–æ„ŸçŸ¥ä¼˜åŒ–
        """
        class QuantizableModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.quant = torch.quantization.QuantStub()
                self.linear = nn.Linear(10, 5)
                self.dequant = torch.quantization.DeQuantStub()
            
            def forward(self, x):
                x = self.quant(x)
                x = self.linear(x)
                x = self.dequant(x)
                return x
        
        model = QuantizableModel()
        
        # é…ç½®é‡åŒ–
        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # å‡†å¤‡é‡åŒ–
        prepared_model = torch.quantization.prepare(model)
        
        # æ ¡å‡†æ•°æ®ï¼ˆç®€åŒ–ï¼‰
        calibration_data = torch.randn(100, 10)
        with torch.no_grad():
            for data in calibration_data:
                prepared_model(data.unsqueeze(0))
        
        # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        quantized_model = torch.quantization.convert(prepared_model)
        
        # JITç¼–è¯‘é‡åŒ–æ¨¡å‹
        jit_quantized = torch.jit.script(quantized_model)
        
        return jit_quantized

# 5. æ€§èƒ½åˆ†æå’Œè°ƒè¯•å·¥å…·
class PerformanceProfiler:
    """
    æ€§èƒ½åˆ†æå·¥å…·
    """
    
    @staticmethod
    def profile_model_execution(model, input_tensor, num_iterations=100):
        """
        è¯¦ç»†çš„æ¨¡å‹æ€§èƒ½åˆ†æ
        """
        # ä½¿ç”¨PyTorch Profiler
        with torch.profiler.profile(
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        ) as prof:
            for _ in range(num_iterations):
                output = model(input_tensor)
                if input_tensor.requires_grad:
                    output.sum().backward()
        
        # æ‰“å°æ€§èƒ½æŠ¥å‘Š
        print(prof.key_averages().table(sort_by="cuda_time_total"))
        
        # å¯¼å‡ºChromeè·Ÿè¸ªæ–‡ä»¶
        prof.export_chrome_trace("model_trace.json")
        
        return prof
    
    @staticmethod
    def memory_profiling(model, input_tensor):
        """
        å†…å­˜ä½¿ç”¨åˆ†æ
        """
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            
            # æ‰§è¡Œæ¨¡å‹
            output = model(input_tensor)
            
            # è·å–å†…å­˜ç»Ÿè®¡
            memory_stats = torch.cuda.memory_stats()
            peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
            
            print(f"Peak memory usage: {peak_memory:.2f} MB")
            print(f"Memory allocations: {memory_stats['num_alloc_retries']}")
            print(f"Memory deallocations: {memory_stats['num_ooms']}")

# 6. å®Œæ•´çš„ä¼˜åŒ–æµæ°´çº¿
class OptimizationPipeline:
    """
    å®Œæ•´çš„æ¨¡å‹ä¼˜åŒ–æµæ°´çº¿
    """
    
    def __init__(self, model: nn.Module):
        self.original_model = model
        self.optimized_model = None
    
    def apply_optimizations(self, example_input: torch.Tensor, 
                          optimization_level: str = "O2") -> nn.Module:
        """
        åº”ç”¨å®Œæ•´çš„ä¼˜åŒ–æµæ°´çº¿
        """
        print("Starting optimization pipeline...")
        
        # é˜¶æ®µ1ï¼šJITç¼–è¯‘
        print("Stage 1: JIT Compilation")
        jit_model = torch.jit.trace(self.original_model, example_input)
        
        # é˜¶æ®µ2ï¼šå›¾ä¼˜åŒ–
        print("Stage 2: Graph Optimization")
        if optimization_level == "O1":
            # åŸºç¡€ä¼˜åŒ–
            optimized_model = jit_model
        elif optimization_level == "O2":
            # æ ‡å‡†ä¼˜åŒ–
            optimized_model = torch.jit.optimize_for_inference(jit_model)
        elif optimization_level == "O3":
            # æ¿€è¿›ä¼˜åŒ–
            optimized_model = torch.jit.optimize_for_inference(jit_model)
            # å¯ä»¥æ·»åŠ æ›´å¤šè‡ªå®šä¹‰ä¼˜åŒ–
        
        # é˜¶æ®µ3ï¼šç®—å­èåˆ
        print("Stage 3: Operator Fusion")
        # TorchScriptä¼šè‡ªåŠ¨è¿›è¡Œç®—å­èåˆ
        
        # é˜¶æ®µ4ï¼šå†…å­˜ä¼˜åŒ–
        print("Stage 4: Memory Optimization")
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ä»¥åº”ç”¨å†…å­˜å‹ç¼©ç­‰æŠ€æœ¯
        
        self.optimized_model = optimized_model
        print("Optimization pipeline completed!")
        
        return optimized_model
    
    def benchmark_improvements(self, example_input: torch.Tensor):
        """
        åŸºå‡†æµ‹è¯•ä¼˜åŒ–æ•ˆæœ
        """
        if self.optimized_model is None:
            raise ValueError("Please run apply_optimizations first")
        
        print("\n=== Performance Comparison ===")
        
        # åŸå§‹æ¨¡å‹
        profiler = PerformanceProfiler()
        print("Original model performance:")
        profiler.profile_model_execution(self.original_model, example_input)
        
        # ä¼˜åŒ–æ¨¡å‹
        print("\nOptimized model performance:")
        profiler.profile_model_execution(self.optimized_model, example_input)
        
        # å†…å­˜ä½¿ç”¨å¯¹æ¯”
        print("\nMemory usage comparison:")
        print("Original model:")
        profiler.memory_profiling(self.original_model, example_input)
        print("Optimized model:")
        profiler.memory_profiling(self.optimized_model, example_input)

# ä½¿ç”¨ç¤ºä¾‹
def main():
    """
    PyTorchä¼˜åŒ–å®Œæ•´ç¤ºä¾‹
    """
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = OptimizedModel(784, 512, 10)
    example_input = torch.randn(64, 784)
    
    # åº”ç”¨ä¼˜åŒ–æµæ°´çº¿
    pipeline = OptimizationPipeline(model)
    optimized_model = pipeline.apply_optimizations(example_input, "O2")
    
    # æ€§èƒ½å¯¹æ¯”
    pipeline.benchmark_improvements(example_input)
    
    # ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
    torch.jit.save(optimized_model, "optimized_model.pt")
    print("Optimized model saved to optimized_model.pt")

if __name__ == "__main__":
    main()
```

é€šè¿‡æ·±å…¥åˆ†æPyTorch ATençš„åŠ¨æ€å›¾ç®—å­å®ç°ï¼Œæˆ‘ä»¬æŒæ¡äº†ï¼š

1. **å¼ é‡åº“æ¶æ„**ï¼šATençš„åˆ†å‘æœºåˆ¶å’Œå†…å­˜ç®¡ç†
2. **è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿ**ï¼šAutogradçš„å·¥ä½œåŸç†å’Œå®ç°
3. **é«˜æ€§èƒ½å†…æ ¸**ï¼šTensorIteratorå’Œå†…æ ¸ä¼˜åŒ–æŠ€æœ¯
4. **æ‰©å±•æœºåˆ¶**ï¼šè‡ªå®šä¹‰ç®—å­å¼€å‘çš„å®Œæ•´æµç¨‹
5. **æ¨¡å‹ä¼˜åŒ–**ï¼šJITç¼–è¯‘å’Œå›¾ä¼˜åŒ–æŠ€æœ¯

è¿™äº›çŸ¥è¯†ä¸ºæˆ‘ä»¬å¼€å‘AIèŠ¯ç‰‡ç®—å­æä¾›äº†é‡è¦çš„å‚è€ƒæ¡†æ¶ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬ç»§ç»­ç ”ç©¶TVMçš„ç¼–è¯‘å™¨ä¼˜åŒ–æŠ€æœ¯ã€‚

---

## ç¬¬å››éƒ¨åˆ†ï¼šTVMç¼–è¯‘å™¨ä¼˜åŒ–æŠ€æœ¯æ·±åº¦åˆ†æ

### 4.1 TVMç¼–è¯‘å™¨æ¶æ„æ ¸å¿ƒè§£æ

#### TVMçš„ç¼–è¯‘æµæ°´çº¿å’ŒIRè®¾è®¡
```python
# TVMç¼–è¯‘å™¨æ ¸å¿ƒæ¶æ„æ·±åº¦åˆ†æ

import tvm
from tvm import relay, te, tir, auto_scheduler
from tvm.contrib import graph_executor
import numpy as np

# 1. TVMçš„å¤šå±‚IR (Intermediate Representation) æ¶æ„
class TVMCompilerStack:
    """
    TVMç¼–è¯‘å™¨æ ˆçš„å®Œæ•´åˆ†æ
    """
    
    def __init__(self):
        self.target = tvm.target.Target("llvm")  # å¯ä»¥æ˜¯cuda, openclç­‰
        
    def demonstrate_ir_levels(self):
        """
        æ¼”ç¤ºTVMçš„å¤šå±‚IRè½¬æ¢è¿‡ç¨‹
        """
        print("=== TVMå¤šå±‚IRç¼–è¯‘æµæ°´çº¿ ===")
        
        # Level 1: Relay IR - é«˜çº§å›¾è¡¨ç¤º
        print("\n1. Relay IR (é«˜çº§å›¾è¡¨ç¤º)")
        relay_func = self.create_relay_function()
        print("Relay Function:")
        print(relay_func)
        
        # Level 2: Tensor Expression (TE) - ç®—å­çº§è¡¨ç¤º  
        print("\n2. Tensor Expression (ç®—å­çº§è¡¨ç¤º)")
        te_schedule = self.create_te_computation()
        print("TE Computation created")
        
        # Level 3: TensorIR (TIR) - ä½çº§å¾ªç¯è¡¨ç¤º
        print("\n3. TensorIR (ä½çº§å¾ªç¯è¡¨ç¤º)")
        tir_func = self.create_tir_function()
        print("TIR Function:")
        print(tir_func.script())
        
        # Level 4: Target Code - ç›®æ ‡ä»£ç 
        print("\n4. Target Code Generation")
        self.generate_target_code(te_schedule)
        
    def create_relay_function(self):
        """
        åˆ›å»ºRelayå‡½æ•°ç¤ºä¾‹ï¼šçŸ©é˜µä¹˜æ³• + åç½® + ReLU
        """
        # å®šä¹‰è¾“å…¥
        x = relay.var("x", shape=(1, 784), dtype="float32")
        w = relay.var("w", shape=(784, 128), dtype="float32") 
        b = relay.var("b", shape=(128,), dtype="float32")
        
        # æ„å»ºè®¡ç®—å›¾
        dense = relay.nn.dense(x, w)           # çŸ©é˜µä¹˜æ³•
        bias_add = relay.nn.bias_add(dense, b) # åŠ åç½®
        relu = relay.nn.relu(bias_add)         # ReLUæ¿€æ´»
        
        # åˆ›å»ºå‡½æ•°
        func = relay.Function([x, w, b], relu)
        return func
    
    def create_te_computation(self):
        """
        åˆ›å»ºTensor Expressionè®¡ç®—ç¤ºä¾‹
        """
        # å®šä¹‰è®¡ç®—ç»´åº¦
        batch, in_dim, out_dim = 1, 784, 128
        
        # å®šä¹‰å ä½ç¬¦
        X = te.placeholder((batch, in_dim), name="X", dtype="float32")
        W = te.placeholder((in_dim, out_dim), name="W", dtype="float32")
        B = te.placeholder((out_dim,), name="B", dtype="float32")
        
        # å®šä¹‰è®¡ç®—ï¼šçŸ©é˜µä¹˜æ³•
        k = te.reduce_axis((0, in_dim), name="k")
        MatMul = te.compute(
            (batch, out_dim),
            lambda i, j: te.sum(X[i, k] * W[k, j], axis=k),
            name="MatMul"
        )
        
        # åŠ åç½®å’ŒReLU
        BiasAdd = te.compute(
            (batch, out_dim),
            lambda i, j: MatMul[i, j] + B[j],
            name="BiasAdd"
        )
        
        ReLU = te.compute(
            (batch, out_dim),
            lambda i, j: te.max(BiasAdd[i, j], 0),
            name="ReLU"
        )
        
        # åˆ›å»ºè°ƒåº¦
        s = te.create_schedule(ReLU.op)
        
        return s, [X, W, B, ReLU]
    
    def create_tir_function(self):
        """
        åˆ›å»ºTensorIRå‡½æ•°ç¤ºä¾‹ï¼šæ‰‹å·¥ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•
        """
        @tvm.script.ir_module
        class OptimizedMatMul:
            @tir.prim_func
            def main(A: tir.Buffer[(1024, 1024), "float32"],
                    B: tir.Buffer[(1024, 1024), "float32"], 
                    C: tir.Buffer[(1024, 1024), "float32"]) -> None:
                # åˆ†å—å¤§å°
                block_size = 64
                
                # å¤–å±‚åˆ†å—å¾ªç¯
                for i_outer in tir.parallel(16):  # 1024 / 64 = 16
                    for j_outer in range(16):
                        for k_outer in range(16):
                            # å†…å±‚åˆ†å—è®¡ç®—
                            for i_inner in range(block_size):
                                for j_inner in range(block_size):
                                    for k_inner in tir.vectorize(8):  # å‘é‡åŒ–
                                        i = i_outer * block_size + i_inner
                                        j = j_outer * block_size + j_inner  
                                        k = k_outer * block_size + k_inner
                                        
                                        if k == 0:
                                            C[i, j] = 0.0
                                        C[i, j] = C[i, j] + A[i, k] * B[k, j]
        
        return OptimizedMatMul
    
    def generate_target_code(self, schedule_info):
        """
        ç”Ÿæˆç›®æ ‡ä»£ç 
        """
        s, args = schedule_info
        
        # ç¼–è¯‘ä¸ºå¯æ‰§è¡Œå‡½æ•°
        func = tvm.build(s, args, target=self.target, name="optimized_dense")
        
        # æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
        print("Generated Code:")
        print(func.get_source())
        
        return func

# 2. TVMçš„è‡ªåŠ¨è°ƒä¼˜ç³»ç»Ÿ (AutoTVM/AutoScheduler)
class TVMAutoTuning:
    """
    TVMè‡ªåŠ¨è°ƒä¼˜ç³»ç»Ÿæ·±åº¦åˆ†æ
    """
    
    def __init__(self):
        self.target = tvm.target.Target("llvm")
        self.log_file = "tuning_log.json"
    
    def auto_schedule_example(self):
        """
        AutoSchedulerä½¿ç”¨ç¤ºä¾‹
        """
        print("=== AutoSchedulerè‡ªåŠ¨è°ƒä¼˜ ===")
        
        # å®šä¹‰è®¡ç®—ä»»åŠ¡
        @auto_scheduler.register_workload
        def matmul_add(M, N, K, dtype):
            A = te.placeholder((M, K), name="A", dtype=dtype)
            B = te.placeholder((K, N), name="B", dtype=dtype)
            C = te.placeholder((M, N), name="C", dtype=dtype)
            
            # çŸ©é˜µä¹˜æ³•
            k = te.reduce_axis((0, K), name="k")
            matmul = te.compute(
                (M, N),
                lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),
                name="matmul"
            )
            
            # åŠ æ³•
            out = te.compute(
                (M, N),
                lambda i, j: matmul[i, j] + C[i, j],
                name="out"
            )
            
            return [A, B, C, out]
        
        # åˆ›å»ºè°ƒä¼˜ä»»åŠ¡
        task = auto_scheduler.SearchTask(
            func=matmul_add,
            args=(1024, 1024, 1024, "float32"),
            target=self.target
        )
        
        # æœç´¢ç­–ç•¥é…ç½®
        search_policy = auto_scheduler.SketchPolicy(
            task,
            program_cost_model=auto_scheduler.XGBModel(),
            params={
                "eps_greedy": 0.05,
                "evolutionary_search_population": 2048,
            }
        )
        
        # è¿è¡Œè°ƒä¼˜
        tune_option = auto_scheduler.TuningOptions(
            num_measure_trials=1000,  # å®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤š
            runner=auto_scheduler.LocalRunner(repeat=3, enable_cpu_cache_flush=True),
            measure_callbacks=[auto_scheduler.RecordToFile(self.log_file)]
        )
        
        print("å¼€å§‹è‡ªåŠ¨è°ƒä¼˜...")
        task.tune(tune_option, search_policy)
        
        # åº”ç”¨æœ€ä½³è°ƒåº¦
        sch, args = task.apply_best(self.log_file)
        
        print("æœ€ä½³è°ƒåº¦å·²åº”ç”¨")
        return sch, args
    
    def custom_search_space(self):
        """
        è‡ªå®šä¹‰æœç´¢ç©ºé—´ç¤ºä¾‹
        """
        @auto_scheduler.register_workload
        def conv2d_custom(N, H, W, CI, CO, KH, KW, stride, padding):
            data = te.placeholder((N, CI, H, W), name="data")
            kernel = te.placeholder((CO, CI, KH, KW), name="kernel")
            
            # å·ç§¯è®¡ç®—
            OH = (H + 2 * padding - KH) // stride + 1
            OW = (W + 2 * padding - KW) // stride + 1
            
            rc = te.reduce_axis((0, CI), name="rc")
            ry = te.reduce_axis((0, KH), name="ry")
            rx = te.reduce_axis((0, KW), name="rx")
            
            conv = te.compute(
                (N, CO, OH, OW),
                lambda n, f, y, x: te.sum(
                    data[n, rc, y * stride + ry - padding, x * stride + rx - padding] *
                    kernel[f, rc, ry, rx],
                    axis=[rc, ry, rx]
                ),
                name="conv2d"
            )
            
            return [data, kernel, conv]
        
        # åˆ›å»ºä»»åŠ¡å¹¶å®šä¹‰æœç´¢ç©ºé—´
        task = auto_scheduler.SearchTask(
            func=conv2d_custom,
            args=(1, 224, 224, 3, 64, 7, 7, 2, 3),
            target=self.target
        )
        
        return task

# 3. TVMçš„å›¾ä¼˜åŒ–æŠ€æœ¯
class TVMGraphOptimization:
    """
    TVMå›¾çº§ä¼˜åŒ–æŠ€æœ¯åˆ†æ
    """
    
    def __init__(self):
        self.ctx = tvm.cpu(0)
    
    def relay_optimization_passes(self):
        """
        Relayä¼˜åŒ–Passè¯¦è§£
        """
        print("=== Relayå›¾ä¼˜åŒ–Pass ===")
        
        # åˆ›å»ºç¤ºä¾‹æ¨¡å‹
        def create_model():
            x = relay.var("x", shape=(1, 3, 224, 224))
            
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            conv1 = relay.nn.conv2d(x, relay.var("w1"), kernel_size=(3, 3), padding=(1, 1))
            bn1 = relay.nn.batch_norm(conv1, relay.var("gamma1"), relay.var("beta1"), 
                                    relay.var("mean1"), relay.var("var1"))[0]
            relu1 = relay.nn.relu(bn1)
            
            # ç¬¬äºŒä¸ªå·ç§¯å—
            conv2 = relay.nn.conv2d(relu1, relay.var("w2"), kernel_size=(3, 3), padding=(1, 1))
            bn2 = relay.nn.batch_norm(conv2, relay.var("gamma2"), relay.var("beta2"),
                                    relay.var("mean2"), relay.var("var2"))[0]
            relu2 = relay.nn.relu(bn2)
            
            # å…¨å±€å¹³å‡æ± åŒ–å’Œåˆ†ç±»
            pool = relay.nn.global_avg_pool2d(relu2)
            flat = relay.nn.batch_flatten(pool)
            dense = relay.nn.dense(flat, relay.var("w3"))
            
            return relay.Function(relay.analysis.free_vars(dense), dense)
        
        # åŸå§‹æ¨¡å‹
        original_func = create_model()
        print("åŸå§‹æ¨¡å‹èŠ‚ç‚¹æ•°:", len(relay.analysis.post_order_visit(original_func, lambda x: x)))
        
        # åº”ç”¨ä¼˜åŒ–Pass
        with tvm.transform.PassContext(opt_level=3):
            # 1. ç®—å­èåˆ
            fused_func = relay.transform.FuseOps(fuse_opt_level=2)(
                tvm.IRModule.from_expr(original_func))
            
            # 2. å¸¸é‡æŠ˜å 
            folded_func = relay.transform.FoldConstant()(fused_func)
            
            # 3. æ­»ä»£ç æ¶ˆé™¤
            eliminated_func = relay.transform.DeadCodeElimination()(folded_func)
            
            # 4. å…¬å…±å­è¡¨è¾¾å¼æ¶ˆé™¤
            cse_func = relay.transform.EliminateCommonSubexpr()(eliminated_func)
            
            # 5. å¸ƒå±€è½¬æ¢ä¼˜åŒ–
            layout_func = relay.transform.ConvertLayout("NCHW")(cse_func)
        
        optimized_func = layout_func["main"]
        print("ä¼˜åŒ–åæ¨¡å‹èŠ‚ç‚¹æ•°:", len(relay.analysis.post_order_visit(optimized_func, lambda x: x)))
        
        return original_func, optimized_func
    
    def operator_fusion_analysis(self):
        """
        æ·±å…¥åˆ†æç®—å­èåˆæœºåˆ¶
        """
        print("\n=== ç®—å­èåˆæ·±åº¦åˆ†æ ===")
        
        # å¯èåˆçš„æ¨¡å¼ç¤ºä¾‹
        x = relay.var("x", shape=(1, 128))
        w = relay.var("w", shape=(128, 64))
        b = relay.var("b", shape=(64,))
        
        # Linear + Bias + ReLU èåˆæ¨¡å¼
        dense = relay.nn.dense(x, w)
        bias_add = relay.nn.bias_add(dense, b)
        relu = relay.nn.relu(bias_add)
        
        func = relay.Function([x, w, b], relu)
        
        # åˆ†æèåˆå‰åçš„å·®å¼‚
        print("èåˆå‰:")
        print(func)
        
        # åº”ç”¨èåˆ
        fused_mod = relay.transform.FuseOps(fuse_opt_level=2)(
            tvm.IRModule.from_expr(func))
        
        print("èåˆå:")
        print(fused_mod["main"])
        
        return func, fused_mod

# 4. TVMä»£ç ç”Ÿæˆå’Œä¼˜åŒ–æŠ€æœ¯
class TVMCodeGeneration:
    """
    TVMä»£ç ç”Ÿæˆä¼˜åŒ–æŠ€æœ¯
    """
    
    def __init__(self):
        self.target = tvm.target.Target("llvm -mcpu=core-avx2")
    
    def schedule_optimization_examples(self):
        """
        è°ƒåº¦ä¼˜åŒ–ç¤ºä¾‹é›†åˆ
        """
        print("=== è°ƒåº¦ä¼˜åŒ–æŠ€æœ¯ ===")
        
        # åŸºç¡€çŸ©é˜µä¹˜æ³•
        M, N, K = 1024, 1024, 1024
        A = te.placeholder((M, K), name="A")
        B = te.placeholder((K, N), name="B")
        
        k = te.reduce_axis((0, K), name="k")
        C = te.compute((M, N), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name="C")
        
        # 1. åŸºç¡€è°ƒåº¦
        s1 = te.create_schedule(C.op)
        self.benchmark_schedule("åŸºç¡€è°ƒåº¦", s1, [A, B, C])
        
        # 2. åˆ†å—ä¼˜åŒ–
        s2 = te.create_schedule(C.op)
        xo, yo, xi, yi = s2[C].tile(C.op.axis[0], C.op.axis[1], 32, 32)
        ko, ki = s2[C].split(k, factor=4)
        s2[C].reorder(xo, yo, ko, xi, yi, ki)
        self.benchmark_schedule("åˆ†å—ä¼˜åŒ–", s2, [A, B, C])
        
        # 3. å‘é‡åŒ–
        s3 = te.create_schedule(C.op)
        xo, yo, xi, yi = s3[C].tile(C.op.axis[0], C.op.axis[1], 32, 32)
        ko, ki = s3[C].split(k, factor=4)
        s3[C].reorder(xo, yo, ko, xi, yi, ki)
        s3[C].vectorize(yi)
        self.benchmark_schedule("å‘é‡åŒ–ä¼˜åŒ–", s3, [A, B, C])
        
        # 4. å¹¶è¡ŒåŒ–
        s4 = te.create_schedule(C.op)
        xo, yo, xi, yi = s4[C].tile(C.op.axis[0], C.op.axis[1], 32, 32)
        ko, ki = s4[C].split(k, factor=4)
        s4[C].reorder(xo, yo, ko, xi, yi, ki)
        s4[C].vectorize(yi)
        s4[C].parallel(xo)
        self.benchmark_schedule("å¹¶è¡ŒåŒ–ä¼˜åŒ–", s4, [A, B, C])
        
        # 5. å±•å¼€ä¼˜åŒ–
        s5 = te.create_schedule(C.op)
        xo, yo, xi, yi = s5[C].tile(C.op.axis[0], C.op.axis[1], 32, 32)
        ko, ki = s5[C].split(k, factor=4)
        s5[C].reorder(xo, yo, ko, xi, yi, ki)
        s5[C].vectorize(yi)
        s5[C].parallel(xo)
        s5[C].unroll(ki)
        self.benchmark_schedule("å±•å¼€ä¼˜åŒ–", s5, [A, B, C])
    
    def benchmark_schedule(self, name, schedule, args):
        """
        åŸºå‡†æµ‹è¯•è°ƒåº¦æ€§èƒ½
        """
        try:
            func = tvm.build(schedule, args, target=self.target)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            ctx = tvm.cpu(0)
            a_np = np.random.uniform(size=(1024, 1024)).astype(np.float32)
            b_np = np.random.uniform(size=(1024, 1024)).astype(np.float32)
            
            a_tvm = tvm.nd.array(a_np, ctx)
            b_tvm = tvm.nd.array(b_np, ctx)
            c_tvm = tvm.nd.array(np.zeros((1024, 1024), dtype=np.float32), ctx)
            
            # æ€§èƒ½æµ‹è¯•
            evaluator = func.time_evaluator(func.entry_name, ctx, number=10)
            time_cost = evaluator(a_tvm, b_tvm, c_tvm).mean
            
            print(f"{name}: {time_cost:.6f} ç§’")
            
        except Exception as e:
            print(f"{name}: ç¼–è¯‘å¤±è´¥ - {e}")
    
    def memory_optimization_techniques(self):
        """
        å†…å­˜ä¼˜åŒ–æŠ€æœ¯
        """
        print("\n=== å†…å­˜ä¼˜åŒ–æŠ€æœ¯ ===")
        
        # ç¤ºä¾‹ï¼šå¸¦å†…å­˜å±‚æ¬¡çš„å·ç§¯ä¼˜åŒ–
        N, CI, H, W = 1, 128, 56, 56
        CO, KH, KW = 128, 3, 3
        
        data = te.placeholder((N, CI, H, W), name="data")
        kernel = te.placeholder((CO, CI, KH, KW), name="kernel")
        
        # å·ç§¯è®¡ç®—
        OH = H - KH + 1
        OW = W - KW + 1
        
        rc = te.reduce_axis((0, CI), name="rc")
        ry = te.reduce_axis((0, KH), name="ry")  
        rx = te.reduce_axis((0, KW), name="rx")
        
        conv = te.compute(
            (N, CO, OH, OW),
            lambda n, f, y, x: te.sum(
                data[n, rc, y + ry, x + rx] * kernel[f, rc, ry, rx],
                axis=[rc, ry, rx]
            ),
            name="conv"
        )
        
        s = te.create_schedule(conv.op)
        
        # å†…å­˜å±‚æ¬¡ä¼˜åŒ–
        # 1. ç¼“å­˜è¾“å…¥æ•°æ®
        data_shared = s.cache_read(data, "shared", [conv])
        kernel_shared = s.cache_read(kernel, "shared", [conv])
        
        # 2. ç¼“å­˜è¾“å‡º
        conv_local = s.cache_write(conv, "local")
        
        # 3. åˆ†å—å’Œè°ƒåº¦
        n, f, y, x = s[conv].op.axis
        rc, ry, rx = s[conv].op.reduce_axis
        
        # å¤–å±‚åˆ†å—
        bf, vf = s[conv].split(f, factor=16)
        by, vy = s[conv].split(y, factor=8)
        bx, vx = s[conv].split(x, factor=8)
        
        s[conv].reorder(n, bf, by, bx, vf, vy, vx, rc, ry, rx)
        
        # æ•°æ®ç§»åŠ¨è°ƒåº¦
        s[data_shared].compute_at(s[conv], bx)
        s[kernel_shared].compute_at(s[conv], bx)
        s[conv_local].compute_at(s[conv], vx)
        
        print("å†…å­˜ä¼˜åŒ–è°ƒåº¦å·²åˆ›å»º")
        return s, [data, kernel, conv]

# 5. TVMä¸ç¡¬ä»¶åŠ é€Ÿå™¨é›†æˆ
class TVMHardwareIntegration:
    """
    TVMä¸ä¸åŒç¡¬ä»¶çš„é›†æˆæŠ€æœ¯
    """
    
    def __init__(self):
        self.targets = {
            "cpu": tvm.target.Target("llvm"),
            "gpu": tvm.target.Target("cuda"),
            "arm": tvm.target.Target("llvm -device=arm_cpu"),
        }
    
    def multi_target_compilation(self):
        """
        å¤šç›®æ ‡ç¼–è¯‘ç¤ºä¾‹
        """
        print("=== å¤šç›®æ ‡ç¡¬ä»¶ç¼–è¯‘ ===")
        
        # å®šä¹‰é€šç”¨è®¡ç®—
        n = 1024
        A = te.placeholder((n,), name="A")
        B = te.placeholder((n,), name="B")
        C = te.compute((n,), lambda i: A[i] + B[i], name="C")
        
        for target_name, target in self.targets.items():
            try:
                s = te.create_schedule(C.op)
                
                # é’ˆå¯¹ä¸åŒç¡¬ä»¶çš„ä¼˜åŒ–
                if target_name == "cpu":
                    s[C].vectorize(C.op.axis[0])
                elif target_name == "gpu":
                    s[C].bind(C.op.axis[0], te.thread_axis("threadIdx.x"))
                elif target_name == "arm":
                    # ARMç‰¹å®šä¼˜åŒ–
                    s[C].parallel(C.op.axis[0])
                
                func = tvm.build(s, [A, B, C], target=target)
                print(f"{target_name.upper()}ç›®æ ‡ç¼–è¯‘æˆåŠŸ")
                
                # æ˜¾ç¤ºç”Ÿæˆçš„ä»£ç ç‰‡æ®µ
                source = func.get_source()
                print(f"{target_name}ä»£ç ç‰‡æ®µ:")
                print(source[:200] + "..." if len(source) > 200 else source)
                print()
                
            except Exception as e:
                print(f"{target_name}ç¼–è¯‘å¤±è´¥: {e}")
    
    def tensor_core_utilization(self):
        """
        Tensor Coreåˆ©ç”¨ç¤ºä¾‹ (é’ˆå¯¹æ”¯æŒçš„GPU)
        """
        print("=== Tensor Coreåˆ©ç”¨ ===")
        
        try:
            # å®šä¹‰é€‚åˆTensor Coreçš„çŸ©é˜µä¹˜æ³•
            M, N, K = 1024, 1024, 1024
            
            A = te.placeholder((M, K), name="A", dtype="float16")
            B = te.placeholder((K, N), name="B", dtype="float16") 
            
            k = te.reduce_axis((0, K), name="k")
            C = te.compute(
                (M, N),
                lambda i, j: te.sum(A[i, k].astype("float32") * B[k, j].astype("float32"), axis=k),
                name="C"
            )
            
            s = te.create_schedule(C.op)
            
            # Tensor Coreä¼˜åŒ–è°ƒåº¦
            block_size = 16
            xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], block_size, block_size)
            ko, ki = s[C].split(k, factor=16)
            
            s[C].reorder(xo, yo, ko, xi, yi, ki)
            
            # ä½¿ç”¨tensorizeè¿›è¡ŒTensor Coreæ˜ å°„
            # æ³¨æ„ï¼šè¿™éœ€è¦ç‰¹å®šçš„intrinsicå®šä¹‰
            print("Tensor Coreè°ƒåº¦å·²é…ç½®")
            
        except Exception as e:
            print(f"Tensor Coreé…ç½®å¤±è´¥: {e}")

# 6. å®Œæ•´çš„TVMå·¥ä½œæµç¤ºä¾‹
class CompleteTVMWorkflow:
    """
    å®Œæ•´çš„TVMç¼–è¯‘ä¼˜åŒ–å·¥ä½œæµ
    """
    
    def __init__(self):
        self.target = tvm.target.Target("llvm")
        self.ctx = tvm.cpu(0)
    
    def end_to_end_optimization(self):
        """
        ç«¯åˆ°ç«¯ä¼˜åŒ–æµç¨‹
        """
        print("=== ç«¯åˆ°ç«¯TVMä¼˜åŒ–å·¥ä½œæµ ===")
        
        # é˜¶æ®µ1: æ¨¡å‹å®šä¹‰
        print("é˜¶æ®µ1: å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹")
        relay_mod = self.define_neural_network()
        
        # é˜¶æ®µ2: å›¾çº§ä¼˜åŒ–
        print("é˜¶æ®µ2: å›¾çº§ä¼˜åŒ–")
        optimized_mod = self.apply_graph_optimizations(relay_mod)
        
        # é˜¶æ®µ3: ç®—å­çº§ä¼˜åŒ–
        print("é˜¶æ®µ3: ç®—å­çº§ä¼˜åŒ–")
        tuned_mod = self.apply_operator_tuning(optimized_mod)
        
        # é˜¶æ®µ4: ä»£ç ç”Ÿæˆ
        print("é˜¶æ®µ4: ç›®æ ‡ä»£ç ç”Ÿæˆ")
        compiled_func = self.compile_and_deploy(tuned_mod)
        
        # é˜¶æ®µ5: æ€§èƒ½éªŒè¯
        print("é˜¶æ®µ5: æ€§èƒ½éªŒè¯")
        self.benchmark_performance(compiled_func)
        
        return compiled_func
    
    def define_neural_network(self):
        """
        å®šä¹‰ç¤ºä¾‹ç¥ç»ç½‘ç»œ
        """
        # è¾“å…¥
        data = relay.var("data", shape=(1, 3, 224, 224), dtype="float32")
        
        # ç¬¬ä¸€å±‚: Conv + BN + ReLU
        conv1 = relay.nn.conv2d(data, relay.var("conv1_weight"), 
                               kernel_size=(7, 7), strides=(2, 2), padding=(3, 3))
        bn1 = relay.nn.batch_norm(conv1, relay.var("bn1_gamma"), relay.var("bn1_beta"),
                                 relay.var("bn1_mean"), relay.var("bn1_var"))[0]
        relu1 = relay.nn.relu(bn1)
        
        # æ± åŒ–
        pool1 = relay.nn.max_pool2d(relu1, pool_size=(3, 3), strides=(2, 2), padding=(1, 1))
        
        # ç¬¬äºŒå±‚: Conv + BN + ReLU  
        conv2 = relay.nn.conv2d(pool1, relay.var("conv2_weight"),
                               kernel_size=(3, 3), padding=(1, 1))
        bn2 = relay.nn.batch_norm(conv2, relay.var("bn2_gamma"), relay.var("bn2_beta"),
                                 relay.var("bn2_mean"), relay.var("bn2_var"))[0]
        relu2 = relay.nn.relu(bn2)
        
        # å…¨è¿æ¥å±‚
        pool2 = relay.nn.global_avg_pool2d(relu2)
        flat = relay.nn.batch_flatten(pool2)
        dense = relay.nn.dense(flat, relay.var("dense_weight"))
        
        # åˆ›å»ºå‡½æ•°
        net = relay.Function(relay.analysis.free_vars(dense), dense)
        mod = tvm.IRModule.from_expr(net)
        
        return mod
    
    def apply_graph_optimizations(self, mod):
        """
        åº”ç”¨å›¾çº§ä¼˜åŒ–
        """
        with tvm.transform.PassContext(opt_level=3):
            # æ ‡å‡†ä¼˜åŒ–åºåˆ—
            seq = tvm.transform.Sequential([
                relay.transform.RemoveUnusedFunctions(),
                relay.transform.ToBasicBlockNormalForm(),
                relay.transform.EliminateCommonSubexpr(),
                relay.transform.FuseOps(fuse_opt_level=2),
                relay.transform.CombineParallelConv2D(),
                relay.transform.CombineParallelDense(),
                relay.transform.FoldConstant(),
                relay.transform.AlterOpLayout(),
                relay.transform.CanonicalizeOps(),
                relay.transform.DeadCodeElimination()
            ])
            
            optimized_mod = seq(mod)
        
        return optimized_mod
    
    def apply_operator_tuning(self, mod):
        """
        åº”ç”¨ç®—å­çº§è°ƒä¼˜
        """
        # æå–è°ƒä¼˜ä»»åŠ¡
        tasks, task_weights = auto_scheduler.extract_tasks(
            mod["main"], params={}, target=self.target)
        
        if not tasks:
            print("æ²¡æœ‰æ‰¾åˆ°å¯è°ƒä¼˜çš„ä»»åŠ¡")
            return mod
        
        print(f"æ‰¾åˆ° {len(tasks)} ä¸ªè°ƒä¼˜ä»»åŠ¡")
        
        # ç®€åŒ–çš„è°ƒä¼˜è¿‡ç¨‹
        log_file = "tune_log.json"
        
        for i, task in enumerate(tasks):
            print(f"è°ƒä¼˜ä»»åŠ¡ {i+1}/{len(tasks)}: {task.workload_key}")
            
            # åˆ›å»ºè°ƒä¼˜é€‰é¡¹
            tune_option = auto_scheduler.TuningOptions(
                num_measure_trials=32,  # å‡å°‘è¯•éªŒæ¬¡æ•°ç”¨äºæ¼”ç¤º
                runner=auto_scheduler.LocalRunner(repeat=1, enable_cpu_cache_flush=True),
                measure_callbacks=[auto_scheduler.RecordToFile(log_file)],
                verbose=0,
            )
            
            # è¿è¡Œè°ƒä¼˜
            search_policy = auto_scheduler.SketchPolicy(task)
            task.tune(tune_option, search_policy)
        
        # åº”ç”¨è°ƒä¼˜ç»“æœ
        with auto_scheduler.ApplyHistoryBest(log_file):
            with tvm.transform.PassContext(opt_level=3, config={
                "relay.backend.use_auto_scheduler": True
            }):
                tuned_mod = tvm.relay.transform.InferType()(mod)
        
        return tuned_mod
    
    def compile_and_deploy(self, mod):
        """
        ç¼–è¯‘å¹¶éƒ¨ç½²æ¨¡å‹
        """
        with tvm.transform.PassContext(opt_level=3):
            graph, lib, params = relay.build(mod, target=self.target)
        
        # åˆ›å»ºæ‰§è¡Œå™¨
        module = graph_executor.create(graph, lib, self.ctx)
        
        return module, params
    
    def benchmark_performance(self, compiled_model):
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        """
        module, params = compiled_model
        
        # è®¾ç½®å‚æ•°
        module.set_input(**params)
        
        # åˆ›å»ºè¾“å…¥æ•°æ®
        input_data = np.random.uniform(size=(1, 3, 224, 224)).astype(np.float32)
        module.set_input("data", input_data)
        
        # é¢„çƒ­
        for _ in range(10):
            module.run()
        
        # æ€§èƒ½æµ‹è¯•
        timer = module.module.time_evaluator("run", self.ctx, number=100, repeat=3)
        prof_res = timer()
        
        print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {prof_res.mean * 1000:.2f} ms")
        print(f"æ ‡å‡†å·®: {prof_res.std * 1000:.2f} ms")

# ä¸»è¦ä½¿ç”¨ç¤ºä¾‹
def main():
    """
    TVMæ ¸å¿ƒåŠŸèƒ½æ¼”ç¤º
    """
    print("=== TVMç¼–è¯‘å™¨æŠ€æœ¯æ·±åº¦åˆ†æ ===\n")
    
    # 1. ç¼–è¯‘å™¨æ¶æ„æ¼”ç¤º
    compiler_stack = TVMCompilerStack()
    compiler_stack.demonstrate_ir_levels()
    
    # 2. è‡ªåŠ¨è°ƒä¼˜æ¼”ç¤º
    auto_tuning = TVMAutoTuning()
    # auto_tuning.auto_schedule_example()  # æ³¨é‡Šæ‰ä»¥èŠ‚çœæ—¶é—´
    
    # 3. å›¾ä¼˜åŒ–æ¼”ç¤º
    graph_opt = TVMGraphOptimization()
    graph_opt.relay_optimization_passes()
    graph_opt.operator_fusion_analysis()
    
    # 4. ä»£ç ç”Ÿæˆæ¼”ç¤º
    codegen = TVMCodeGeneration()
    codegen.schedule_optimization_examples()
    codegen.memory_optimization_techniques()
    
    # 5. ç¡¬ä»¶é›†æˆæ¼”ç¤º
    hardware_int = TVMHardwareIntegration()
    hardware_int.multi_target_compilation()
    
    # 6. å®Œæ•´å·¥ä½œæµæ¼”ç¤º
    workflow = CompleteTVMWorkflow()
    # workflow.end_to_end_optimization()  # æ³¨é‡Šæ‰ä»¥èŠ‚çœæ—¶é—´
    
    print("\n=== TVMåˆ†æå®Œæˆ ===")

if __name__ == "__main__":
    main()
```

### 4.2 TVMç¼–è¯‘å™¨ä¼˜åŒ–ç­–ç•¥æ€»ç»“

é€šè¿‡æ·±å…¥åˆ†æTVMç¼–è¯‘å™¨æŠ€æœ¯ï¼Œæˆ‘ä»¬æŒæ¡äº†ç°ä»£æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨çš„æ ¸å¿ƒæŠ€æœ¯ï¼š

#### æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹ï¼š

1. **å¤šå±‚IRè®¾è®¡**
   - Relay IRï¼šé«˜çº§å›¾è¡¨ç¤ºï¼Œæ”¯æŒåŠ¨æ€å½¢çŠ¶å’Œæ§åˆ¶æµ
   - Tensor Expressionï¼šç®—å­çº§è¡¨ç¤ºï¼Œæè¿°è®¡ç®—æ¨¡å¼
   - TensorIRï¼šä½çº§å¾ªç¯è¡¨ç¤ºï¼Œæ”¯æŒç»†ç²’åº¦ä¼˜åŒ–
   - ç›®æ ‡ä»£ç ï¼šé’ˆå¯¹å…·ä½“ç¡¬ä»¶çš„ä¼˜åŒ–ä»£ç 

2. **è‡ªåŠ¨è°ƒä¼˜ç³»ç»Ÿ**
   - AutoSchedulerï¼šåŸºäºæœç´¢çš„è‡ªåŠ¨è°ƒåº¦ä¼˜åŒ–
   - æˆæœ¬æ¨¡å‹ï¼šé¢„æµ‹ä¸åŒè°ƒåº¦çš„æ€§èƒ½
   - æœç´¢ç­–ç•¥ï¼šé—ä¼ ç®—æ³•ã€æ¨¡æ‹Ÿé€€ç«ç­‰ä¼˜åŒ–ç®—æ³•

3. **å›¾çº§ä¼˜åŒ–**
   - ç®—å­èåˆï¼šå‡å°‘å†…å­˜è®¿é—®å’Œè®¡ç®—å¼€é”€
   - å¸¸é‡æŠ˜å ï¼šç¼–è¯‘æ—¶è®¡ç®—å¸¸é‡è¡¨è¾¾å¼
   - æ­»ä»£ç æ¶ˆé™¤ï¼šç§»é™¤æœªä½¿ç”¨çš„è®¡ç®—
   - å¸ƒå±€ä¼˜åŒ–ï¼šé€‰æ‹©æœ€ä¼˜çš„æ•°æ®å¸ƒå±€

4. **ä»£ç ç”Ÿæˆä¼˜åŒ–**
   - å¾ªç¯ä¼˜åŒ–ï¼šåˆ†å—ã€å‘é‡åŒ–ã€å¹¶è¡ŒåŒ–ã€å±•å¼€
   - å†…å­˜å±‚æ¬¡ï¼šåˆ©ç”¨ç¼“å­˜å±‚æ¬¡ç»“æ„
   - æŒ‡ä»¤è°ƒåº¦ï¼šæœ€å¤§åŒ–æŒ‡ä»¤çº§å¹¶è¡Œ
   - å¯„å­˜å™¨åˆ†é…ï¼šå‡å°‘å†…å­˜è®¿é—®

5. **ç¡¬ä»¶é€‚é…**
   - å¤šç›®æ ‡æ”¯æŒï¼šCPUã€GPUã€ä¸“ç”¨åŠ é€Ÿå™¨
   - ç¡¬ä»¶ç‰¹æ€§åˆ©ç”¨ï¼šTensor Coreã€å‘é‡æŒ‡ä»¤
   - å†…å­˜æ¨¡å‹ï¼šé€‚é…ä¸åŒçš„å†…å­˜å±‚æ¬¡ç»“æ„

---

## æ€»ç»“ï¼šAIèŠ¯ç‰‡ç®—å­å¼€å‘æŠ€æœ¯æ ˆæŒæ¡

é€šè¿‡å¯¹å››ä¸ªæ ¸å¿ƒå¼€æºé¡¹ç›®çš„æ·±åº¦åˆ†æï¼Œæˆ‘ä»¬å»ºç«‹äº†å®Œæ•´çš„AIèŠ¯ç‰‡ç®—å­å¼€å‘æŠ€æœ¯ä½“ç³»ï¼š

### æŠ€æœ¯æ ˆæ€»è§ˆ

1. **ONNXRuntime** - å·¥ä¸šçº§ç®—å­ä¼˜åŒ–å‚è€ƒ
   - æ‰§è¡Œæä¾›è€…æ¶æ„ï¼šå¤šç¡¬ä»¶ç»Ÿä¸€æ¥å£
   - å›¾ä¼˜åŒ–æ¡†æ¶ï¼šé«˜æ•ˆçš„æ¨ç†ä¼˜åŒ–
   - å†…æ ¸æ³¨å†Œæœºåˆ¶ï¼šå¯æ‰©å±•çš„ç®—å­ç³»ç»Ÿ

2. **Triton** - ç°ä»£GPUç¼–ç¨‹èŒƒå¼
   - Python DSLï¼šç®€åŒ–GPUç¼–ç¨‹å¤æ‚åº¦
   - è‡ªåŠ¨ä¼˜åŒ–ï¼šJITç¼–è¯‘å’Œè‡ªåŠ¨è°ƒä¼˜
   - å—çº§ç¼–ç¨‹ï¼šé«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—æ¨¡å‹

3. **PyTorch ATen** - åŠ¨æ€å›¾ç®—å­å®ç°
   - å¼ é‡åº“è®¾è®¡ï¼šç»Ÿä¸€çš„æ•°æ®æŠ½è±¡
   - è‡ªåŠ¨å¾®åˆ†ï¼šé«˜æ•ˆçš„æ¢¯åº¦è®¡ç®—
   - åŠ¨æ€åˆ†å‘ï¼šè¿è¡Œæ—¶ç®—å­é€‰æ‹©

4. **TVM** - ç¼–è¯‘å™¨ä¼˜åŒ–æŠ€æœ¯
   - å¤šå±‚IRï¼šä»é«˜çº§åˆ°ä½çº§çš„æ¸è¿›ä¼˜åŒ–
   - è‡ªåŠ¨è°ƒä¼˜ï¼šæœç´¢æœ€ä¼˜å®ç°
   - è·¨ç¡¬ä»¶ç¼–è¯‘ï¼šç»Ÿä¸€çš„ä¼˜åŒ–æ¡†æ¶

### å®è·µåº”ç”¨æŒ‡å¯¼

è¿™å››ä¸ªé¡¹ç›®ä¸ºAIèŠ¯ç‰‡ç®—å­å¼€å‘æä¾›äº†å®Œæ•´çš„æŠ€æœ¯å‚è€ƒï¼š

- **æ¶æ„è®¾è®¡**ï¼šå­¦ä¹ ONNXRuntimeçš„åˆ†å±‚æ¶æ„å’ŒPyTorchçš„åˆ†å‘æœºåˆ¶
- **æ€§èƒ½ä¼˜åŒ–**ï¼šæŒæ¡Tritonçš„GPUç¼–ç¨‹æŠ€å·§å’ŒTVMçš„ç¼–è¯‘ä¼˜åŒ–
- **å·¥ç¨‹å®è·µ**ï¼šç†è§£å·¥ä¸šçº§ç³»ç»Ÿçš„è®¾è®¡æ¨¡å¼å’Œæœ€ä½³å®è·µ
- **åˆ›æ–°æ–¹å‘**ï¼šåŸºäºè¿™äº›æŠ€æœ¯æ ˆæ¢ç´¢æ–°çš„ç®—å­ä¼˜åŒ–æ–¹æ³•

é€šè¿‡æ·±å…¥ç ”ç©¶è¿™äº›å¼€æºé¡¹ç›®ï¼Œæˆ‘ä»¬ä¸ä»…æŒæ¡äº†AIèŠ¯ç‰‡ç®—å­å¼€å‘çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ›´é‡è¦çš„æ˜¯ç†è§£äº†å¦‚ä½•å°†è¿™äº›æŠ€æœ¯åº”ç”¨åˆ°å®é™…çš„èŠ¯ç‰‡è®¾è®¡å’Œä¼˜åŒ–ä¸­ï¼Œä¸ºæˆä¸ºä¼˜ç§€çš„AIèŠ¯ç‰‡ç®—å­å¼€å‘å·¥ç¨‹å¸ˆå¥ å®šäº†åšå®çš„åŸºç¡€ã€‚


