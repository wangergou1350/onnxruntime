# Triton 面向自研 AI 芯片算子开发与性能优化系统教程 (Tutorial Book)

> 版本：v1.0  （后续可增补 FlashAttention 进阶 / 低精度 FP8 / 多芯片并行）
> 适用读者：具备 Python、线性代数、基础深度学习、GPU 基本概念的工程师 / 研究人员。
> 学习目标：从 0 构建“算子原理 → Triton 实现 → 性能建模 →  优化 → 迁移适配 → 开源输出”完整闭环能力。

---
## 全书阅读指引
1. Phase 0~2：核心编程与并行心智模型（必须精读）
2. Phase 3~4：复杂算子与融合、Attention（重点 + 反复迭代）
3. Phase 5：性能方法论（与实践穿插）
4. Phase 6：适配抽象（面向芯片/后端者必读）
5. Phase 7：输出与工程化沉淀
6. 附录：术语 / Roofline / 常见陷阱 / 调参 Checklist

---
## 目录
1. 前言与总体目标
2. 环境搭建与工具链
3. 并行计算与内存体系速览
4. Triton 基础编程模型
5. Elementwise 与 Reduction Kernel 构建
6. MatMul 进化之路（v0→v4）
7. Softmax & LayerNorm 深度实现
8. 卷积：直接算法与 im2col 对比 + Fused Epilogue
9. Attention 与 Flash 思想简化实现
10. Autotuning 与配置搜索策略
11. 性能分析：指标、Roofline、瓶颈分类
12. Kernel 设计模式与复用（Epilogue / 参数化模板）
13. 混合精度与数值稳定性
14. 面向自研芯片的抽象适配流程
15. 性能回归 CI 与基准体系建设
16. 开源协作与报告撰写
17. 附录 A：术语表
18. 附录 B：常见错误与排查
19. 附录 C：调参 & 优化 Checklist
20. 附录 D：练习答案参考（可单独文件）

（以下章节按 Phase 结构组织，后续附录集中概念。）

> 目标画像：6 个月内具备基于 Triton 进行主流深度学习核心算子 (MatMul / Conv / Attention / Norm / Fused Elementwise) 的实现、分析、端到端性能优化与跨芯片（参考 GPU -> 自研架构抽象）迁移能力，并能够产出性能分析报告与开源贡献。
>
> 使用方式：按阶段迭代推进；每阶段包含：核心主题 / 必备知识点 / 实践任务 / 里程碑 (Milestone) / 评估标准 / 推荐资源。若已有基础，可加速跳级；保持“读源码 + 做 profiling + 写总结”闭环。

---
## 总体阶段概览 (建议时间分配)
| 阶段 | 时间 | 主题定位 |
|------|------|----------|
| Phase 0 | 第 0 周 (启动) | 基线评估 & 环境搭建 & 指标建立 |
| Phase 1 | 第 1-2 周 | 计算 & 内存 & 并行基础巩固；Triton 初识 |
| Phase 2 | 第 3-6 周 | Triton 编程模型深度 & 基础算子 (MatMul / Softmax) |
| Phase 3 | 第 7-10 周 | 卷积 / GEMM 进阶 / Fused Kernels / Autotune |
| Phase 4 | 第 11-14 周 | Attention 系列 & Mixed Precision & Pipeline |
| Phase 5 | 第 15-18 周 | 端到端性能优化方法论 & Roofline & Memory BW 分析 |
| Phase 6 | 第 19-22 周 | 面向自研芯片的适配抽象：指令/访存/并行模型映射 |
| Phase 7 | 第 23-24 周 | 开源贡献 / 性能白皮书 / 面试强化准备 |

---
## Phase 0：启动与基线 (Baseline & Toolchain)
### 0.1 目标
建立可复现实验环境、明确初始性能基线、掌握度量指标与基准采集方法。

### 0.2 环境与依赖
- Python >=3.10 （建议使用 `conda create -n triton-dev python=3.10`）
- GPU 驱动 + CUDA Toolkit（以实际硬件为准）
- Triton 安装：`pip install triton` 或克隆源码（需要确保 `LLVM` 依赖）

### 0.3 关键工具
| 目标 | 工具 | 用途 | 输出指标 |
|------|------|------|----------|
| 时间测量 | Python `time.perf_counter` | 基本延迟统计 | 平均/中位/99th |
| 访存/执行分析 | Nsight Compute | 单 kernel 深度指标 | dram_throughput / sm_efficiency |
| 时序 & 依赖 | Nsight Systems | 全局时间线 | Kernel 排布 / 重叠 |
| 轻量分析 | torch.profiler | PyTorch 集成 | 算子级耗时列表 |

### 0.4 核心指标定义
- Latency (ms)
- Throughput = FLOPs / Time
- Arithmetic Intensity AI = Total FLOPs / Bytes Moved (估算：读 + 写字节)
- Effective Bandwidth = Bytes / Time
- Occupancy：实际并发 warps / 理论最大
- Warp Utilization：活跃线程比例

### 0.5 首个 Kernel：向量加法
示例代码：见 `kernels/vector_add.py`

运行示例 (PowerShell)：
```powershell
python .\kernels\vector_add.py
```
输出示例：
```
n=1000000 avg=0.12ms effective_bw=XXX GB/s
...
```

### 0.6 Benchmark 结构建议
```
project/
  kernels/          # Triton kernel 源码
  benchmarks/       # 基准脚本
  reports/          # 输出 markdown / json 指标
  docs/             # 学习与总结
```

### 0.7 基线实验步骤
1. 运行官方 matmul 示例记录形状 (e.g. 4096x4096) 性能
2. 实现 vector add，分别测试 N = 1e6, 4e6, 16e6，采集延迟与有效带宽
3. 估算：理论带宽上限 (来自 GPU 规格) → 计算利用率

### 0.8 常见问题 (FAQ)
| 问题 | 可能原因 | 快速排查 |
|------|----------|----------|
| 安装失败 (LLVM) | 编译依赖缺失 | 使用预编译 pip 版本；检查 `pip debug --verbose` |
| Kernel 非法内存访问 | 越界 / mask 缺失 | 使用 `mask=` 参数；打印 shape 断言 |
| 性能极低 | 未合并访存 / 太小网格 | 增大问题规模，检查 `tl.arange` 步长 |
| 波动大 | 冷启动 / 频率调节 | 预热多次，固定 `CUDA_LAUNCH_BLOCKING=1` 测试一致性 |

### 0.9 里程碑与验收
- 形成《基线性能与工具链准备报告.md》：包含环境版本、首个 kernel 指标表、问题记录
- 能准确解释：global vs L2 vs L1 / register 的作用与典型 latency（数量级即可）

### 0.10 本章练习
1. 写一个 kernel 将输入张量加上标量 α（支持任意长度 N，不足部分用 mask）
2. 统计 5 次运行的 p50/p90/p99 延迟，分析抖动来源
3. 估算 Arithmetic Intensity 并判断该 kernel 是否典型内存受限

### 0.11 性能基准测试方法学与统计规范

#### 0.11.1 测量精度要求
**抖动控制目标**：相对标准差 (RSD) < 5%，否则需要增加样本或优化测试环境。

**预热策略**：
1. 冷启动预热：3-5 次空跑，避免 GPU 频率调节与 JIT 编译影响
2. 缓存预热：确保数据已在 L2/L3 缓存中（对小规模测试重要）
3. 内核预热：Triton kernel 首次编译耗时不计入基准

#### 0.11.2 统计指标标准
```python
def benchmark_kernel(kernel_func, *args, warmup=5, repeat=20):
    # 预热阶段
    for _ in range(warmup):
        kernel_func(*args)
        torch.cuda.synchronize()
    
    # 测量阶段
    latencies = []
    for _ in range(repeat):
        start = time.perf_counter()
        kernel_func(*args)
        torch.cuda.synchronize()  # 关键：等待 GPU 完成
        end = time.perf_counter()
        latencies.append((end - start) * 1000)  # ms
    
    return {
        'min': min(latencies),
        'p50': percentile(latencies, 50),
        'p90': percentile(latencies, 90), 
        'p99': percentile(latencies, 99),
        'mean': sum(latencies) / len(latencies),
        'std': statistics.stdev(latencies),
        'rsd': statistics.stdev(latencies) / (sum(latencies) / len(latencies))
    }
```

#### 0.11.3 环境控制检查清单
| 环境变量 | 建议值 | 作用 |
|---------|-------|------|
| `CUDA_LAUNCH_BLOCKING` | 1 | 强制同步，精确计时 |
| `TORCH_CUDA_ARCH_LIST` | 具体架构 (如 8.0) | 避免运行时重编译 |
| GPU 时钟锁定 | 固定频率 (如可用) | 减少动态调频影响 |
| CPU 频率 | 固定 performance 模式 | 避免节能调频 |

#### 0.11.4 统计显著性检验
- **重复次数建议**：至少 20 次，RSD > 5% 时增加到 50-100 次
- **异常值处理**：使用 IQR (四分位距) 法过滤超过 1.5*IQR 的极端值
- **对比检验**：使用 Mann-Whitney U 检验判断两组性能是否显著不同

#### 0.11.5 形状敏感性测试
不同问题规模的性能特征差异：
```python
shapes = [(1024, 1024), (2048, 2048), (4096, 4096), (8192, 8192)]
for M, N in shapes:
    # 测试并记录：Latency, GFLOPs, Effective BW, Occupancy
    # 绘制：规模 vs 效率曲线，找出最佳工作点
```

#### 0.11.6 功耗与能效测量 (可选)
若硬件支持 (`nvidia-ml-py`)：
```python
import pynvml
# 测量 kernel 执行期间的功耗变化
# 计算 GOPS/W (每瓦特 GIGA 操作数)
```

#### 0.11.7 可复现性检查
- **环境指纹记录**：CUDA 版本、驱动版本、GPU 型号、内存容量
- **随机种子固定**：`torch.manual_seed()`, `numpy.random.seed()`
- **结果归档**：包含时间戳、硬件信息、代码版本的 JSON 格式报告

---
## Phase 1：计算体系 & Triton 基本模型 (Fundamentals)
### 1.1 并行抽象心智模型
CUDA: Grid -> Block -> Thread；Triton: program = 逻辑“block”实例。`tl.program_id(axis)` 返回当前 program 在某轴的 id。

### 1.2 访存合并 (Coalescing)
目标：相邻线程访问连续地址 → 合并为更少的 DRAM/L2 事务。Triton 中通过 `tl.arange(0, BLOCK_SIZE)` 自然生成连续索引。

### 1.3 常用内建
| API | 作用 | 备注 |
|-----|------|------|
| `tl.arange` | 生成 0..n-1 向量 | 做索引基础 |
| `tl.load(ptr, mask=, other=)` | 读取 | 利用 mask 防止越界 |
| `tl.store` | 写入 | 与 mask 对应 |
| `tl.full` | 构造常量张量 | 调试 placeholder |
| `tl.where(cond,a,b)` | 选择 | elementwise |
| `tl.constexpr` | 编译期常量 | 用于调优参数 |

### 1.4 实例：ReLU Kernel 讲解（伪代码片段）
```
@triton.jit
def relu_kernel(x_ptr, y_ptr, n, BLOCK: tl.constexpr):
  pid = tl.program_id(0)
  offs = pid * BLOCK + tl.arange(0, BLOCK)
  mask = offs < n
  x = tl.load(x_ptr + offs, mask=mask, other=0.)
  y = tl.where(x > 0, x, 0.)
  tl.store(y_ptr + offs, y, mask=mask)
```
调参要点：BLOCK 通常取 1024/2048，保持并行度同时不过度增加寄存器需求。

### 1.5 LayerNorm (均值与方差) 分解
1. 计算均值：`mean = sum(x) / N`
2. 计算方差：`var = sum((x - mean)^2) / N`
3. 数值稳定：使用两遍法 / Welford（进阶）

### 1.6 性能分析三角
并行度 (Occupancy) ↔ 访存效率 (Load/Store 合并 + 复用) ↔ 指令效率 (ILP / Tensor Core 利用)。任一短板都会限制上限。

### 1.7 练习
1. 实现近似 GELU：`0.5 * x * (1 + tanh(√(2/π)*(x + 0.044715x^3)))`
2. 实现 LayerNorm 前半段：输出 mean, var（返回两个张量）
3. 统计带宽利用率：估算 bytes = (读 + 写)*元素字节，计算 Effective Bandwidth

### 1.8 验收 Checklist
- 所有 kernel 对齐 PyTorch 参考实现 (max abs diff < 1e-6 FP32)
- 比纯 Python 循环加速 >= 10x
- 能口述 program id → 索引映射过程

---
## Phase 2：MatMul 深入 & 基础算子族
### 2.1 GEMM 结构复盘
访存-计算复用关键：A 每行被多个 N 分块复用，B 每列被多个 M 分块复用 → 分块把高复用数据留在寄存器或 L1。

### 2.2 版本演进思路
| 版本 | 特点 | 预期收益 |
|------|------|----------|
| v0 | 三重循环 / 直接 load | 正确性基线 |
| v1 | 基本 tile (BLOCK_M/N/K) | 减少重复读 |
| v2 | 软件流水 (num_stages>1) | 隐藏 global mem latency |
| v3 | Autotune 参数搜索 | 找 Pareto 最佳点 |
| v4 | Epilogue 融合 (bias/act) | 减少额外访存 |

### 2.3 关键调参维度
- BLOCK_M / BLOCK_N：影响寄存器矩阵尺寸
- BLOCK_K：影响加载批次大小（过大 → 寄存器压力 / 共享缓存压力）
- num_warps：并行粒度（过大占资源、过小利用不足）
- num_stages：pipeline 深度（过多→寄存器膨胀）

### 2.4 Softmax 行归约模式
两遍：第一遍 max，第二遍 exp+sum；或归并在线算法（避免中间 buffer）。注意数值稳定：`x - max`。

### 2.5 混合精度支持
读入 half → 转 float32 累加 → 写回 half。核对误差：`torch.allclose`(atol, rtol)。

### 2.6 指标记录模板
| Shape | Version | Gflops | Registers/Thread | Occupancy | L2 Hit | DRAM BW | Notes |

### 2.7 练习
1. 实现 v0→v3 每版保留代码文件，写差异注释
2. 编写 autotune 装饰器：对 BLOCK_* / num_warps 网格搜索
3. 加入 epilogue：支持 bias + SiLU 激活

### 2.8 验收
- 参考代码：`kernels/matmul_stepwise.py`、基准：`benchmarks/benchmark_matmul.py`

新增：自动调参示例 `kernels/matmul_autotune.py` 运行：
```powershell
python .\kernels\matmul_autotune.py
```
首次运行会枚举配置并缓存最快方案。

运行基准 (PowerShell)：
```powershell
python .\benchmarks\benchmark_matmul.py
```
会生成 `matmul_results.json`，可用于后续绘图。
- 单一大形状 (4096^2) ≥ cuBLAS 70%（相同精度）
- Autotune 带来 ≥15% 提升
- Softmax kernel 优于 PyTorch 同尺寸 eager 实现

---
## Phase 3：卷积 & Fused Kernels
### 3.1 卷积索引推导
输出 (n, oc, oh, ow) 来源于输入 (n, ic, ih, iw) 与权重 (oc, ic, kh, kw)。直接卷积：遍历核窗口累加。

### 3.2 im2col 与直接卷积取舍
| 方案 | 优点 | 缺点 |
|------|------|------|
| im2col+GEMM | 复用成熟 GEMM 优化 | 中间展开内存放大 |
| 直接卷积 | 减少中间存储 | 优化复杂、memory access pattern 更难 |

### 3.3 Fused Epilogue 原理
在输出写回前直接加 bias / 激活，省去再次读写 C。

### 3.4 数据布局影响
NHWC 在部分硬件（向量化通道方向）上更优；不同布局影响 coalescing 与 cache locality。

### 3.5 练习
1. 实现 stride=1 same padding 直接卷积
2. 实现 im2col 展开 + 复用已有 matmul
3. 对比 fused Conv+Bias+ReLU vs 三 kernel 总时间与内存字节估算

### 3.6 验收
- Fused 方案全局内存字节减少 ≥25%
- Epilogue 版本性能 ≥ 原 GEMM 95%

- 知识点：
  - Conv 形式：NCHW vs NHWC；im2col 概念；直接卷积 vs Winograd 简要（不必实现全部）
  - 数据布局与 Cache 行命中；padding / stride / dilation 的索引计算
  - Fused Pattern：Conv + Bias + Activation；LayerNorm + Dropout；MatMul + Softmax + MatMul (部分融合)
  - Memory Traffic 估算：访存字节 vs 算术操作
- 实践：
  1. 实现一个 2D Conv (NCHW, stride=1, padding=same) 直接算法 + 简易 im2col 版本对比
  2. 编写 Conv+Bias+ReLU 融合 kernel；测单独三算子 vs 融合带宽下降比例
  3. 为 GEMM 增加 epilogue (bias + activation) 模式
  4. 建立“版本迭代性能表” Markdown
- 里程碑：Fused Conv 比未融合流水线减少 >= 25% 全局内存访问；Epilogue GEMM 达到原 GEMM 95% 性能

---
## Phase 4：Attention & Flash 思想
### 4.1 标准公式分解
Q ∈ (B,H,N,D), K ∈ (B,H,N,D), V ∈ (B,H,N,Dv)
Scores = (Q K^T)/√D → Softmax → *V

### 4.2 Mask 处理
对上三角 causal：if col > row → -inf；Triton 中可用 mask + `other=-float('inf')`（再做 softmax 前）。

### 4.3 FlashAttention 思路要点
- Tiling over sequence：分块加载 K/V，维护当前行的 max & sum_exp 在线更新
- 避免存完整 Scores 矩阵
- 精度：log-sum-exp 在线更新

### 4.4 简化实现步骤
1. 外层循环 over K 块
2. 计算局部分块 scores
3. 更新：`m_new = max(m_prev, max(scores))`
4. `exp(scores - m_new)` 修正累加：`sum_new = exp(m_prev - m_new)*sum_prev + sum(exp(...))`
5. 输出：`output = (exp(scores - m_new)/sum_new) * V` 同步累加

### 4.5 练习
1. 实现多 kernel 基础版 → 融合 QK^T+Softmax
2. 实现简化 Flash 内主循环
3. 对比不同 N=512/1024/2048/4096 的显存占用与时间

参考实现：`kernels/attention.py`；可修改 BLOCK_N 观察误差变化。

### 4.6 验收
### 4.7 在线 Softmax 数值稳定推导
### 4.8 多头 FlashAttention 扩展
多头拆分：Q/B,H,N,D 视作 (B*H) 个独立序列；将 batch*head 合并为 grid axis0，行块为 axis1。
关键差异：增加 head 维 stride；D 通常较小（64/80/128），可将整 head dim 放入 BLOCK_DMODEL，减少循环。
示例代码：`kernels/flash_attention_multihead.py`
调参提示：若 N 很大可在 BLOCK_N 上做分块 + 增加 stages；D>128 时需按 D tile 再做 acc 合并。

#### 扩展特性
- Causal Mask：内部使用 `offs_n <= offs_m` 条件屏蔽未来 token。
- Padding Mask：传入形状 (B,1,N,N) 或 (B,H,N,N) 的掩码（1/True 保留）。
- Dropout：基于简单 hash 生成伪随机，Inverted scaling (除以 1-p)。
- FP8 量化演示：对 V / 输出做线性缩放与 int8 存储（示例性质，真实 FP8 需 E4M3/E5M2 格式转换）。
 - Sliding Window (局部稀疏) Attention：支持 `window_left`, `window_right` 参数，仅计算当前 query 行附近局部范围，配合 causal/padding 可叠加。大幅降低 N 很大但局部相关的算量与显存访存。
 - 组合稀疏屏蔽优先级：先窗口裁剪 → 再 causal → 再 padding/mask → 最终 softmax。

#### 真 FP8 支持 (E4M3 / E5M2) 与动态缩放
新增 `utils/fp8.py`：
 - `DynamicScaler`：跟踪张量 amax，指数滑动平均 (EMA) 生成 scale。
 - `fp8_quantize / fp8_dequantize`：模拟量化到 int8 与反量化。
 - `choose_format`：基于动态范围简单启发选择 E4M3 (较高精度) 或 E5M2 (更大范围)。
集成策略：
1. 前向：对 V (及/或 Q,K) 先更新 scaler → 量化存储 (可选缓存) → kernel 内加载后乘以 inv_scale 还原。
2. 输出：acc 结果再按动态 scale 量化写回 (保持链路低精度存储)。
3. 优势：显存/带宽压力进一步下降；需注意累加仍在 FP32/FP16 以控制误差。

Per-Channel 版本：
- 使用 `PerChannelScaler` 对最后一维 (channel/head dim) 单独统计 amax，降低少数大值对整体 scale 的挤压。
- 减少量化误差：特别是注意力中 V 的列向量幅度差异大时效果显著。
- 验证：统计逐元素误差直方图，对比 per-tensor vs per-channel 最大/平均相对误差。
 - Kernel 已支持：通过 `PER_CHANNEL` 分支与 `INV_S_PTR` 传入逐通道 inverse scale，实现加载与输出双向 per-channel 去 / 量化。
 - 外部缓存：`save_per_channel_state / load_per_channel_state` 可将逐通道 amax + EMA 状态持久化 (`fp8_state_path`，支持训练多轮稳定量化策略)。
 - Group-wise FP8：通过 `GroupScaler(channels, group_size)` 按 group 聚合 amax（折中压缩 scale 向量长度与误差）；wrapper 传 `fp8_group_size` 自动走 group path 并展开为 INV_S_PTR。

验证指标：
 - 记录每 step amax 与 scale；若频繁饱和（大量 clipping），应调大 EMA 或分层尺度 (per-channel)。
 - 对比 FP16 基线的 max/mean abs diff 与下游 (e.g. MHA) 质量指标。

#### Sliding Window 使用示例
```
out = flash_attention(q,k,v, window_left=128, window_right=0, causal=False)
```
可在长序列 (N>=8k) 下减少 O(N^2) -> O(N * (WL+WR)) 复杂度。评估：比较算术 FLOPs、实际耗时与显存读写字节。

#### 验证建议
1. 与 `scaled_dot_product_attention` 对比 `max diff` 与 `relative error`。
2. Dropout 模式下统计输出方差是否与理论一致（期望放大 1/(1-p)）。
3. FP8 路径：比较反量化后误差；调节 `scale` 控制溢出比例 (<0.5% clipping)。

设块迭代 i 时局部分块得分矩阵为 S_i，行向量维度长度 N_i，定义：
当前块最大值 m_i = max(S_i)，历史最大值 M_{i-1}，更新 M_i = max(M_{i-1}, m_i)。
历史累计归一化和 L_{i-1} = \sum_j exp(S_j - M_{i-1})。
则新块并入：
L_i = exp(M_{i-1} - M_i) * L_{i-1} + \sum_{x\in S_i} exp(x - M_i)
输出权重对每块可写：
P = ( exp(S_i - M_i) + exp(M_{i-1}-M_i) * 之前加权和 ) / L_i
该形式避免存全部 S，同时保持与一次性 softmax 数值等价（log-sum-exp 恒等式）。
实践注意：
1. 需使用 FP32 累加 L_i 与 m_i
2. 避免在 D 很大时 BLOCK_N 过小导致 kernel launch 过多（可并行聚合）
3. 与参考实现比较 relative error，检查误差集中分布（注意极小概率尾部）

- Flash 简化版性能 ≥ 官方实现同级 60%
- 内存峰值显著低于基础多 kernel 方案

### 4.9 FlashAttention 反向传播与 Autograd 集成

#### 4.9.1 反向传播数学推导
FlashAttention 反向的核心挑战：前向时未保存完整 attention 权重矩阵，需要重计算。

**关键量保存策略**：
- 保存：Q, K, V（或其分块），以及每行的 max 值 (m) 和归一化项 (l)
- 重计算：在反向时重建局部 attention 权重，计算梯度

**梯度计算核心公式**：
```
设 O = Attention(Q,K,V)，损失 L 对 O 的梯度为 dO
需要计算：dQ, dK, dV

对于每个 query 行 i：
1. 重计算 P_i = softmax(Q_i K^T / √d)  # 利用保存的 m_i, l_i 优化
2. dV += P_i^T dO_i
3. dP_i = dO_i V^T  
4. dS_i = P_i ⊙ (dP_i - (dP_i ⊙ P_i).sum())  # softmax 反向
5. dQ_i = dS_i K / √d
6. dK += Q_i^T dS_i / √d
```

#### 4.9.2 Triton 实现要点
```python
@triton.jit  
def flash_attention_backward_kernel(...):
    # 1. 重新加载 Q_i, K 块，以及保存的 m_i, l_i
    # 2. 重计算 P_i = exp(S_i - m_i) / l_i
    # 3. 计算 dV 贡献
    # 4. 计算 dQ_i, dK 贡献
    # 5. 注意数值稳定性：避免 exp 爆炸
```

**内存与计算权衡**：
- 保存 O(seq_len) 的 m, l vs 保存 O(seq_len²) 的完整权重矩阵
- 重计算成本 vs 内存节约
- 适合训练（重视内存）vs 推理（重视速度）

#### 4.9.3 PyTorch Autograd 集成
```python
class FlashAttentionFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, q, k, v, causal=False):
        # 1. 调用前向 Triton kernel
        output, m, l = flash_attention_forward(q, k, v, causal)
        # 2. 保存反向所需的中间结果
        ctx.save_for_backward(q, k, v, m, l)
        ctx.causal = causal
        return output
    
    @staticmethod  
    def backward(ctx, grad_output):
        q, k, v, m, l = ctx.saved_tensors
        # 调用反向 Triton kernel
        dq, dk, dv = flash_attention_backward(
            grad_output, q, k, v, m, l, ctx.causal
        )
        return dq, dk, dv, None

# 使用方式
flash_attention = FlashAttentionFunction.apply
```

#### 4.9.4 数值稳定性考虑
- **重计算一致性**：确保前向、反向的 softmax 计算路径完全一致
- **梯度检查**：使用 `torch.autograd.gradcheck` 验证实现正确性
- **混合精度**：前向 FP16，反向累加用 FP32，避免梯度下溢

#### 4.9.5 性能优化要点
- **Kernel 融合**：dQ, dK, dV 计算尽量在同一 kernel 中完成
- **内存访问优化**：重用前向的数据加载模式
- **Pipeline**：重计算与梯度计算并行

#### 4.9.6 练习与验证
1. 实现简化版 FlashAttention autograd wrapper
2. 与 PyTorch 原生 attention 的梯度进行数值对比
3. 测试不同序列长度下的内存使用与速度

- 知识点：
  - 标准 Attention 数学：softmax(QK^T / sqrt(d))V
  - Causal / Padding mask 应用；上三角屏蔽策略 (mask)
  - FlashAttention 原理：在线计算 softmax 归一化；块级重计算 vs 存储权衡
  - **FlashAttention 反向**：重计算策略、梯度累加、数值稳定性
  - 混合精度：FP16 / BF16 / FP8 (概念)；累加精度；Loss scaling 基本思想
  - warp-level reduction vs block-level reduction
- 实践：
  1. 实现基础 Attention (分成 QK^T、Softmax、乘 V 多 kernel) → 再做部分融合 (QK^T+Softmax) 版本
  2. 尝试实现简化版 FlashAttention 核心（tile over sequence, 在线归一化）
  3. **实现 FlashAttention autograd wrapper 并通过梯度检查**
  4. 在 BF16 下对比精度 (max diff, relative error) && 性能
  5. Profile 不同 sequence length (e.g. 512/1k/2k/4k) 的性能拐点
- 里程碑：简化 FlashAttention 版本达到官方实现同级别长度 60%+ 性能；**通过梯度检查验证反向正确性**；写技术博客总结

---
## Phase 5：性能分析方法论 (Roofline & Bottlenecks)
### 5.1 Roofline 建模步骤
1. 估算 FLOPs：GEMM ≈ 2*M*N*K
2. 估算 Bytes：读写 A/B/C（考虑复用与缓存命中）
3. AI = FLOPs/Bytes；落点决定受限类型
4. 测试实际性能点与 Roofline 差距

### 5.2 常用指标释义
- `sm_efficiency`: 有效周期 SM 有指令执行比例
- `achieved_occupancy`: 实际活动 warp / 最大 warp
- `dram_read/write_throughput`: 内存带宽利用
- `l2_hit_rate`: 缓存复用有效度
- `registers_per_thread`: 过高 → 降低 occupancy

### 5.2A FP8 量化风险控制与生产实践

#### 5.2A.1 FP8 格式选择决策框架
```python
def choose_fp8_format(model_layer, accuracy_target):
    """基于层类型和精度需求选择 FP8 格式"""
    
    # E4M3: 适合激活值，范围更大但精度稍低
    # E5M2: 适合权重，精度更高但范围受限
    
    if layer_type == "attention":
        # 注意力计算对精度敏感
        return "E5M2" if accuracy_target > 0.99 else "E4M3"
    elif layer_type == "mlp":
        # MLP 可容忍更多量化噪声
        return "E4M3"
    elif layer_type == "embedding":
        # 嵌入层对范围要求高
        return "E4M3"
    else:
        return "E5M2"  # 保守选择
```

#### 5.2A.2 动态缩放因子管理
```python
class AdaptiveScalingManager:
    def __init__(self, smoothing=0.99, safety_margin=1.2):
        self.smoothing = smoothing
        self.safety_margin = safety_margin
        self.history = []
        
    def update_scale(self, amax_observed, current_scale):
        """基于观测到的最大值自适应调整缩放因子"""
        optimal_scale = (127.0 / amax_observed) / self.safety_margin
        
        if len(self.history) > 0:
            # 平滑更新，避免震荡
            new_scale = (self.smoothing * current_scale + 
                        (1 - self.smoothing) * optimal_scale)
        else:
            new_scale = optimal_scale
            
        self.history.append(amax_observed)
        return new_scale
        
    def detect_anomaly(self, amax):
        """检测数值异常（梯度爆炸等）"""
        if len(self.history) < 5:
            return False
            
        recent_avg = sum(self.history[-5:]) / 5
        return amax > recent_avg * 10  # 10x 增长视为异常
```

#### 5.2A.3 量化误差累积分析
```python
def analyze_quantization_error_propagation(model, dataset_sample):
    """分析量化误差在模型中的传播"""
    
    layer_errors = {}
    cumulative_error = 0
    
    for layer_name, layer in model.named_modules():
        if hasattr(layer, 'quantize_fp8'):
            # 测量该层的量化误差
            fp32_output = layer(input_fp32)
            fp8_output = layer.quantize_fp8(input_fp32)
            
            layer_error = torch.norm(fp32_output - fp8_output)
            layer_errors[layer_name] = layer_error
            cumulative_error += layer_error
            
            print(f"{layer_name}: Error = {layer_error:.6f}")
    
    # 检查是否超过预设阈值
    error_threshold = 0.01 * len(layer_errors)  # 每层平均误差不超过 1%
    if cumulative_error > error_threshold:
        print(f"Warning: Cumulative error {cumulative_error:.4f} exceeds threshold")
    
    return layer_errors
```

#### 5.2A.4 FP8 量化生产部署检查清单

**数值稳定性检查**
- [ ] 各层 amax 监控正常，无异常尖峰
- [ ] 缩放因子更新平滑，收敛到稳定区间
- [ ] 量化误差累积在可接受范围 (< 2% overall degradation)
- [ ] 特殊输入处理 (全零、极值、NaN/Inf)

**性能验证**
- [ ] 端到端推理速度提升 > 1.5x
- [ ] GPU 内存使用减少 > 40%
- [ ] Tensor Core 利用率 > 80%
- [ ] 与 FP16 baseline 精度差异 < 1%

**生产监控指标**
```python
class FP8ProductionMonitor:
    def __init__(self):
        self.metrics = {
            'amax_outliers': 0,     # amax 异常次数
            'scale_updates': 0,     # 缩放因子更新次数
            'numerical_errors': 0,  # 数值错误（NaN/Inf）
            'accuracy_drops': 0     # 精度下降次数
        }
    
    def check_health(self, model_output, expected_range):
        """健康检查，实时监控模型输出"""
        if torch.isnan(model_output).any():
            self.metrics['numerical_errors'] += 1
            return False
            
        if model_output.abs().max() > expected_range[1] * 2:
            self.metrics['amax_outliers'] += 1
            # 触发降级到 FP16
            return False
            
        return True
```

#### 5.2A.5 混合精度策略优化
```python
def design_mixed_precision_policy(model_complexity, accuracy_requirement):
    """设计混合精度策略，平衡性能与精度"""
    
    policy = {}
    
    # 关键层保持高精度
    critical_layers = ['attention.q_proj', 'attention.k_proj', 'lm_head']
    
    for name, module in model.named_modules():
        if any(critical in name for critical in critical_layers):
            if accuracy_requirement > 0.99:
                policy[name] = 'FP16'  # 极高精度需求
            else:
                policy[name] = 'E5M2'  # 较高精度 FP8
        else:
            policy[name] = 'E4M3'      # 标准 FP8
    
    return policy
```

### 5.3 瓶颈分类对策简表
| 瓶颈 | 识别信号 | 优化策略 |
|------|----------|----------|
| 内存带宽 | dram 高 / AI 低 | 提升复用、融合、压缩精度 |
| 访存不合并 | transactions 异常多 | 重排索引、调整 BLOCK 大小 |
| 寄存器压力 | occupancy 下降 | 缩小 tile / 减少 staging |
| L2 命中低 | l2_hit_rate 低 | 重新划分 tile、确保局部性 |
| Pipeline 不足 | dram idle 明显 | 增加 num_stages / 预取 |

### 5.4 Autotune 策略
初始全量网格 → 记录前 20% 最佳 → 局部微调 BLOCK_K / num_warps；缓存历史结果到 JSON。

### 5.5 性能报告结构
背景 → 算法/版本 → 指标表 → Roofline 图（可用 matplotlib）→ 结论 & 下一步。

### 5.6 练习
1. 为自写 MatMul 生成 Roofline 数据点
2. 改变 BLOCK_K，观察 AI 与寄存器使用变化
3. 写脚本：自动遍历 config 输出 CSV
**核心目标**：建立系统性能模型：找瓶颈 → 假设 → 验证 → 优化 → 量化收益；掌握 Roofline & 指标采集。

- 知识点：
  - Roofline 模型：I = FLOPs / Bytes；带宽上限 vs 计算上限
  - 指标抓取：sm_efficiency, achieved_occupancy, dram_read/write_throughput, l2_hit_rate
  - 指令级：Tensor Core 利用 (mma 指令统计)（如适用）
  - Autotuning 策略：网格搜索 / 经验裁剪；参数维度 (BLOCK_* / num_warps / num_stages)
  - 常见瓶颈分类：访存未合并 / 寄存器溢出 / bank conflict / pipeline depth 不足 / occupancy 过低
- 实践：
  1. 对自己的 MatMul、Conv、Attention 各出一份 Roofline 报告
  2. 写 Autotune 脚本：输入矩阵形状集合 → 输出最佳配置表 (JSON)
  3. 给出一个“性能回归检测”脚本 (CI 友好)
- 里程碑：能对任一 kernel 给出瓶颈归类 + 两条可验证优化方向；Autotune 提升某 kernel ≥15%

---
## Phase 6：面向自研芯片的抽象迁移
### 6.1 架构差异建模
定义 JSON：`{"warp_size":32, "shared_mem_kb":128, "vector_width":4, ...}` → 作为调参输入。

### 6.2 可移植性关键点
- 分离：算法逻辑 vs 调度参数
- 参数化 tile → 根据后端限制裁剪搜索空间
- 引入 capability 表：是否支持 Tensor Core / 特定指令

### 6.3 模拟实验
减少带宽（乘以系数）模拟 → 重新 autotune → 对比恢复率。

### 6.4 练习
1. 写 `configs/default_gpu.json` 与 `configs/low_bw.json`
2. 让 autotune 脚本加载不同 config 输出最优参数差异
3. 汇总适配报告

### 6.4A 高级 AutoTune 策略与自适应优化

#### 6.4A.1 分层 AutoTune 策略
```python
class HierarchicalAutoTuner:
    def __init__(self):
        # 第一层：粗粒度参数空间
        self.coarse_grid = {
            'BLOCK_M': [32, 64, 128],
            'BLOCK_N': [32, 64, 128], 
            'BLOCK_K': [16, 32, 64],
            'num_warps': [2, 4, 8]
        }
        
        # 第二层：细粒度局部搜索
        self.fine_tune_delta = {
            'BLOCK_M': [-16, -8, 8, 16],
            'BLOCK_N': [-16, -8, 8, 16],
            'num_warps': [-1, 1]
        }
    
    def search(self, kernel_func, inputs):
        # 第一阶段：粗粒度搜索
        best_config, best_time = self.coarse_search(kernel_func, inputs)
        
        # 第二阶段：局部细化
        refined_config = self.fine_tune(kernel_func, inputs, best_config)
        
        return refined_config
    
    def coarse_search(self, kernel_func, inputs):
        results = []
        for config in self.generate_configs(self.coarse_grid):
            try:
                time = self.benchmark_config(kernel_func, inputs, config)
                results.append((config, time))
            except:
                continue  # 跳过无效配置
                
        return min(results, key=lambda x: x[1])
    
    def fine_tune(self, kernel_func, inputs, base_config):
        best_config = base_config
        best_time = self.benchmark_config(kernel_func, inputs, base_config)
        
        for param, deltas in self.fine_tune_delta.items():
            for delta in deltas:
                new_config = base_config.copy()
                new_config[param] = max(1, base_config[param] + delta)
                
                try:
                    time = self.benchmark_config(kernel_func, inputs, new_config)
                    if time < best_time:
                        best_config = new_config
                        best_time = time
                except:
                    continue
                    
        return best_config
```

#### 6.4A.2 自适应参数空间裁剪
```python
class AdaptiveParameterPruning:
    def __init__(self):
        self.performance_history = {}
        self.pruning_threshold = 0.1  # 性能差异阈值
        
    def prune_search_space(self, base_grid, problem_size):
        """基于问题规模和历史性能，裁剪搜索空间"""
        
        # 基于问题规模的硬约束
        M, N, K = problem_size
        
        # 确保 BLOCK 不超过维度大小
        valid_block_m = [b for b in base_grid['BLOCK_M'] if b <= M]
        valid_block_n = [b for b in base_grid['BLOCK_N'] if b <= N]
        valid_block_k = [b for b in base_grid['BLOCK_K'] if b <= K]
        
        # 基于历史性能的软约束
        if self.performance_history:
            # 移除历史上表现差的配置组合
            valid_block_m = self.filter_by_history(valid_block_m, 'BLOCK_M')
            valid_block_n = self.filter_by_history(valid_block_n, 'BLOCK_N')
        
        return {
            'BLOCK_M': valid_block_m,
            'BLOCK_N': valid_block_n,
            'BLOCK_K': valid_block_k,
            'num_warps': self.select_warps_by_blocks(valid_block_m, valid_block_n)
        }
    
    def select_warps_by_blocks(self, block_m_list, block_n_list):
        """基于 BLOCK 大小选择合适的 warp 数量"""
        warp_options = []
        for block_m in block_m_list:
            for block_n in block_n_list:
                threads_per_block = (block_m * block_n) // 32  # 32 threads per warp
                optimal_warps = min(8, max(1, threads_per_block // 4))
                warp_options.extend([optimal_warps//2, optimal_warps, optimal_warps*2])
        
        return sorted(list(set(w for w in warp_options if 1 <= w <= 8)))
```

#### 6.4A.3 多目标优化框架
```python
class MultiObjectiveOptimizer:
    def __init__(self, objectives=['performance', 'memory', 'energy']):
        self.objectives = objectives
        self.weights = {'performance': 0.6, 'memory': 0.3, 'energy': 0.1}
        
    def evaluate_config(self, config, kernel_func, inputs):
        """多目标评估配置"""
        metrics = {}
        
        # 性能指标
        execution_time = self.benchmark_performance(kernel_func, inputs, config)
        metrics['performance'] = 1.0 / execution_time  # 倒数，越大越好
        
        # 内存指标
        memory_usage = self.estimate_memory_usage(config)
        metrics['memory'] = 1.0 / memory_usage
        
        # 能耗指标（简化估算）
        energy_cost = self.estimate_energy(config, execution_time)
        metrics['energy'] = 1.0 / energy_cost
        
        # 加权综合评分
        score = sum(self.weights[obj] * metrics[obj] for obj in self.objectives)
        return score, metrics
```
### 6.5 数据布局优化与内存访问模式

#### 6.5.1 内存访问模式分析
```python
class MemoryAccessAnalyzer:
    def __init__(self):
        self.access_patterns = {
            'sequential': {'stride': 1, 'efficiency': 1.0},
            'strided': {'stride': lambda s: s, 'efficiency': lambda s: max(0.1, 1.0/s)},
            'random': {'stride': 'random', 'efficiency': 0.1},
            'broadcast': {'stride': 0, 'efficiency': 0.95}
        }
    
    def analyze_pattern(self, tensor_shape, access_indices):
        """分析张量访问模式的内存效率"""
        strides = self.calculate_strides(access_indices)
        pattern_type = self.classify_pattern(strides)
        
        # 计算缓存行利用率
        cache_line_size = 128  # bytes
        element_size = 4  # FP32
        elements_per_line = cache_line_size // element_size
        
        utilization = self.calculate_cache_utilization(strides, elements_per_line)
        
        return {
            'pattern_type': pattern_type,
            'average_stride': sum(strides) / len(strides),
            'cache_utilization': utilization,
            'bandwidth_efficiency': self.estimate_bandwidth_efficiency(pattern_type)
        }
    
    def suggest_optimization(self, analysis_result):
        """基于访问模式分析建议优化策略"""
        suggestions = []
        
        if analysis_result['cache_utilization'] < 0.5:
            suggestions.append("Consider data layout transformation (transpose/reorder)")
            
        if analysis_result['pattern_type'] == 'strided':
            suggestions.append("Use vectorized loads with appropriate stride")
            
        if analysis_result['bandwidth_efficiency'] < 0.3:
            suggestions.append("Consider data prefetching or tiling strategies")
            
        return suggestions
```

#### 6.5.2 SIMD 友好数据布局
```python
class SIMDOptimizedLayout:
    def __init__(self, vector_width=8):
        self.vector_width = vector_width
        
    def optimize_for_simd(self, tensor_shape, access_pattern):
        """为 SIMD 指令优化数据布局"""
        
        if access_pattern == 'row_major':
            # 确保每行长度是向量宽度的倍数
            optimized_shape = self.pad_for_alignment(tensor_shape)
            layout_transformation = "row_padding"
            
        elif access_pattern == 'column_major':
            # 考虑按列访问的向量化
            optimized_shape = self.transpose_and_pad(tensor_shape)
            layout_transformation = "transpose_and_pad"
            
        elif access_pattern == 'blocked':
            # 使用分块布局提高局部性
            optimized_shape = self.create_blocked_layout(tensor_shape)
            layout_transformation = "blocked_layout"
            
        return {
            'original_shape': tensor_shape,
            'optimized_shape': optimized_shape,
            'transformation': layout_transformation,
            'expected_speedup': self.estimate_speedup(tensor_shape, optimized_shape)
        }
    
    def create_blocked_layout(self, shape, block_size=None):
        """创建分块布局以提高缓存局部性"""
        if block_size is None:
            block_size = min(64, min(shape))
            
        # 将矩阵重新组织为分块格式
        M, N = shape
        block_M = (M + block_size - 1) // block_size
        block_N = (N + block_size - 1) // block_size
        
        return (block_M, block_N, block_size, block_size)
```

#### 6.5.3 自适应数据重排算法
```python
class AdaptiveDataReorganizer:
    def __init__(self):
        self.layout_cache = {}
        self.performance_history = {}
        
    def determine_optimal_layout(self, operation_type, tensor_shapes, access_patterns):
        """根据操作类型和访问模式确定最优数据布局"""
        
        cache_key = (operation_type, tuple(tensor_shapes), tuple(access_patterns))
        
        if cache_key in self.layout_cache:
            return self.layout_cache[cache_key]
        
        candidates = self.generate_layout_candidates(operation_type, tensor_shapes)
        best_layout = self.evaluate_layouts(candidates, access_patterns)
        
        self.layout_cache[cache_key] = best_layout
        return best_layout
    
    def generate_layout_candidates(self, operation_type, shapes):
        """为特定操作生成候选数据布局"""
        candidates = []
        
        if operation_type == 'matmul':
            candidates = [
                'row_major',           # 标准行主序
                'column_major',        # 列主序
                'blocked_4x4',         # 4x4 分块
                'blocked_8x8',         # 8x8 分块  
                'nt_interleaved',      # 非转置交错
                'vector_packed'        # 向量打包
            ]
        elif operation_type == 'attention':
            candidates = [
                'head_first',          # 头优先布局
                'sequence_first',      # 序列优先布局
                'blocked_attention',   # 分块注意力布局
                'sparse_optimized'     # 稀疏优化布局
            ]
            
        return candidates
    
    def evaluate_layouts(self, candidates, access_patterns):
        """评估不同布局的性能"""
        scores = {}
        
        for layout in candidates:
            # 计算内存访问效率
            memory_score = self.calculate_memory_efficiency(layout, access_patterns)
            
            # 计算计算效率
            compute_score = self.calculate_compute_efficiency(layout)
            
            # 综合评分
            scores[layout] = 0.7 * memory_score + 0.3 * compute_score
            
        return max(scores.items(), key=lambda x: x[1])[0]
```

#### 6.5.4 动态布局转换
```python
class DynamicLayoutConverter:
    def __init__(self):
        self.conversion_kernels = {}
        
    def register_conversion_kernel(self, from_layout, to_layout, kernel_func):
        """注册布局转换内核"""
        self.conversion_kernels[(from_layout, to_layout)] = kernel_func
        
    def convert_layout(self, tensor, from_layout, to_layout):
        """执行布局转换"""
        conversion_key = (from_layout, to_layout)
        
        if conversion_key not in self.conversion_kernels:
            raise ValueError(f"No conversion kernel for {from_layout} -> {to_layout}")
            
        kernel = self.conversion_kernels[conversion_key]
        return kernel(tensor)
    
    def estimate_conversion_cost(self, tensor_size, from_layout, to_layout):
        """估算布局转换的开销"""
        # 基本转换成本：内存带宽限制
        base_cost = tensor_size * 2  # 读 + 写
        
        # 复杂度因子
        complexity_factor = {
            ('row_major', 'column_major'): 1.5,    # 转置
            ('any', 'blocked'): 2.0,               # 分块重排
            ('sparse', 'dense'): 3.0,              # 稀疏转密集
        }
        
        # 查找匹配的转换类型
        for (from_pattern, to_pattern), factor in complexity_factor.items():
            if (from_pattern == 'any' or from_pattern in from_layout) and \
               (to_pattern == 'any' or to_pattern in to_layout):
                base_cost *= factor
                break
                
        return base_cost
    
    def auto_convert_pipeline(self, tensors, operation_sequence):
        """为操作序列自动插入最优的布局转换"""
        converted_tensors = tensors.copy()
        total_conversion_cost = 0
        
        for i, operation in enumerate(operation_sequence):
            current_layouts = [t.layout for t in converted_tensors]
            optimal_layout = self.determine_optimal_layout_for_op(operation)
            
            for j, tensor in enumerate(converted_tensors):
                if current_layouts[j] != optimal_layout:
                    conversion_cost = self.estimate_conversion_cost(
                        tensor.numel(), current_layouts[j], optimal_layout
                    )
                    
                    # 只有转换收益 > 成本时才执行
                    if self.estimate_operation_speedup(operation, optimal_layout) > conversion_cost:
                        converted_tensors[j] = self.convert_layout(
                            tensor, current_layouts[j], optimal_layout
                        )
                        total_conversion_cost += conversion_cost
                        
        return converted_tensors, total_conversion_cost
```
  - 编译接口：若需生成中间 IR (e.g. MLIR dialect)，IR 设计粒度 & 算子 Lowering 路径
  - 可移植性策略：参数化 tile size、访存 pattern 分离、后端配置表
- 实践：
  1. 抽象出一个 `KernelConfig` Python 数据类 + 后端特性描述 JSON（带：max_threads_per_sm, shared_mem_kb, warp_size, vector_width）
  2. 让已有 MatMul/Conv 根据后端 JSON 动态选择 tile 参数
  3. 模拟“带宽下降 30% / 寄存器上限减少”场景，调参恢复 ≥ 原 80% 性能
  4. 设计一页架构适配 Checklist
- 里程碑：形成《面向自研架构参数化适配设计说明》文档

---
## Phase 7：成果沉淀与输出
### 7.1 开源最小可行包结构
```
repo/
  triton_kernels/
  benchmarks/
  scripts/
  docs/
  tests/
```
### 7.2 PR 准备要点
- 复现步骤明确
- 性能表 + 硬件环境
- 与 baseline 对比（含差异分析）

### 7.3 技术博客撰写模版
问题背景 → 硬件/软件挑战 → 优化路径时间线 → 指标 → 失败尝试 → 总结

### 7.4 面试强化题
- 解释 FlashAttention 在线归一化公式
- 描述你的 MatMul 优化演进与每步收益
- 如何判断一个 kernel 已接近硬件上限

### 7.5 练习
1. 输出一篇 MatMul 优化记录
2. 开一个 issue 或提交一份改进建议（即便是文档增强）
3. 构建简单 CI：运行基准并生成 markdown 表
**核心目标**：将阶段成果开源 / 形成可展示资产（简历 / 面试材料 / 博客 / 开源 PR）。

- 知识点：
  - 开源协作流程：Fork / Branch / PR / Review；代码规范 & Benchmark 复现说明
  - 性能报告写作结构：背景→方法→实验设置→结果→分析→未来工作
  - 面试常问：GEMM 优化顺序、FlashAttention 原理、Roofline 解释、访存未对齐后果
- 实践：
  1. 选 1-2 个 kernel 提交到 Triton issue 区或相关 repo（或写独立优化示例仓库）
  2. 输出一篇“从 0 到 70% cuBLAS：我优化 MatMul 的路径”文章
  3. 整理性能数据表 + 可复现实验脚本 + README
- 里程碑：公开仓库 star ≥ 若干（次要）；完成至少 1 次有效技术分享 / 1 个 PR

---
## 能力矩阵速查 (横向)
| 维度 | 入门 | 熟练 | 精通 指标示例 |
|------|------|------|----------------|
| Memory | 知道 global/shared | 做到合并访问 | 预测瓶颈并重构 layout |
| 并行 | program id 概念 | 合理 tile + warp 配置 | 设计自适应调度策略 |
| 算子 | 基本 Elementwise | GEMM/Conv 可实现 | Attention/Flash/Fused 体系化抽象 |
| 优化 | 粗 Profiling | 指标驱动迭代 | 建立自动化性能 CI + 模型 |
| 芯片映射 | 了解差异 | 参数化适配 | 抽象 IR / 后端策略生成 |
| 输出 | 内部记录 | 技术博客 | 开源 PR / 讲座 / 白皮书 |

---
## 周度 Checkpoints (示例)
- W2：完成 3 个基础 elementwise + softmax 初版
- W6：MatMul ≥ cuBLAS 70% (单形状)；Softmax 优化版
- W10：Conv+Bias+Act 融合；GEMM epilogue 完整
- W14：Simplified FlashAttention 可运行
- W18：三类核心算子 Roofline 报告
- W22：后端参数化适配 demo
- W24：开源成果与总结报告

---
## 自测问题清单（摘录）
1. 为什么 BLOCK_K 过大会掉性能？
2. 如何估算一个 kernel 的 Arithmetic Intensity？
3. FlashAttention 如何避免存 N×N 注意力矩阵？
4. 当寄存器压力过高时有哪些退化信号？
5. 什么时候需要做算子融合，什么时候不值得？
6. Autotune 的搜索空间应该如何裁剪？
7. Roofline 上点落在带宽区，能有哪些优化方向？
8. Triton 中 `num_warps` 与 `num_stages` 各影响什么？

---
## 推荐资源
- 官方：Triton 官方文档与 examples 目录
- 源码路径：`python/triton/runtime`, `python/triton/language`, `triton/codegen`
- 博客 / 论文：
  - FlashAttention 系列论文
  - CUTLASS 博客（理解 GEMM Tile 思想可迁移）
  - NVIDIA GPU 架构白皮书（Ampere / Hopper）
  - TVM Unity / MLIR Dialect 设计文章
- 工具：Nsight Compute, Nsight Systems, perfetto, PyTorch profiler
- 课程：MIT 6.172 (性能工程思想), CMU 15-418 (并行架构)

---
## 进阶拓展建议
- 构建“自动配置搜索 + 性能模型”小工具：输入张量形状 -> 推荐 tile 参数
- 尝试在 Triton 上实现一个简单 DSL → 生成若干 kernel 代码片段
- 将某算子 IR（如 MLIR linalg dialect）Lower 到 Triton Kernel（实验性）

---
## 输出模板示例
```markdown
# Kernel 优化报告 - MatMul (形状 4096x4096)
## 版本演进
| Version | 变更 | Gflops | 提升 | 备注 |
|---------|------|--------|------|------|
| v0 | 朴素三重循环 | 0.5 | - | baseline |
| v1 | BLOCK_M/N/K 分块 | 2.1 | +320% | 引入 tile |
| v2 | 软件流水 + num_stages=2 | 3.4 | +62% | overlap load/compute |
| v3 | Autotune + 更优 BLOCK_K | 4.1 | +20% | register OK |
| v4 | 融合 bias + act | 4.0 | -2% | epilogue 成本可接受 |

## Profiler 关键指标
- dram_read_throughput: ...
- sm_efficiency: ...
- l2_hit_rate: ...

## 分析 & 下一步
...
```

---
## 使用策略
1. 每阶段保留“代码（kernels/）、数据（bench/）、文档 (docs/)”三分结构
2. 保持每周一次复盘：性能表 + 难点 + 下一步假设
3. 所有性能数字需记录：硬件 / Shape / 精度 / 重复次数 / 环境指纹 (CUDA, Driver)
4. 形成可复现脚本（seed 固定 + 形状列表）

---
**终极产出**：一份可公开的“Trition+自研芯片适配”小型技术白皮书 + 核心算子性能对比仓库。

新增脚本：
- 性能与精度摘要：`scripts/compare_accuracy_performance.py` 生成 `reports/summary.md`
- 手动搜索 autotune：`kernels/matmul_autotune_manual.py` 缓存到 `configs/matmul_autotune_cache.json`

辅助代码索引：
- Softmax / LayerNorm：`kernels/softmax_layernorm.py`
- Attention 简化版：`kernels/attention.py`
- 自动调参 MatMul：`kernels/matmul_autotune.py`
- Roofline 初步分析：`scripts/roofline.py`
 - 多头 FlashAttention：`kernels/flash_attention_multihead.py`
 - Roofline 可视化：`scripts/plot_roofline.py`
 - 基础 CI 校验：`scripts/ci_check.py`
  - 性能与精度对比：`scripts/compare_accuracy_performance.py`
  - 手动 Autotune 缓存：`kernels/matmul_autotune_manual.py` + `configs/matmul_autotune_cache.json`

### Roofline Autotune 对比说明
使用 `benchmarks/benchmark_matmul.py` 生成包含 v1 与 v_auto 的 `matmul_results.json`，再运行：
```powershell
python .\scripts\plot_roofline.py
```
图中蓝色 (v1) 与绿色 (autotune) 点之间的箭头指示调参收益；若仍落在带宽线附近，说明需提升算术强度（更大 tile / 融合 epilogue）。

### 新增：Autotune 结果持久化与复用
文件：`kernels/matmul_autotune.py` / `configs/matmul_autotune_cache.json`
 - 首次运行：若 cache 不存在，使用默认网格搜索（或装饰器内部）并写入最优参数。
 - 之后相同形状 + dtype：直接加载缓存跳过搜索，缩短 CI 时间。
 - Key 格式：`MxNxK:dtype`。
进一步：可统计失败/越界配置，精简下次搜索空间。
 - 已增强：若缓存缺失，会对内置 SEARCH_CONFIGS 做一次轻量 timing 并写入真实最佳 BLOCK 参数。

### Attention 精度评估脚本
文件：`scripts/eval_attention_accuracy.py`
用途：对比 baseline (完整)、窗口裁剪、dropout、FP8 量化输出与全精度参考的误差。
输出：`reports/attention_accuracy.json`，含 max_abs / mean_abs / max_rel / mean_rel。
示例：
```powershell
python .\scripts\eval_attention_accuracy.py --N 512 --D 64 --window-left 128 --fp8 --dropout 0.1
```
可扩展：增加 per-channel FP8 模式 / 统计熵差异 / 有效头数分布。
 已增强：输出额外指标
 - `abs_diff_hist_counts`: 误差直方图计数（默认 50 bins）
 - `kl_vs_laplace_like`: 与近似 Laplace 参考分布的 KL（衡量尾部分布偏差）
 - `clip_ratio`: 量化前数据（V 张量）按对应 scale 映射到 FP8 (int8) 动态范围后超出 ±127 的比例
      * 用于监控是否 scale 过小导致饱和（理想应 <1%~2%，具体阈值视任务容忍）
group_size 自动选择：若 D 可被 16 整除取 16，否则取 8。

可视化增强：`--plot` 生成的 `reports/attention_error_hist.png` 现在：
- X/Y 双对数尺度（便于同时观察中小误差与尾部）
- 对每条曲线叠加 CDF（虚线，第二 y 轴）展示累计分布，便于观察 90% / 95% 集中区间
- 支持 baseline / window / fp8_per_channel / fp8_group / dropout 多曲线共存

使用示例：
```powershell
python .\scripts\eval_attention_accuracy.py --B 2 --H 4 --N 512 --D 64 --fp8 --window-left 128 --dropout 0.1 --plot
```
输出 JSON 片段示例：
{
  "fp8_per_channel": {
      "max_abs": 0.0071,
      "mean_abs": 0.00083,
      "clip_ratio": 0.0045,
      ...
  },
  "fp8_group": {
      "max_abs": 0.0089,
      "mean_abs": 0.00097,
      "clip_ratio": 0.0021,
      ...
  }
}
解读：若 fp8_group 的 clip_ratio 明显低而误差接近，则 group-wise 在更低 scale 元数据存储成本下保持精度，可考虑在大模型推理中优先。
后续可扩展：
 - 记录 per-channel / per-group 最大与平均 amax
 - 动态调整 group_size 以目标 clip_ratio 为约束自动搜索
 - 统计误差分布分位点 (p50/p90/p99)

# 补充：FP8 评估新增指标与自适应分组
# 1. 误差分位数：metric 现在输出 p50_abs / p90_abs / p99_abs，定位尾部拉长（长尾漂移）更敏感。
# 2. amax / scale 统计：
#    - per-channel: amax_mean / amax_max / scale_mean / scale_max
#    - group-wise : scale_mean / scale_max + 记录选出的 group_size
# 3. 自适应 group_size 搜索：
#    - 参数：--clip-target (默认 0.01)，--group-candidates '4,8,16,32,64'
#    - 策略：按 candidates 升序遍历，选第一个 clip_ratio <= clip_target；否则回退到最大可整除 group（精度更好）。
#    - 输出：`results['fp8_group']['candidates'] = {size: {clip_ratio}}` 与 target_clip。
# 4. 解读建议：
#    - clip_ratio 超过 target_clip：说明当前 scale 仍有饱和风险，可尝试更小 group_size（更细粒度）或动态扩大 EMA 缓冲。
#    - p99_abs 明显上升但 mean_abs 稳定：长尾恶化（可能少数通道尺度异常）。
#    - scale_max/scale_mean 比值过大：分布高度偏斜，尝试 per-channel 或缩小分组。
# 5. 使用示例：
# ```powershell
# python .\scripts\eval_attention_accuracy.py --fp8 --group-candidates 8,16,32 --clip-target 0.005 --plot
# ```
# 6. 后续可再加：自适应搜索结果写回缓存、分位数趋势文件、p99 回归报警。

---
## 附录 A：术语表 (Glossary)
| 术语 | 定义 | 备注 |
|------|------|------|
| Arithmetic Intensity | FLOPs / Bytes | 判定计算/带宽受限 |
| Occupancy | 活跃 warp 比 | 受寄存器/共享内存限制 |
| Latency Hiding | 通过并行覆盖延迟 | 增加并发或 pipeline |
| Bank Conflict | 多线程访问同一 bank | 导致序列化 |
| Epilogue | 主计算后处理阶段 | 融合减少访存 |
| Autotune | 参数搜索过程 | 性能敏感调参 |
| FlashAttention | 流式 tile softmax 技术 | 降低 O(N^2) 内存 |

## 附录 C：Triton Kernel 测试框架与验证体系

### C1 单元测试框架设计

#### C1.1 Kernel 功能测试基础架构
```python
import pytest
import torch
import triton
import numpy as np
from typing import List, Tuple, Callable

class TritonKernelTestBase:
    """Triton kernel 测试基类"""
    
    def __init__(self, kernel_func, reference_func):
        self.kernel_func = kernel_func
        self.reference_func = reference_func
        self.tolerance = {'atol': 1e-6, 'rtol': 1e-5}
        
    def generate_test_cases(self, input_shapes: List[Tuple], dtypes: List[torch.dtype]):
        """生成测试用例"""
        test_cases = []
        
        for shape in input_shapes:
            for dtype in dtypes:
                # 正常情况
                test_cases.append(self.create_normal_case(shape, dtype))
                
                # 边界情况
                test_cases.extend(self.create_edge_cases(shape, dtype))
                
                # 特殊值测试
                test_cases.extend(self.create_special_value_cases(shape, dtype))
                
        return test_cases
    
    def create_normal_case(self, shape, dtype):
        """创建正常测试用例"""
        torch.manual_seed(42)
        if dtype.is_floating_point:
            tensor = torch.randn(shape, dtype=dtype, device='cuda')
        else:
            tensor = torch.randint(0, 100, shape, dtype=dtype, device='cuda')
        return {'input': tensor, 'case_type': 'normal'}
    
    def create_edge_cases(self, shape, dtype):
        """创建边界测试用例"""
        edge_cases = []
        
        # 全零张量
        zero_tensor = torch.zeros(shape, dtype=dtype, device='cuda')
        edge_cases.append({'input': zero_tensor, 'case_type': 'all_zeros'})
        
        # 全一张量
        ones_tensor = torch.ones(shape, dtype=dtype, device='cuda')
        edge_cases.append({'input': ones_tensor, 'case_type': 'all_ones'})
        
        if dtype.is_floating_point:
            # 极小值
            tiny_tensor = torch.full(shape, 1e-7, dtype=dtype, device='cuda')
            edge_cases.append({'input': tiny_tensor, 'case_type': 'tiny_values'})
            
            # 极大值
            if dtype == torch.float16:
                large_val = 65000.0
            else:
                large_val = 1e6
            large_tensor = torch.full(shape, large_val, dtype=dtype, device='cuda')
            edge_cases.append({'input': large_tensor, 'case_type': 'large_values'})
            
        return edge_cases
    
    def create_special_value_cases(self, shape, dtype):
        """创建特殊值测试用例"""
        if not dtype.is_floating_point:
            return []
            
        special_cases = []
        
        # NaN 值测试
        nan_tensor = torch.full(shape, float('nan'), dtype=dtype, device='cuda')
        special_cases.append({'input': nan_tensor, 'case_type': 'nan_values'})
        
        # 无穷大值测试
        inf_tensor = torch.full(shape, float('inf'), dtype=dtype, device='cuda')
        special_cases.append({'input': inf_tensor, 'case_type': 'inf_values'})
        
        return special_cases
    
    def run_correctness_test(self, test_case):
        """运行正确性测试"""
        input_tensor = test_case['input']
        case_type = test_case['case_type']
        
        try:
            # 运行 Triton kernel
            triton_result = self.kernel_func(input_tensor)
            
            # 运行参考实现
            reference_result = self.reference_func(input_tensor)
            
            # 比较结果
            if case_type in ['nan_values', 'inf_values']:
                # 特殊值需要特殊处理
                self.compare_special_values(triton_result, reference_result, case_type)
            else:
                self.compare_tensors(triton_result, reference_result)
                
            return True, None
            
        except Exception as e:
            return False, str(e)
    
    def compare_tensors(self, triton_result, reference_result):
        """比较张量结果"""
        if not torch.allclose(triton_result, reference_result, **self.tolerance):
            diff = (triton_result - reference_result).abs()
            max_diff = diff.max().item()
            mean_diff = diff.mean().item()
            
            raise AssertionError(
                f"Results do not match!\n"
                f"Max difference: {max_diff}\n"
                f"Mean difference: {mean_diff}\n"
                f"Tolerance: {self.tolerance}"
            )
    
    def compare_special_values(self, triton_result, reference_result, case_type):
        """比较包含特殊值的结果"""
        if case_type == 'nan_values':
            # 检查 NaN 的位置是否一致
            triton_nan_mask = torch.isnan(triton_result)
            ref_nan_mask = torch.isnan(reference_result)
            if not torch.equal(triton_nan_mask, ref_nan_mask):
                raise AssertionError("NaN positions do not match")
                
        elif case_type == 'inf_values':
            # 检查无穷大的处理
            triton_inf_mask = torch.isinf(triton_result)
            ref_inf_mask = torch.isinf(reference_result)
            if not torch.equal(triton_inf_mask, ref_inf_mask):
                raise AssertionError("Inf positions do not match")
```

#### C1.2 性能回归测试
```python
class PerformanceRegressionTester:
    def __init__(self, baseline_db_path="performance_baselines.json"):
        self.baseline_db_path = baseline_db_path
        self.baselines = self.load_baselines()
        
    def load_baselines(self):
        """加载性能基线数据"""
        try:
            with open(self.baseline_db_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def save_baselines(self):
        """保存性能基线数据"""
        with open(self.baseline_db_path, 'w') as f:
            json.dump(self.baselines, f, indent=2)
    
    def benchmark_kernel(self, kernel_func, inputs, num_runs=100, warmup=10):
        """对 kernel 进行性能测试"""
        # 预热
        for _ in range(warmup):
            kernel_func(*inputs)
        torch.cuda.synchronize()
        
        # 正式测试
        start_time = time.time()
        for _ in range(num_runs):
            kernel_func(*inputs)
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        return avg_time
    
    def check_regression(self, kernel_name, current_time, threshold=0.1):
        """检查性能回归"""
        if kernel_name not in self.baselines:
            # 首次运行，建立基线
            self.baselines[kernel_name] = current_time
            self.save_baselines()
            return True, f"Baseline established: {current_time:.6f}s"
        
        baseline_time = self.baselines[kernel_name]
        relative_change = (current_time - baseline_time) / baseline_time
        
        if relative_change > threshold:
            return False, f"Performance regression detected: {relative_change*100:.2f}% slower"
        elif relative_change < -0.05:  # 5% 改善时更新基线
            self.baselines[kernel_name] = current_time
            self.save_baselines()
            return True, f"Performance improved: {-relative_change*100:.2f}% faster"
        else:
            return True, f"Performance stable: {relative_change*100:.2f}% change"
```

#### C1.3 集成测试框架
```python
class TritonIntegrationTester:
    def __init__(self):
        self.test_pipelines = []
        
    def register_pipeline(self, pipeline_name, kernels_sequence):
        """注册测试流水线"""
        self.test_pipelines.append({
            'name': pipeline_name,
            'kernels': kernels_sequence
        })
    
    def test_pipeline_consistency(self, pipeline_name, test_inputs):
        """测试流水线各阶段的一致性"""
        pipeline = next(p for p in self.test_pipelines if p['name'] == pipeline_name)
        
        # 运行完整流水线
        full_result = self.run_full_pipeline(pipeline['kernels'], test_inputs)
        
        # 分段运行并检查中间结果
        stage_results = []
        current_input = test_inputs
        
        for i, kernel in enumerate(pipeline['kernels']):
            stage_output = kernel(current_input)
            stage_results.append(stage_output)
            
            # 数值稳定性检查
            if torch.isnan(stage_output).any():
                raise ValueError(f"NaN detected at stage {i}")
            if torch.isinf(stage_output).any():
                raise ValueError(f"Inf detected at stage {i}")
                
            current_input = stage_output
        
        # 比较最终结果
        if not torch.allclose(stage_results[-1], full_result, atol=1e-6):
            raise AssertionError("Pipeline consistency check failed")
        
        return stage_results
    
    def test_memory_leaks(self, kernel_func, test_inputs, num_iterations=1000):
        """测试内存泄漏"""
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated()
        
        for _ in range(num_iterations):
            output = kernel_func(*test_inputs)
            del output
            
        torch.cuda.empty_cache()
        final_memory = torch.cuda.memory_allocated()
        
        memory_increase = final_memory - initial_memory
        if memory_increase > 1024 * 1024:  # 1MB threshold
            raise AssertionError(f"Memory leak detected: {memory_increase} bytes")
        
        return memory_increase
```

#### C1.4 自动化测试运行器
```python
class TritonTestRunner:
    def __init__(self):
        self.test_suites = []
        self.results = []
        
    def add_test_suite(self, suite_name, test_func, test_data):
        """添加测试套件"""
        self.test_suites.append({
            'name': suite_name,
            'test_func': test_func,
            'test_data': test_data
        })
    
    def run_all_tests(self, verbose=True):
        """运行所有测试"""
        total_tests = 0
        passed_tests = 0
        failed_tests = []
        
        for suite in self.test_suites:
            if verbose:
                print(f"Running test suite: {suite['name']}")
            
            for test_case in suite['test_data']:
                total_tests += 1
                try:
                    suite['test_func'](test_case)
                    passed_tests += 1
                    if verbose:
                        print(f"  ✓ {test_case.get('name', 'unnamed test')}")
                except Exception as e:
                    failed_tests.append({
                        'suite': suite['name'],
                        'test': test_case.get('name', 'unnamed test'),
                        'error': str(e)
                    })
                    if verbose:
                        print(f"  ✗ {test_case.get('name', 'unnamed test')}: {e}")
        
        # 生成测试报告
        self.generate_test_report(total_tests, passed_tests, failed_tests)
        
        return len(failed_tests) == 0
    
    def generate_test_report(self, total, passed, failed):
        """生成测试报告"""
        report = f"""
Triton Kernel Test Report
========================
Total Tests: {total}
Passed: {passed}
Failed: {len(failed)}
Success Rate: {passed/total*100:.2f}%

"""
        if failed:
            report += "Failed Tests:\n"
            for fail in failed:
                report += f"- {fail['suite']}/{fail['test']}: {fail['error']}\n"
        
        with open('test_report.txt', 'w') as f:
            f.write(report)
        
        print(report)
```
| 现象 | 可能原因 | 排查步骤 |
|------|----------|----------|
| 结果随机 | 未同步 / 未固定 seed | 使用 `torch.cuda.synchronize()`；设定 seed |
| OOM | Tile 过大 / 中间缓冲 | 减小 BLOCK / 分批处理 |
| 性能回退 | commit 更改参数 | 建立回归基准，git bisect |
| 数值爆炸 | softmax 未减 max | 检查数值稳定预处理 |
| 低 Occupancy | 寄存器压力 | Nsight 查看 registers/thread 调整 tile |

### 附录 B2：Triton Kernel 调试与诊断决策树

#### B2.1 结果错误诊断流程
```
Kernel 输出错误？
├─ 与参考实现逐元素对比
│  ├─ 全部为 0 / NaN / Inf？
│  │  ├─ 检查内存访问越界 (add mask=)
│  │  ├─ 检查数值溢出 (FP16 range check)
│  │  └─ 检查除零 (分母加 epsilon)
│  └─ 部分元素错误？
│     ├─ 缩小问题到最小复现 (如 2x2 矩阵)
│     ├─ 打印中间结果 (tl.device_print, 但限制调用次数)
│     └─ 手工验证边界条件 (program_id 边界)
└─ 误差在可接受范围？
   ├─ 检查数值精度 (FP16 vs FP32 累加)
   └─ 检查算法差异 (如 softmax 稳定性)
```

#### B2.2 性能问题诊断流程
```
Kernel 速度慢？
├─ 与 baseline 对比倍数
│  ├─ > 10x 慢？
│  │  ├─ 检查 program grid 配置 (是否太小)
│  │  ├─ 检查 BLOCK 大小 (过小导致并行度不足)
│  │  └─ 检查访存模式 (stride 访问 vs coalesced)
│  └─ 2-10x 慢？
│     ├─ 使用 Nsight Compute profiling
│     ├─ 检查 Occupancy (寄存器/共享内存限制)
│     ├─ 检查 Memory throughput (带宽利用率)
│     └─ 检查 SM efficiency (计算利用率)
└─ 已接近硬件上限？
   ├─ 尝试 autotune 不同参数
   └─ 考虑算法优化 (fusion / tiling)
```

#### B2.3 编译错误快速定位
| 错误信息关键词 | 可能原因 | 解决方向 |
|---------------|---------|---------|
| `out of memory` | 寄存器/共享内存超限 | 减小 BLOCK_*, num_warps |
| `misaligned address` | 指针访问未对齐 | 检查 stride 计算，确保字节对齐 |
| `invalid configuration` | grid/block 配置不当 | 检查 num_warps 与 BLOCK 匹配 |
| `compilation timeout` | 编译器资源不足 | 简化 kernel 逻辑或分拆 |

#### B2.4 数值稳定性检查清单
1. **输入范围验证**：检查输入是否在 FP16 表示范围内
2. **中间计算精度**：累加、softmax 等用 FP32
3. **特殊值处理**：检查 0/0、log(0)、exp(大数) 等
4. **舍入误差**：对比不同运算顺序的结果差异

#### B2.5 常用调试技巧
```python
# 1. 逐步缩小问题规模
def debug_kernel():
    # 从最小输入开始 (如 BLOCK_SIZE=32, 单个 block)
    # 逐步增加到目标规模，找出开始出错的边界
    
# 2. 中间结果导出 (谨慎使用，影响性能)
@triton.jit
def debug_kernel(...):
    # ...
    tl.device_print("debug val:", some_tensor[:4])  # 只打印前几个元素
    
# 3. 对比参考实现
def compare_with_torch():
    torch_result = torch.nn.functional.softmax(input_tensor)
    triton_result = my_softmax_kernel(input_tensor)
    diff = (torch_result - triton_result).abs()
    print(f"Max diff: {diff.max()}, Mean diff: {diff.mean()}")
```

## 附录 C：调参 Checklist (摘选)
1. 是否首先确认正确性且有 baseline？
2. 是否估算 AI 判断受限类型？
3. 是否尝试 BLOCK_{M,N,K} 不同倍数网格？
4. num_warps 增减是否影响寄存器溢出？
5. num_stages 是否带来额外寄存器压力超过收益？
6. 访存是否对齐 128B 边界（按硬件要求）？
7. Epilogue 是否值得融合（访存占比高才融合）？

## 附录 D：练习答案参考
(建议单独文件或后续补充，此处占位)

---
## 后续版本规划
- v1.1：加入 FP8 / 混合精度更系统一章
- v1.2：加入 多 GPU / 分片并行 初步指导
- v2.0：增加 自研后端 MLIR Dialect 到 Triton Lowering 案例

> 行动从现在开始：创建 `benchmarks/` 与 `kernels/` 目录，写下第一个 vector add Kernel 与计时脚本。

# CI 低精度监控增强：
# scripts/ci_check.py 新增：
#  - 阈值：THRESHOLDS['attention_fp8_p99_abs'], THRESHOLDS['attention_fp8_clip_ratio']
#  - CLI 覆盖：--fp8-p99-threshold / --fp8-clip-threshold
#  - 使用最新 eval_attention_accuracy 的 fp8_per_channel / fp8_group 字段（p99_abs / clip_ratio / group_size）
#  - 趋势文件：reports/fp8_accuracy_trend.json 保留最近 100 条记录（p99/clip/kl/group_size）
# 典型触发策略：
#  - p99_abs 超阈值：尾部误差漂移，需检查 scale 或窗口配置
#  - clip_ratio 超阈值：缩小 group_size 或放宽 clip-target / 调整 EMA
#  - 可结合 kl_history.json 判断是否伴随分布形态变化
# 调整示例：
# ```powershell
# python .\scripts\ci_check.py --fp8-p99-threshold 3e-2 --fp8-clip-threshold 0.03
# ```
# 后续扩展建议：
#  - 引入自适应 p99 阈值 (mean+3*std) 类似 KL
#  - 把 trend 转换为 Markdown 报表或折线图
#  - 针对 group vs per-channel 生成差分 (p99_group - p99_pc)

# FP8 精度自动报告与阈值回写
# 新脚本：scripts/generate_fp8_report.py
#  - 汇总 fp8_accuracy_trend.json / p99_history.json / kl_history.json
#  - 输出 reports/fp8_report.md：均值、分位数、最近 5 条记录、建议
#  - 建议逻辑：p99 超阈/新低、clip 比例过高或过低
# 阈值外部化：configs/ci_thresholds.json 可覆盖默认 THRESHOLDS
# 自适应阈值回写：
#  - ci_check.py 加 --write-adaptive-thresholds 时，会把自适应 p99 (更低者) 写回
#  - 用于逐步收紧质量门槛（避免回退）
# 工作流示例：
# ```powershell
# python .\scripts\ci_check.py --write-adaptive-thresholds
# python .\scripts\plot_fp8_trends.py
# python .\scripts\generate_fp8_report.py
# ```
# 报告可在 CI artifact 中归档，用于审阅最近精度/量化稳定性趋势。

### 7.1 端到端性能分析与流水线优化

#### 7.1.1 计算图分析框架
```python
class ComputationGraphAnalyzer:
    def __init__(self):
        self.nodes = []  # 计算节点
        self.edges = []  # 数据依赖边
        self.bottlenecks = []
        
    def build_graph_from_model(self, model, sample_input):
        """从模型构建计算图"""
        hooks = []
        node_id = 0
        
        def create_hook(name, node_id):
            def hook_fn(module, input, output):
                # 记录计算节点信息
                input_shapes = [t.shape for t in input if torch.is_tensor(t)]
                output_shape = output.shape if torch.is_tensor(output) else None
                
                self.nodes.append({
                    'id': node_id,
                    'name': name,
                    'type': type(module).__name__,
                    'input_shapes': input_shapes,
                    'output_shape': output_shape,
                    'flops': self.estimate_flops(module, input, output),
                    'memory': self.estimate_memory(input, output)
                })
            return hook_fn
        
        # 为每个模块注册 hook
        for name, module in model.named_modules():
            if len(list(module.children())) == 0:  # 叶子节点
                hook = module.register_forward_hook(create_hook(name, node_id))
                hooks.append(hook)
                node_id += 1
        
        # 运行一次前向传播来构建图
        with torch.no_grad():
            model(sample_input)
        
        # 清理 hooks
        for hook in hooks:
            hook.remove()
        
        return self.nodes
    
    def identify_bottlenecks(self):
        """识别性能瓶颈"""
        # 计算密度分析
        compute_densities = []
        for node in self.nodes:
            ai_ratio = node['flops'] / max(node['memory'], 1)  # Arithmetic Intensity
            compute_densities.append((node['id'], node['name'], ai_ratio))
        
        # 按计算密度排序，找出计算受限的节点
        compute_densities.sort(key=lambda x: x[2])
        
        # 内存瓶颈分析
        memory_intensive = [(n['id'], n['name'], n['memory']) 
                           for n in self.nodes if n['memory'] > 100*1024*1024]  # >100MB
        
        return {
            'compute_bound': compute_densities[-5:],  # 最计算密集的5个
            'memory_bound': compute_densities[:5],    # 最内存密集的5个
            'memory_intensive': memory_intensive
        }
```

#### 7.1.2 分布式计算与多GPU协调
```python
class DistributedTritonOptimizer:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
        self.communication_patterns = {}
        
    def analyze_communication_overhead(self, attention_config):
        """分析分布式注意力机制的通信开销"""
        seq_len = attention_config['seq_len']
        hidden_dim = attention_config['hidden_dim']
        num_heads = attention_config['num_heads']
        
        # 计算不同分布式策略的通信量
        strategies = {
            'tensor_parallel': self.calc_tensor_parallel_comm(seq_len, hidden_dim, num_heads),
            'sequence_parallel': self.calc_sequence_parallel_comm(seq_len, hidden_dim),
            'pipeline_parallel': self.calc_pipeline_parallel_comm(hidden_dim)
        }
        
        return strategies
    
    def optimize_communication_schedule(self, computation_graph):
        """优化通信调度以减少网络开销"""
        # 分析计算与通信的重叠机会
        overlap_opportunities = []
        
        for i, node in enumerate(computation_graph):
            if node.get('requires_communication', False):
                # 找前序和后续可重叠的计算
                prev_compute = self.find_overlappable_compute(computation_graph[:i])
                next_compute = self.find_overlappable_compute(computation_graph[i+1:])
                
                overlap_opportunities.append({
                    'comm_node': node,
                    'prev_overlap': prev_compute,
                    'next_overlap': next_compute,
                    'estimated_savings': self.estimate_overlap_savings(node, prev_compute, next_compute)
                })
        
        return overlap_opportunities
```

#### 7.1.3 自动化报告生成系统
```python
class AutomatedReportingSystem:
    def __init__(self, output_dir="reports"):
        self.output_dir = output_dir
        self.metrics_history = []
        self.trend_analyzer = TrendAnalyzer()
        
    def generate_comprehensive_report(self, performance_data, accuracy_data):
        """生成综合性能和精度报告"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = f"{self.output_dir}/comprehensive_report_{timestamp}.md"
        
        # 报告结构
        report_sections = [
            self.generate_executive_summary(performance_data, accuracy_data),
            self.generate_performance_analysis(performance_data),
            self.generate_accuracy_analysis(accuracy_data),
            self.generate_trend_analysis(),
            self.generate_recommendations(),
            self.generate_appendix()
        ]
        
        # 写入报告文件
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n\n'.join(report_sections))
        
        # 生成配套的可视化图表
        self.generate_visualizations(performance_data, accuracy_data, timestamp)
        
        return report_path
    
    def generate_executive_summary(self, perf_data, acc_data):
        """生成执行摘要"""
        summary = f"""# Triton Kernel Performance Report
        
## Executive Summary

**Performance Highlights:**
- Best performing kernel: {perf_data['best_kernel']['name']} ({perf_data['best_kernel']['throughput']:.2f} GFLOPS)
- Average speedup over baseline: {perf_data['avg_speedup']:.2f}x
- Memory efficiency: {perf_data['memory_efficiency']:.1f}%

**Accuracy Assessment:**
- All kernels maintain accuracy within {acc_data['max_error']:.2e} relative error
- FP8 quantization preserves {acc_data['fp8_retention']:.1f}% of FP32 precision
- No numerical instabilities detected

**Key Recommendations:**
{self.format_recommendations(perf_data, acc_data)}
"""
        return summary
    
    def setup_automated_dashboard(self):
        """设置自动化仪表板"""
        dashboard_config = {
            'update_frequency': '1h',  # 每小时更新
            'metrics_to_track': [
                'kernel_throughput',
                'memory_utilization', 
                'accuracy_metrics',
                'compilation_time'
            ],
            'alert_thresholds': {
                'performance_regression': 0.1,  # 10% 性能回退
                'accuracy_degradation': 1e-4,   # 精度降低
                'memory_leak': 1024*1024        # 1MB 内存泄漏
            }
        }
        
        return dashboard_config
```

### 7.2 生产部署完整检查清单

#### 7.2.1 部署前验证清单
- [ ] **性能基准测试**
  - [ ] 所有核心 kernel 性能达到预期基线
  - [ ] 端到端流水线延迟满足 SLA 要求
  - [ ] 内存使用量在合理范围内
  - [ ] GPU 利用率 > 80%

- [ ] **数值精度验证**
  - [ ] 与参考实现误差 < 1e-5
  - [ ] FP8 量化误差 < 1%
  - [ ] 边界条件处理正确
  - [ ] 无 NaN/Inf 产生

- [ ] **稳定性测试**
  - [ ] 长时间运行无性能衰减
  - [ ] 无内存泄漏
  - [ ] 异常输入处理健壮
  - [ ] 多线程安全

- [ ] **监控和告警**
  - [ ] 性能监控指标就位
  - [ ] 精度监控阈值设定
  - [ ] 自动告警系统配置
  - [ ] 降级策略准备就绪
