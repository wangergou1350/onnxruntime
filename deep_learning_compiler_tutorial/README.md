# ä»é›¶å®ç°æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ï¼šTVM é£æ ¼ç¼–è¯‘å™¨æ•™ç¨‹

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [å‰ç½®çŸ¥è¯†](#å‰ç½®çŸ¥è¯†)
3. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
4. [å®ç°è·¯çº¿å›¾](#å®ç°è·¯çº¿å›¾)
5. [ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¡†æ¶](#ç¬¬ä¸€é˜¶æ®µåŸºç¡€æ¡†æ¶)
6. [ç¬¬äºŒé˜¶æ®µï¼šå‰ç«¯å®ç°](#ç¬¬äºŒé˜¶æ®µå‰ç«¯å®ç°)
7. [ç¬¬ä¸‰é˜¶æ®µï¼šä¸­é—´è¡¨ç¤º](#ç¬¬ä¸‰é˜¶æ®µä¸­é—´è¡¨ç¤º)
8. [ç¬¬å››é˜¶æ®µï¼šä¼˜åŒ–å™¨](#ç¬¬å››é˜¶æ®µä¼˜åŒ–å™¨)
9. [ç¬¬äº”é˜¶æ®µï¼šä»£ç ç”Ÿæˆ](#ç¬¬äº”é˜¶æ®µä»£ç ç”Ÿæˆ)
10. [ç¬¬å…­é˜¶æ®µï¼šè¿è¡Œæ—¶ç³»ç»Ÿ](#ç¬¬å…­é˜¶æ®µè¿è¡Œæ—¶ç³»ç»Ÿ)
11. [é«˜çº§ç‰¹æ€§](#é«˜çº§ç‰¹æ€§)
12. [æµ‹è¯•å’ŒéªŒè¯](#æµ‹è¯•å’ŒéªŒè¯)

---

## é¡¹ç›®æ¦‚è¿°

### ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ï¼Ÿ

æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨æ˜¯ä¸€ç§ä¸“é—¨ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹ä¼˜åŒ–çš„ç¼–è¯‘ç³»ç»Ÿï¼Œå®ƒå¯ä»¥ï¼š

- **æ¨¡å‹å¯¼å…¥**ï¼šä»ä¸åŒæ¡†æ¶ï¼ˆPyTorchã€TensorFlowã€ONNXï¼‰å¯¼å…¥æ¨¡å‹
- **å›¾ä¼˜åŒ–**ï¼šå¯¹è®¡ç®—å›¾è¿›è¡Œå„ç§ä¼˜åŒ–ï¼ˆç®—å­èåˆã€å†…å­˜ä¼˜åŒ–ç­‰ï¼‰
- **ä»£ç ç”Ÿæˆ**ï¼šä¸ºä¸åŒç¡¬ä»¶å¹³å°ç”Ÿæˆé«˜æ•ˆä»£ç 
- **è¿è¡Œæ—¶è°ƒåº¦**ï¼šåœ¨è¿è¡Œæ—¶è¿›è¡ŒåŠ¨æ€ä¼˜åŒ–å’Œè°ƒåº¦

### æˆ‘ä»¬è¦å®ç°çš„ MiniTVM

æˆ‘ä»¬å°†å®ç°ä¸€ä¸ªåä¸º **MiniTVM** çš„ç®€åŒ–ç‰ˆæ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ï¼ŒåŒ…å«ï¼š

- ğŸ”¹ **å‰ç«¯**ï¼šæ”¯æŒç®€å•çš„æ·±åº¦å­¦ä¹ æ“ä½œç¬¦
- ğŸ”¹ **IRï¼ˆä¸­é—´è¡¨ç¤ºï¼‰**ï¼šå¤šçº§ IR ç³»ç»Ÿ
- ğŸ”¹ **ä¼˜åŒ–å™¨**ï¼šåŸºç¡€çš„å›¾ä¼˜åŒ–å’Œç®—å­ä¼˜åŒ–
- ğŸ”¹ **åç«¯**ï¼šæ”¯æŒ CPU å’Œ CUDA ä»£ç ç”Ÿæˆ
- ğŸ”¹ **è¿è¡Œæ—¶**ï¼šå†…å­˜ç®¡ç†å’Œä»»åŠ¡è°ƒåº¦

### å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹åï¼Œæ‚¨å°†ï¼š

- ç†è§£æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨çš„å®Œæ•´æ¶æ„
- æŒæ¡ç¼–è¯‘å™¨å‰ç«¯ã€ä¸­ç«¯ã€åç«¯çš„è®¾è®¡åŸç†
- å­¦ä¼šå®ç°å›¾ä¼˜åŒ–å’Œä»£ç ç”Ÿæˆç®—æ³•
- äº†è§£å¦‚ä½•ä¸ºä¸åŒç¡¬ä»¶å¹³å°ç”Ÿæˆé«˜æ•ˆä»£ç 
- å…·å¤‡æ‰©å±•å’Œä¼˜åŒ–ç¼–è¯‘å™¨çš„èƒ½åŠ›

---

## å‰ç½®çŸ¥è¯†

### å¿…éœ€çŸ¥è¯†
- **C++ ç¼–ç¨‹**ï¼šç†Ÿç»ƒæŒæ¡ C++11/14/17 ç‰¹æ€§
- **æ•°æ®ç»“æ„**ï¼šå›¾ã€æ ‘ã€å“ˆå¸Œè¡¨ç­‰
- **ç¼–è¯‘åŸç†**ï¼šåŸºç¡€çš„ç¼–è¯‘å™¨çŸ¥è¯†
- **æ·±åº¦å­¦ä¹ åŸºç¡€**ï¼šäº†è§£ç¥ç»ç½‘ç»œåŸºæœ¬æ¦‚å¿µ

### æ¨èçŸ¥è¯†
- **LLVM**ï¼šäº†è§£ LLVM IR å’Œä»£ç ç”Ÿæˆ
- **CUDA ç¼–ç¨‹**ï¼šGPU è®¡ç®—åŸºç¡€
- **å›¾ç®—æ³•**ï¼šæ‹“æ‰‘æ’åºã€æœ€çŸ­è·¯å¾„ç­‰
- **æ“ä½œç³»ç»Ÿ**ï¼šå†…å­˜ç®¡ç†ã€çº¿ç¨‹è°ƒåº¦

### å¼€å‘ç¯å¢ƒ
- **ç¼–è¯‘å™¨**ï¼šGCC 8+ æˆ– Clang 10+
- **æ„å»ºç³»ç»Ÿ**ï¼šCMake 3.15+
- **ä¾èµ–åº“**ï¼š
  - LLVM 12+ (ä»£ç ç”Ÿæˆ)
  - CUDA Toolkit (GPU æ”¯æŒ)
  - ONNXRuntime (æ¨¡å‹å¯¼å…¥)
  - Google Test (å•å…ƒæµ‹è¯•)

---

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Optimizer     â”‚    â”‚    Backend      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Model Importâ”‚ â”‚â”€â”€â”€â–¶â”‚ â”‚ Graph Opt   â”‚ â”‚â”€â”€â”€â–¶â”‚ â”‚ Code Gen    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ AST Builder â”‚ â”‚    â”‚ â”‚ Tensor Opt  â”‚ â”‚    â”‚ â”‚ CPU Backend â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Type System â”‚ â”‚    â”‚ â”‚ Memory Opt  â”‚ â”‚    â”‚ â”‚ GPU Backend â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚    Runtime      â”‚
                       â”‚                 â”‚
                       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                       â”‚ â”‚ Memory Mgr  â”‚ â”‚
                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                       â”‚ â”‚ Scheduler   â”‚ â”‚
                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                       â”‚ â”‚ Device API  â”‚ â”‚
                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

#### 1. å‰ç«¯ (Frontend)
- **æ¨¡å‹å¯¼å…¥å™¨**ï¼šè§£æ ONNXã€PyTorch æ¨¡å‹
- **AST æ„å»ºå™¨**ï¼šå°†æ¨¡å‹è½¬æ¢ä¸ºæŠ½è±¡è¯­æ³•æ ‘
- **ç±»å‹ç³»ç»Ÿ**ï¼šç®¡ç†å¼ é‡ç±»å‹ã€å½¢çŠ¶æ¨å¯¼

#### 2. ä¸­é—´è¡¨ç¤º (IR)
- **Graph IR**ï¼šé«˜çº§è®¡ç®—å›¾è¡¨ç¤º
- **Tensor IR**ï¼šå¼ é‡çº§ä¼˜åŒ–çš„ä¸­é—´è¡¨ç¤º
- **Schedule IR**ï¼šè°ƒåº¦å’Œå†…å­˜ç®¡ç†çš„è¡¨ç¤º

#### 3. ä¼˜åŒ–å™¨ (Optimizer)
- **å›¾ä¼˜åŒ–**ï¼šç®—å­èåˆã€æ­»ä»£ç æ¶ˆé™¤
- **å¼ é‡ä¼˜åŒ–**ï¼šå¾ªç¯ä¼˜åŒ–ã€å‘é‡åŒ–
- **å†…å­˜ä¼˜åŒ–**ï¼šå†…å­˜å¤ç”¨ã€å¸ƒå±€è½¬æ¢

#### 4. åç«¯ (Backend)
- **ä»£ç ç”Ÿæˆå™¨**ï¼šç”Ÿæˆ C++/CUDA ä»£ç 
- **ç›®æ ‡æŠ½è±¡**ï¼šæ”¯æŒå¤šç§ç¡¬ä»¶å¹³å°
- **æ€§èƒ½è°ƒä¼˜**ï¼šè‡ªåŠ¨è°ƒä¼˜å’Œç¼“å­˜

#### 5. è¿è¡Œæ—¶ (Runtime)
- **å†…å­˜ç®¡ç†**ï¼šå¼ é‡å†…å­˜åˆ†é…å’Œå›æ”¶
- **ä»»åŠ¡è°ƒåº¦**ï¼šå¹¶è¡Œæ‰§è¡Œå’Œä¾èµ–ç®¡ç†
- **è®¾å¤‡æŠ½è±¡**ï¼šç»Ÿä¸€çš„è®¾å¤‡æ¥å£

---

## å®ç°è·¯çº¿å›¾

### ç¬¬1-2å‘¨ï¼šåŸºç¡€æ¡†æ¶
- [x] é¡¹ç›®ç»“æ„è®¾è®¡
- [ ] æ ¸å¿ƒæ•°æ®ç»“æ„å®šä¹‰
- [ ] åŸºç¡€ IR ç³»ç»Ÿ
- [ ] ç®€å•çš„æµ‹è¯•æ¡†æ¶

### ç¬¬3-4å‘¨ï¼šå‰ç«¯å®ç°
- [ ] æ¨¡å‹è§£æå™¨
- [ ] AST æ„å»º
- [ ] ç±»å‹æ¨å¯¼ç³»ç»Ÿ
- [ ] é”™è¯¯å¤„ç†æœºåˆ¶

### ç¬¬5-6å‘¨ï¼šä¸­é—´è¡¨ç¤º
- [ ] Graph IR è®¾è®¡ä¸å®ç°
- [ ] Tensor IR è®¾è®¡
- [ ] IR è½¬æ¢å’ŒéªŒè¯
- [ ] å¯è§†åŒ–å·¥å…·

### ç¬¬7-8å‘¨ï¼šåŸºç¡€ä¼˜åŒ–å™¨
- [ ] å›¾ä¼˜åŒ– Pass
- [ ] ç®—å­èåˆ
- [ ] å¸¸é‡æŠ˜å 
- [ ] æ­»ä»£ç æ¶ˆé™¤

### ç¬¬9-10å‘¨ï¼šä»£ç ç”Ÿæˆ
- [ ] CPU ä»£ç ç”Ÿæˆå™¨
- [ ] CUDA ä»£ç ç”Ÿæˆå™¨
- [ ] LLVM é›†æˆ
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

### ç¬¬11-12å‘¨ï¼šè¿è¡Œæ—¶ç³»ç»Ÿ
- [ ] å†…å­˜ç®¡ç†å™¨
- [ ] ä»»åŠ¡è°ƒåº¦å™¨
- [ ] è®¾å¤‡æŠ½è±¡å±‚
- [ ] æ€§èƒ½åˆ†æå·¥å…·

### ç¬¬13-14å‘¨ï¼šé«˜çº§ç‰¹æ€§
- [ ] è‡ªåŠ¨è°ƒä¼˜
- [ ] åŠ¨æ€å½¢çŠ¶æ”¯æŒ
- [ ] é‡åŒ–æ”¯æŒ
- [ ] æ¨¡å‹å‹ç¼©

### ç¬¬15-16å‘¨ï¼šæµ‹è¯•å’Œä¼˜åŒ–
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] æ–‡æ¡£å®Œå–„
- [ ] ç¤ºä¾‹ç¨‹åº

---

## ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¡†æ¶

### é¡¹ç›®ç»“æ„

```
MiniTVM/
â”œâ”€â”€ include/                    # å¤´æ–‡ä»¶
â”‚   â”œâ”€â”€ minitvm/
â”‚   â”‚   â”œâ”€â”€ core/              # æ ¸å¿ƒæ•°æ®ç»“æ„
â”‚   â”‚   â”œâ”€â”€ frontend/          # å‰ç«¯æ¥å£
â”‚   â”‚   â”œâ”€â”€ ir/                # ä¸­é—´è¡¨ç¤º
â”‚   â”‚   â”œâ”€â”€ optimizer/         # ä¼˜åŒ–å™¨
â”‚   â”‚   â”œâ”€â”€ codegen/           # ä»£ç ç”Ÿæˆ
â”‚   â”‚   â””â”€â”€ runtime/           # è¿è¡Œæ—¶
â”œâ”€â”€ src/                       # æºæ–‡ä»¶
â”‚   â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ ir/
â”‚   â”œâ”€â”€ optimizer/
â”‚   â”œâ”€â”€ codegen/
â”‚   â””â”€â”€ runtime/
â”œâ”€â”€ tests/                     # æµ‹è¯•æ–‡ä»¶
â”‚   â”œâ”€â”€ unit/                  # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ integration/           # é›†æˆæµ‹è¯•
â”‚   â””â”€â”€ benchmarks/            # æ€§èƒ½æµ‹è¯•
â”œâ”€â”€ examples/                  # ç¤ºä¾‹ç¨‹åº
â”œâ”€â”€ docs/                      # æ–‡æ¡£
â”œâ”€â”€ third_party/               # ç¬¬ä¸‰æ–¹åº“
â”œâ”€â”€ tools/                     # å·¥å…·è„šæœ¬
â”œâ”€â”€ CMakeLists.txt            # æ„å»ºé…ç½®
â””â”€â”€ README.md                 # é¡¹ç›®è¯´æ˜
```

### æ ¸å¿ƒæ•°æ®ç»“æ„

#### 1. å¼ é‡ (Tensor)

```cpp
// include/minitvm/core/tensor.h
#pragma once

#include <vector>
#include <memory>
#include <string>

namespace minitvm {

enum class DataType {
    FLOAT32,
    FLOAT64,
    INT32,
    INT64,
    BOOL,
    // æ›´å¤šç±»å‹...
};

enum class DeviceType {
    CPU,
    CUDA,
    // æ›´å¤šè®¾å¤‡...
};

struct Shape {
    std::vector<int64_t> dims;
    
    int64_t size() const;
    int64_t rank() const { return dims.size(); }
    bool is_scalar() const { return dims.empty(); }
    std::string to_string() const;
};

class Tensor {
public:
    Tensor(const Shape& shape, DataType dtype, DeviceType device);
    
    const Shape& shape() const { return shape_; }
    DataType dtype() const { return dtype_; }
    DeviceType device() const { return device_; }
    
    // æ•°æ®è®¿é—®
    void* data() const { return data_.get(); }
    size_t size_bytes() const;
    
    // æ“ä½œ
    Tensor reshape(const Shape& new_shape) const;
    Tensor to_device(DeviceType device) const;
    
private:
    Shape shape_;
    DataType dtype_;
    DeviceType device_;
    std::shared_ptr<void> data_;
};

} // namespace minitvm
```

#### 2. æ“ä½œç¬¦ (Operator)

```cpp
// include/minitvm/core/operator.h
#pragma once

#include "tensor.h"
#include <vector>
#include <unordered_map>

namespace minitvm {

class Operator {
public:
    virtual ~Operator() = default;
    
    virtual std::string name() const = 0;
    virtual std::vector<Tensor> compute(const std::vector<Tensor>& inputs) = 0;
    virtual std::vector<Shape> infer_shape(const std::vector<Shape>& input_shapes) = 0;
    
protected:
    std::unordered_map<std::string, std::string> attrs_;
};

// å…·ä½“æ“ä½œç¬¦ç¤ºä¾‹
class AddOperator : public Operator {
public:
    std::string name() const override { return "add"; }
    std::vector<Tensor> compute(const std::vector<Tensor>& inputs) override;
    std::vector<Shape> infer_shape(const std::vector<Shape>& input_shapes) override;
};

class MatMulOperator : public Operator {
public:
    std::string name() const override { return "matmul"; }
    std::vector<Tensor> compute(const std::vector<Tensor>& inputs) override;
    std::vector<Shape> infer_shape(const std::vector<Shape>& input_shapes) override;
};

} // namespace minitvm
```

#### 3. è®¡ç®—å›¾ (Graph)

```cpp
// include/minitvm/core/graph.h
#pragma once

#include "operator.h"
#include <memory>
#include <vector>
#include <unordered_map>

namespace minitvm {

class Node {
public:
    using NodePtr = std::shared_ptr<Node>;
    
    Node(std::shared_ptr<Operator> op, const std::vector<NodePtr>& inputs);
    
    const Operator& op() const { return *op_; }
    const std::vector<NodePtr>& inputs() const { return inputs_; }
    const std::vector<NodePtr>& outputs() const { return outputs_; }
    
    void add_output(NodePtr output) { outputs_.push_back(output); }
    
    // è°ƒè¯•ä¿¡æ¯
    std::string to_string() const;
    
private:
    std::shared_ptr<Operator> op_;
    std::vector<NodePtr> inputs_;
    std::vector<NodePtr> outputs_;
    
    // è¿è¡Œæ—¶ä¿¡æ¯
    std::vector<Shape> output_shapes_;
    bool shape_inferred_ = false;
};

class Graph {
public:
    using NodePtr = std::shared_ptr<Node>;
    
    // æ„å»ºæ¥å£
    NodePtr add_node(std::shared_ptr<Operator> op, const std::vector<NodePtr>& inputs);
    void set_inputs(const std::vector<NodePtr>& inputs) { inputs_ = inputs; }
    void set_outputs(const std::vector<NodePtr>& outputs) { outputs_ = outputs; }
    
    // è®¿é—®æ¥å£
    const std::vector<NodePtr>& inputs() const { return inputs_; }
    const std::vector<NodePtr>& outputs() const { return outputs_; }
    const std::vector<NodePtr>& nodes() const { return nodes_; }
    
    // åˆ†æå’Œä¼˜åŒ–
    void infer_shapes();
    void validate();
    std::vector<NodePtr> topological_sort();
    
    // å¯è§†åŒ–
    std::string to_dot() const;
    void visualize(const std::string& filename) const;
    
private:
    std::vector<NodePtr> nodes_;
    std::vector<NodePtr> inputs_;
    std::vector<NodePtr> outputs_;
};

} // namespace minitvm
```

### ç¬¬ä¸€é˜¶æ®µå®ç°ä»»åŠ¡

#### ä»»åŠ¡1ï¼šåŸºç¡€æ•°æ®ç»“æ„ (3å¤©)

1. **å®ç° Shape ç±»**
   ```cpp
   // src/core/tensor.cpp
   int64_t Shape::size() const {
       int64_t result = 1;
       for (auto dim : dims) {
           result *= dim;
       }
       return result;
   }
   
   std::string Shape::to_string() const {
       std::string result = "(";
       for (size_t i = 0; i < dims.size(); ++i) {
           if (i > 0) result += ", ";
           result += std::to_string(dims[i]);
       }
       result += ")";
       return result;
   }
   ```

2. **å®ç° Tensor ç±»**
   ```cpp
   Tensor::Tensor(const Shape& shape, DataType dtype, DeviceType device)
       : shape_(shape), dtype_(dtype), device_(device) {
       // åˆ†é…å†…å­˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
       size_t bytes = size_bytes();
       if (device == DeviceType::CPU) {
           data_ = std::shared_ptr<void>(std::malloc(bytes), std::free);
       } else {
           // CUDA å†…å­˜åˆ†é…
           void* ptr;
           cudaMalloc(&ptr, bytes);
           data_ = std::shared_ptr<void>(ptr, [](void* p) { cudaFree(p); });
       }
   }
   ```

#### ä»»åŠ¡2ï¼šåŸºç¡€æ“ä½œç¬¦ (3å¤©)

1. **å®ç° AddOperator**
   ```cpp
   // src/core/operators/add.cpp
   std::vector<Tensor> AddOperator::compute(const std::vector<Tensor>& inputs) {
       assert(inputs.size() == 2);
       const auto& a = inputs[0];
       const auto& b = inputs[1];
       
       // å½¢çŠ¶æ£€æŸ¥
       assert(a.shape().dims == b.shape().dims);
       
       // åˆ›å»ºè¾“å‡ºå¼ é‡
       Tensor output(a.shape(), a.dtype(), a.device());
       
       // CPU å®ç°ï¼ˆç®€åŒ–ï¼‰
       if (a.device() == DeviceType::CPU && a.dtype() == DataType::FLOAT32) {
           const float* a_data = static_cast<const float*>(a.data());
           const float* b_data = static_cast<const float*>(b.data());
           float* out_data = static_cast<float*>(output.data());
           
           int64_t size = a.shape().size();
           for (int64_t i = 0; i < size; ++i) {
               out_data[i] = a_data[i] + b_data[i];
           }
       }
       
       return {output};
   }
   ```

#### ä»»åŠ¡3ï¼šè®¡ç®—å›¾åŸºç¡€ (4å¤©)

1. **å®ç° Node ç±»**
   ```cpp
   // src/core/graph.cpp
   Node::Node(std::shared_ptr<Operator> op, const std::vector<NodePtr>& inputs)
       : op_(op), inputs_(inputs) {
       // å»ºç«‹åå‘è¿æ¥
       for (auto& input : inputs_) {
           input->add_output(shared_from_this());
       }
   }
   ```

2. **å®ç°åŸºç¡€å›¾æ“ä½œ**
   ```cpp
   Graph::NodePtr Graph::add_node(std::shared_ptr<Operator> op, 
                                   const std::vector<NodePtr>& inputs) {
       auto node = std::make_shared<Node>(op, inputs);
       nodes_.push_back(node);
       return node;
   }
   
   void Graph::infer_shapes() {
       auto sorted_nodes = topological_sort();
       for (auto& node : sorted_nodes) {
           // æ¨å¯¼å½¢çŠ¶
           std::vector<Shape> input_shapes;
           for (auto& input : node->inputs()) {
               // è·å–è¾“å…¥å½¢çŠ¶...
           }
           auto output_shapes = node->op().infer_shape(input_shapes);
           // è®¾ç½®è¾“å‡ºå½¢çŠ¶...
       }
   }
   ```

### ç¬¬ä¸€é˜¶æ®µæµ‹è¯•

```cpp
// tests/unit/test_basic.cpp
#include <gtest/gtest.h>
#include "minitvm/core/tensor.h"
#include "minitvm/core/graph.h"

using namespace minitvm;

TEST(TensorTest, BasicOperations) {
    Shape shape({2, 3});
    Tensor tensor(shape, DataType::FLOAT32, DeviceType::CPU);
    
    EXPECT_EQ(tensor.shape().size(), 6);
    EXPECT_EQ(tensor.dtype(), DataType::FLOAT32);
}

TEST(GraphTest, SimpleGraph) {
    Graph graph;
    
    // åˆ›å»ºè¾“å…¥èŠ‚ç‚¹
    auto input1 = graph.add_node(std::make_shared<InputOperator>(), {});
    auto input2 = graph.add_node(std::make_shared<InputOperator>(), {});
    
    // åˆ›å»ºåŠ æ³•èŠ‚ç‚¹
    auto add_node = graph.add_node(std::make_shared<AddOperator>(), {input1, input2});
    
    graph.set_inputs({input1, input2});
    graph.set_outputs({add_node});
    
    EXPECT_EQ(graph.nodes().size(), 3);
    
    // æµ‹è¯•æ‹“æ‰‘æ’åº
    auto sorted = graph.topological_sort();
    EXPECT_EQ(sorted.size(), 3);
}
```

---

## å…³é”®å­¦ä¹ è¦ç‚¹

### ç¬¬ä¸€é˜¶æ®µå­¦ä¹ ç›®æ ‡

1. **ç†è§£åŸºç¡€æŠ½è±¡**
   - å¼ é‡çš„è¡¨ç¤ºå’Œæ“ä½œ
   - æ“ä½œç¬¦çš„æŠ½è±¡æ¥å£
   - è®¡ç®—å›¾çš„æ•°æ®ç»“æ„

2. **æŒæ¡è®¾è®¡æ¨¡å¼**
   - è®¿é—®è€…æ¨¡å¼ï¼ˆIR éå†ï¼‰
   - å·¥å‚æ¨¡å¼ï¼ˆæ“ä½œç¬¦åˆ›å»ºï¼‰
   - è§‚å¯Ÿè€…æ¨¡å¼ï¼ˆå›¾ä¼˜åŒ–ï¼‰

3. **ç†Ÿæ‚‰æ„å»ºç³»ç»Ÿ**
   - CMake é…ç½®
   - ç¬¬ä¸‰æ–¹åº“é›†æˆ
   - æµ‹è¯•æ¡†æ¶ä½¿ç”¨

### ä¸‹ä¸€é˜¶æ®µé¢„å‘Š

åœ¨ä¸‹ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å°†å®ç°ï¼š
- å®Œæ•´çš„å‰ç«¯ç³»ç»Ÿï¼ˆæ¨¡å‹å¯¼å…¥ï¼‰
- ç±»å‹ç³»ç»Ÿå’Œå½¢çŠ¶æ¨å¯¼
- é”™è¯¯å¤„ç†å’Œè°ƒè¯•å·¥å…·
- æ›´å¤šçš„å†…ç½®æ“ä½œç¬¦

---

è¿™ä¸ªæ•™ç¨‹å°†åˆ†å¤šä¸ªéƒ¨åˆ†è¯¦ç»†è®²è§£ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰å…·ä½“çš„å®ç°ä»»åŠ¡å’Œå­¦ä¹ ç›®æ ‡ã€‚æ‚¨æƒ³ä»å“ªä¸ªéƒ¨åˆ†å¼€å§‹æ·±å…¥å­¦ä¹ ï¼Ÿ
