# 第四阶段：代码生成与后端

## 概述

第四阶段将实现 MiniTVM 的代码生成后端，包括多目标代码生成、运行时系统、执行引擎等。这是深度学习编译器的最终环节，将优化后的IR转换为高性能的可执行代码。

## 实现计划

### Day 17-19: 多目标代码生成
### Day 20-21: 运行时系统
### Day 22: 执行引擎与JIT

---

## Day 17-19: 多目标代码生成

### 代码生成基础框架

```cpp
// include/minitvm/codegen/codegen_base.h
#pragma once

#include "minitvm/ir/tensor_ir.h"
#include "minitvm/core/target.h"
#include <string>
#include <memory>

namespace minitvm {
namespace codegen {

// 目标设备信息
struct Target {
    enum class Kind {
        LLVM,     // LLVM后端 (CPU)
        CUDA,     // CUDA C++
        OPENCL,   // OpenCL C
        METAL,    // Metal Shading Language
        VULKAN    // Vulkan SPIR-V
    };
    
    Kind kind;
    std::string device_name;
    std::unordered_map<std::string, std::string> attributes;
    
    // 便利构造函数
    static Target cpu(const std::string& arch = "x86_64");
    static Target cuda(const std::string& arch = "sm_75");
    static Target opencl(const std::string& device = "");
    
    std::string to_string() const;
};

// 代码生成选项
struct CodeGenOptions {
    int optimization_level = 2;  // 0-3
    bool enable_vectorization = true;
    bool enable_loop_unrolling = true;
    bool enable_fast_math = false;
    bool debug_info = false;
    size_t shared_memory_size = 48 * 1024;  // CUDA共享内存
    
    std::unordered_map<std::string, std::string> custom_options;
};

// 生成的代码信息
struct GeneratedCode {
    std::string source_code;
    std::string binary_code;  // 编译后的二进制
    std::vector<std::string> dependencies;  // 依赖的库
    std::unordered_map<std::string, std::string> metadata;
    
    // 函数信息
    struct FunctionInfo {
        std::string name;
        std::vector<std::string> arg_types;
        size_t workspace_size = 0;
        size_t shared_memory_size = 0;
    };
    std::vector<FunctionInfo> functions;
};

// 代码生成器基类
class CodeGenerator {
public:
    CodeGenerator(const Target& target, const CodeGenOptions& options = {})
        : target_(target), options_(options) {}
    
    virtual ~CodeGenerator() = default;
    
    // 核心接口
    virtual GeneratedCode generate(const ir::TensorIRFunction& func) = 0;
    virtual GeneratedCode generate_module(const std::vector<ir::TensorIRFunction>& functions);
    
    // 配置
    const Target& target() const { return target_; }
    const CodeGenOptions& options() const { return options_; }
    void set_options(const CodeGenOptions& options) { options_ = options; }
    
    // 工具函数
    virtual std::string get_type_string(DataType dtype) const = 0;
    virtual std::string get_intrinsic_name(const std::string& op) const = 0;
    
protected:
    Target target_;
    CodeGenOptions options_;
    
    // 代码生成助手
    virtual std::string generate_function_signature(const ir::TensorIRFunction& func) = 0;
    virtual std::string generate_function_body(const ir::TensorIRFunction& func) = 0;
    virtual std::string generate_prologue() = 0;
    virtual std::string generate_epilogue() = 0;
};

// 代码生成器工厂
class CodeGeneratorFactory {
public:
    static std::unique_ptr<CodeGenerator> create(const Target& target, 
                                               const CodeGenOptions& options = {});
    
    static void register_generator(Target::Kind kind, 
                                 std::function<std::unique_ptr<CodeGenerator>(const Target&, const CodeGenOptions&)> factory);
    
    static std::vector<Target::Kind> get_supported_targets();
    
private:
    static std::unordered_map<Target::Kind, 
                            std::function<std::unique_ptr<CodeGenerator>(const Target&, const CodeGenOptions&)>> 
        generators_;
};

} // namespace codegen
} // namespace minitvm
```

### LLVM后端实现

```cpp
// include/minitvm/codegen/llvm_codegen.h
#pragma once

#include "codegen_base.h"
#include "minitvm/ir/tensor_ir.h"

#include <llvm/IR/Module.h>
#include <llvm/IR/IRBuilder.h>
#include <llvm/IR/LLVMContext.h>
#include <llvm/IR/Function.h>
#include <llvm/IR/Value.h>
#include <llvm/ExecutionEngine/ExecutionEngine.h>
#include <llvm/ExecutionEngine/JITSymbol.h>

namespace minitvm {
namespace codegen {

class LLVMCodeGenerator : public CodeGenerator {
public:
    LLVMCodeGenerator(const Target& target, const CodeGenOptions& options = {});
    ~LLVMCodeGenerator();
    
    GeneratedCode generate(const ir::TensorIRFunction& func) override;
    GeneratedCode generate_module(const std::vector<ir::TensorIRFunction>& functions) override;
    
    std::string get_type_string(DataType dtype) const override;
    std::string get_intrinsic_name(const std::string& op) const override;
    
    // LLVM特定接口
    llvm::Module* get_module() const { return module_.get(); }
    void* get_function_pointer(const std::string& name);
    
protected:
    std::string generate_function_signature(const ir::TensorIRFunction& func) override;
    std::string generate_function_body(const ir::TensorIRFunction& func) override;
    std::string generate_prologue() override;
    std::string generate_epilogue() override;
    
private:
    std::unique_ptr<llvm::LLVMContext> context_;
    std::unique_ptr<llvm::Module> module_;
    std::unique_ptr<llvm::IRBuilder<>> builder_;
    std::unique_ptr<llvm::ExecutionEngine> execution_engine_;
    
    // 类型映射
    llvm::Type* get_llvm_type(DataType dtype);
    llvm::Type* get_tensor_type(const Shape& shape, DataType dtype);
    
    // 代码生成访问者
    class LLVMCodeGenVisitor;
    std::unique_ptr<LLVMCodeGenVisitor> visitor_;
    
    // 内存管理
    llvm::Value* create_tensor_alloca(const std::string& name, const Shape& shape, DataType dtype);
    llvm::Value* get_element_ptr(llvm::Value* tensor, const std::vector<llvm::Value*>& indices,
                                const Shape& shape);
    
    // 向量化支持
    void generate_vectorized_loop(llvm::Value* start, llvm::Value* end, llvm::Value* step,
                                 std::function<void(llvm::Value*)> body_gen);
    
    // 内置函数
    void declare_intrinsics();
    llvm::Function* create_memcpy_function();
    llvm::Function* create_math_function(const std::string& name, DataType dtype);
};

// LLVM代码生成访问者
class LLVMCodeGenerator::LLVMCodeGenVisitor {
public:
    LLVMCodeGenVisitor(LLVMCodeGenerator* codegen) : codegen_(codegen) {}
    
    // 表达式访问
    llvm::Value* visit_expr(const ir::ExprPtr& expr);
    llvm::Value* visit_var(const ir::Var* var);
    llvm::Value* visit_int_imm(const ir::IntImm* imm);
    llvm::Value* visit_float_imm(const ir::FloatImm* imm);
    llvm::Value* visit_binary_op(const ir::BinaryOp* op);
    llvm::Value* visit_unary_op(const ir::UnaryOp* op);
    llvm::Value* visit_select(const ir::Select* select);
    llvm::Value* visit_call(const ir::Call* call);
    llvm::Value* visit_tensor_load(const ir::TensorLoad* load);
    
    // 语句访问
    void visit_stmt(const ir::StmtPtr& stmt);
    void visit_tensor_store(const ir::TensorStore* store);
    void visit_for(const ir::For* for_loop);
    void visit_if_then_else(const ir::IfThenElse* if_stmt);
    void visit_block(const ir::Block* block);
    void visit_allocate(const ir::Allocate* alloc);
    
private:
    LLVMCodeGenerator* codegen_;
    std::unordered_map<std::string, llvm::Value*> var_map_;
    std::unordered_map<std::string, llvm::AllocaInst*> tensor_map_;
    
    // 二元操作生成
    llvm::Value* generate_binary_op(ir::BinaryOpType op_type, llvm::Value* lhs, llvm::Value* rhs);
    llvm::Value* generate_unary_op(ir::UnaryOpType op_type, llvm::Value* operand);
    
    // 类型转换
    llvm::Value* cast_value(llvm::Value* value, llvm::Type* target_type);
};

} // namespace codegen
} // namespace minitvm
```

### CUDA后端实现

```cpp
// include/minitvm/codegen/cuda_codegen.h
#pragma once

#include "codegen_base.h"
#include <sstream>

namespace minitvm {
namespace codegen {

class CUDACodeGenerator : public CodeGenerator {
public:
    CUDACodeGenerator(const Target& target, const CodeGenOptions& options = {});
    
    GeneratedCode generate(const ir::TensorIRFunction& func) override;
    
    std::string get_type_string(DataType dtype) const override;
    std::string get_intrinsic_name(const std::string& op) const override;
    
protected:
    std::string generate_function_signature(const ir::TensorIRFunction& func) override;
    std::string generate_function_body(const ir::TensorIRFunction& func) override;
    std::string generate_prologue() override;
    std::string generate_epilogue() override;
    
private:
    std::ostringstream code_;
    int indent_level_ = 0;
    
    // 代码格式化
    void emit_line(const std::string& line);
    void emit_indent();
    void increase_indent() { indent_level_++; }
    void decrease_indent() { indent_level_--; }
    
    // CUDA特定代码生成
    std::string generate_kernel_signature(const ir::TensorIRFunction& func);
    std::string generate_host_function(const ir::TensorIRFunction& func);
    std::string generate_thread_indexing();
    std::string generate_shared_memory_declaration();
    
    // 访问者
    class CUDACodeGenVisitor;
    std::unique_ptr<CUDACodeGenVisitor> visitor_;
    
    // 内存层次结构
    enum class MemoryScope {
        GLOBAL,
        SHARED,
        LOCAL,
        TEXTURE
    };
    
    std::string get_memory_scope_prefix(MemoryScope scope);
    
    // 线程块配置
    struct BlockConfig {
        int block_x = 256;
        int block_y = 1;
        int block_z = 1;
        size_t shared_memory = 0;
    };
    
    BlockConfig analyze_block_config(const ir::TensorIRFunction& func);
    
    // 向量化和合并访问
    bool can_vectorize_access(const ir::TensorLoad* load);
    std::string generate_vectorized_load(const ir::TensorLoad* load);
    std::string generate_vectorized_store(const ir::TensorStore* store);
};

// CUDA代码生成访问者
class CUDACodeGenerator::CUDACodeGenVisitor {
public:
    CUDACodeGenVisitor(CUDACodeGenerator* codegen) : codegen_(codegen) {}
    
    // 表达式生成
    std::string visit_expr(const ir::ExprPtr& expr);
    std::string visit_var(const ir::Var* var);
    std::string visit_int_imm(const ir::IntImm* imm);
    std::string visit_float_imm(const ir::FloatImm* imm);
    std::string visit_binary_op(const ir::BinaryOp* op);
    std::string visit_unary_op(const ir::UnaryOp* op);
    std::string visit_select(const ir::Select* select);
    std::string visit_call(const ir::Call* call);
    std::string visit_tensor_load(const ir::TensorLoad* load);
    
    // 语句生成
    void visit_stmt(const ir::StmtPtr& stmt);
    void visit_tensor_store(const ir::TensorStore* store);
    void visit_for(const ir::For* for_loop);
    void visit_if_then_else(const ir::IfThenElse* if_stmt);
    void visit_block(const ir::Block* block);
    void visit_allocate(const ir::Allocate* alloc);
    
private:
    CUDACodeGenerator* codegen_;
    std::unordered_map<std::string, std::string> var_map_;
    
    // CUDA特定操作
    std::string generate_thread_index(const std::string& axis);
    std::string generate_block_index(const std::string& axis);
    std::string generate_grid_stride_loop(const ir::For* for_loop);
    
    // 数学函数映射
    std::string get_cuda_math_function(const std::string& op, DataType dtype);
};

} // namespace codegen
} // namespace minitvm
```

### 代码生成实现示例

```cpp
// src/codegen/llvm_codegen.cpp
#include "minitvm/codegen/llvm_codegen.h"
#include <llvm/IR/Verifier.h>
#include <llvm/Support/TargetSelect.h>
#include <llvm/ExecutionEngine/Orc/LLJIT.h>

namespace minitvm {
namespace codegen {

LLVMCodeGenerator::LLVMCodeGenerator(const Target& target, const CodeGenOptions& options)
    : CodeGenerator(target, options) {
    
    llvm::InitializeNativeTarget();
    llvm::InitializeNativeTargetAsmPrinter();
    
    context_ = std::make_unique<llvm::LLVMContext>();
    module_ = std::make_unique<llvm::Module>("minitvm_module", *context_);
    builder_ = std::make_unique<llvm::IRBuilder<>>(*context_);
    visitor_ = std::make_unique<LLVMCodeGenVisitor>(this);
    
    declare_intrinsics();
}

GeneratedCode LLVMCodeGenerator::generate(const ir::TensorIRFunction& func) {
    // 创建函数
    auto llvm_func_type = llvm::FunctionType::get(
        llvm::Type::getVoidTy(*context_),
        {}, // 参数类型将根据TensorIR函数推导
        false
    );
    
    auto llvm_func = llvm::Function::Create(
        llvm_func_type,
        llvm::Function::ExternalLinkage,
        func.name(),
        module_.get()
    );
    
    // 创建基本块
    auto entry_bb = llvm::BasicBlock::Create(*context_, "entry", llvm_func);
    builder_->SetInsertPoint(entry_bb);
    
    // 生成函数体
    visitor_->visit_stmt(func.body());
    
    // 添加返回语句
    builder_->CreateRetVoid();
    
    // 验证函数
    if (llvm::verifyFunction(*llvm_func, &llvm::errs())) {
        throw std::runtime_error("Generated LLVM function is invalid");
    }
    
    // 生成代码
    GeneratedCode result;
    result.source_code = generate_llvm_ir_string();
    result.functions.push_back({func.name(), {}, 0, 0});
    
    return result;
}

llvm::Value* LLVMCodeGenerator::LLVMCodeGenVisitor::visit_binary_op(const ir::BinaryOp* op) {
    auto lhs = visit_expr(op->lhs());
    auto rhs = visit_expr(op->rhs());
    
    switch (op->op_type()) {
        case ir::BinaryOpType::ADD:
            if (lhs->getType()->isFloatingPointTy()) {
                return codegen_->builder_->CreateFAdd(lhs, rhs, "add");
            } else {
                return codegen_->builder_->CreateAdd(lhs, rhs, "add");
            }
        case ir::BinaryOpType::MUL:
            if (lhs->getType()->isFloatingPointTy()) {
                return codegen_->builder_->CreateFMul(lhs, rhs, "mul");
            } else {
                return codegen_->builder_->CreateMul(lhs, rhs, "mul");
            }
        case ir::BinaryOpType::LT:
            if (lhs->getType()->isFloatingPointTy()) {
                return codegen_->builder_->CreateFCmpOLT(lhs, rhs, "lt");
            } else {
                return codegen_->builder_->CreateICmpSLT(lhs, rhs, "lt");
            }
        // 其他操作...
        default:
            throw std::runtime_error("Unsupported binary operation");
    }
}

void LLVMCodeGenerator::LLVMCodeGenVisitor::visit_for(const ir::For* for_loop) {
    auto func = codegen_->builder_->GetInsertBlock()->getParent();
    
    // 创建循环基本块
    auto preheader_bb = codegen_->builder_->GetInsertBlock();
    auto loop_bb = llvm::BasicBlock::Create(*codegen_->context_, "loop", func);
    auto body_bb = llvm::BasicBlock::Create(*codegen_->context_, "body", func);
    auto exit_bb = llvm::BasicBlock::Create(*codegen_->context_, "exit", func);
    
    // 计算循环边界
    auto start = visit_expr(for_loop->min());
    auto extent = visit_expr(for_loop->extent());
    auto end = codegen_->builder_->CreateAdd(start, extent, "end");
    
    // 跳转到循环头
    codegen_->builder_->CreateBr(loop_bb);
    
    // 循环头：PHI节点和条件检查
    codegen_->builder_->SetInsertPoint(loop_bb);
    auto phi = codegen_->builder_->CreatePHI(start->getType(), 2, for_loop->loop_var());
    phi->addIncoming(start, preheader_bb);
    
    // 保存循环变量
    var_map_[for_loop->loop_var()] = phi;
    
    auto cond = codegen_->builder_->CreateICmpSLT(phi, end, "cond");
    codegen_->builder_->CreateCondBr(cond, body_bb, exit_bb);
    
    // 循环体
    codegen_->builder_->SetInsertPoint(body_bb);
    visit_stmt(for_loop->body());
    
    // 循环增量
    auto step = llvm::ConstantInt::get(phi->getType(), 1);
    auto next = codegen_->builder_->CreateAdd(phi, step, "next");
    phi->addIncoming(next, body_bb);
    
    codegen_->builder_->CreateBr(loop_bb);
    
    // 循环退出
    codegen_->builder_->SetInsertPoint(exit_bb);
    
    // 清理循环变量
    var_map_.erase(for_loop->loop_var());
}

} // namespace codegen
} // namespace minitvm
```

### CUDA代码生成实现

```cpp
// src/codegen/cuda_codegen.cpp
#include "minitvm/codegen/cuda_codegen.h"

namespace minitvm {
namespace codegen {

CUDACodeGenerator::CUDACodeGenerator(const Target& target, const CodeGenOptions& options)
    : CodeGenerator(target, options) {
    visitor_ = std::make_unique<CUDACodeGenVisitor>(this);
}

GeneratedCode CUDACodeGenerator::generate(const ir::TensorIRFunction& func) {
    code_.str("");
    code_.clear();
    indent_level_ = 0;
    
    // 生成CUDA代码
    emit_line(generate_prologue());
    emit_line(generate_kernel_signature(func));
    emit_line("{");
    increase_indent();
    
    emit_line(generate_thread_indexing());
    emit_line(generate_shared_memory_declaration());
    emit_line("");
    
    // 生成函数体
    visitor_->visit_stmt(func.body());
    
    decrease_indent();
    emit_line("}");
    
    // 生成主机函数
    emit_line("");
    emit_line(generate_host_function(func));
    
    GeneratedCode result;
    result.source_code = code_.str();
    
    auto block_config = analyze_block_config(func);
    GeneratedCode::FunctionInfo func_info;
    func_info.name = func.name();
    func_info.shared_memory_size = block_config.shared_memory;
    result.functions.push_back(func_info);
    
    return result;
}

std::string CUDACodeGenerator::generate_kernel_signature(const ir::TensorIRFunction& func) {
    std::ostringstream sig;
    sig << "__global__ void " << func.name() << "_kernel(";
    
    for (size_t i = 0; i < func.param_names().size(); ++i) {
        if (i > 0) sig << ", ";
        
        DataType dtype = func.param_dtypes()[i];
        const Shape& shape = func.param_shapes()[i];
        
        sig << get_type_string(dtype) << "* " << func.param_names()[i];
    }
    
    sig << ")";
    return sig.str();
}

std::string CUDACodeGenerator::generate_thread_indexing() {
    return R"(
    int tid = threadIdx.x;
    int bid = blockIdx.x;
    int gid = bid * blockDim.x + tid;
    int grid_size = gridDim.x * blockDim.x;
)";
}

std::string CUDACodeGenerator::CUDACodeGenVisitor::visit_for(const ir::For* for_loop) {
    std::ostringstream code;
    
    if (for_loop->loop_type() == ir::For::LoopType::PARALLEL) {
        // 生成网格跨步循环
        auto start = visit_expr(for_loop->min());
        auto extent = visit_expr(for_loop->extent());
        
        code << "for (int " << for_loop->loop_var() << " = gid + " << start 
             << "; " << for_loop->loop_var() << " < " << start << " + " << extent
             << "; " << for_loop->loop_var() << " += grid_size) {\n";
        
        codegen_->increase_indent();
        visit_stmt(for_loop->body());
        codegen_->decrease_indent();
        
        code << "}\n";
    } else {
        // 生成常规串行循环
        auto start = visit_expr(for_loop->min());
        auto extent = visit_expr(for_loop->extent());
        
        code << "for (int " << for_loop->loop_var() << " = " << start 
             << "; " << for_loop->loop_var() << " < " << start << " + " << extent
             << "; " << for_loop->loop_var() << "++) {\n";
        
        codegen_->increase_indent();
        visit_stmt(for_loop->body());
        codegen_->decrease_indent();
        
        code << "}\n";
    }
    
    return code.str();
}

std::string CUDACodeGenerator::generate_host_function(const ir::TensorIRFunction& func) {
    std::ostringstream code;
    
    // 主机函数签名
    code << "void " << func.name() << "(";
    for (size_t i = 0; i < func.param_names().size(); ++i) {
        if (i > 0) code << ", ";
        code << get_type_string(func.param_dtypes()[i]) << "* " << func.param_names()[i];
    }
    code << ") {\n";
    
    // 计算网格和块配置
    auto block_config = analyze_block_config(func);
    code << "    dim3 block(" << block_config.block_x << ", " 
         << block_config.block_y << ", " << block_config.block_z << ");\n";
    code << "    dim3 grid((size + block.x - 1) / block.x);\n";
    
    // 启动内核
    code << "    " << func.name() << "_kernel<<<grid, block, " 
         << block_config.shared_memory << ">>>(";
    for (size_t i = 0; i < func.param_names().size(); ++i) {
        if (i > 0) code << ", ";
        code << func.param_names()[i];
    }
    code << ");\n";
    code << "    cudaDeviceSynchronize();\n";
    code << "}\n";
    
    return code.str();
}

} // namespace codegen
} // namespace minitvm
```

---

## Day 20-21: 运行时系统

### 运行时基础框架

```cpp
// include/minitvm/runtime/runtime_base.h
#pragma once

#include "minitvm/core/tensor.h"
#include "minitvm/core/device.h"
#include <memory>
#include <unordered_map>

namespace minitvm {
namespace runtime {

// 可执行模块
class Module {
public:
    virtual ~Module() = default;
    
    // 获取函数
    virtual void* get_function(const std::string& name) = 0;
    
    // 执行函数
    virtual void call_function(const std::string& name, const std::vector<Tensor>& args) = 0;
    
    // 模块信息
    virtual std::vector<std::string> list_functions() const = 0;
    virtual std::string get_source() const = 0;
    
    // 资源管理
    virtual size_t get_memory_usage() const = 0;
    virtual void optimize_memory() = 0;
};

using ModulePtr = std::shared_ptr<Module>;

// 设备API抽象
class DeviceAPI {
public:
    virtual ~DeviceAPI() = default;
    
    // 内存管理
    virtual void* allocate(size_t size, size_t alignment = 0) = 0;
    virtual void deallocate(void* ptr) = 0;
    virtual void copy(void* dst, const void* src, size_t size) = 0;
    virtual void copy_async(void* dst, const void* src, size_t size, void* stream = nullptr) = 0;
    
    // 同步操作
    virtual void synchronize() = 0;
    virtual void* create_stream() = 0;
    virtual void destroy_stream(void* stream) = 0;
    
    // 设备信息
    virtual DeviceType device_type() const = 0;
    virtual int device_id() const = 0;
    virtual size_t get_memory_info() const = 0;
    
    // 工厂方法
    static std::unique_ptr<DeviceAPI> create(DeviceType device_type, int device_id = 0);
};

// 内存管理器
class MemoryManager {
public:
    explicit MemoryManager(DeviceType device_type);
    ~MemoryManager();
    
    // 内存分配
    void* allocate(size_t size, size_t alignment = 64);
    void deallocate(void* ptr);
    
    // 内存池管理
    void enable_memory_pool(bool enable) { pool_enabled_ = enable; }
    void set_pool_size(size_t size) { pool_size_ = size; }
    
    // 统计信息
    size_t get_allocated_memory() const { return allocated_memory_; }
    size_t get_peak_memory() const { return peak_memory_; }
    size_t get_pool_memory() const { return pool_memory_; }
    
    // 内存优化
    void garbage_collect();
    void defragment();
    
private:
    DeviceType device_type_;
    std::unique_ptr<DeviceAPI> device_api_;
    
    // 内存池
    struct MemoryBlock {
        void* ptr;
        size_t size;
        bool in_use;
        size_t alignment;
    };
    
    std::vector<MemoryBlock> memory_pool_;
    bool pool_enabled_ = true;
    size_t pool_size_ = 1024 * 1024 * 1024; // 1GB
    
    // 统计
    std::atomic<size_t> allocated_memory_{0};
    std::atomic<size_t> peak_memory_{0};
    std::atomic<size_t> pool_memory_{0};
    
    // 内存对齐
    void* aligned_alloc(size_t size, size_t alignment);
    void aligned_free(void* ptr);
    
    // 内存池算法
    void* allocate_from_pool(size_t size, size_t alignment);
    void return_to_pool(void* ptr, size_t size);
    void expand_pool(size_t min_size);
};

// 执行上下文
class ExecutionContext {
public:
    ExecutionContext(DeviceType device_type, int device_id = 0);
    ~ExecutionContext();
    
    // 设备管理
    DeviceType device_type() const { return device_type_; }
    int device_id() const { return device_id_; }
    DeviceAPI* device_api() const { return device_api_.get(); }
    
    // 内存管理
    MemoryManager* memory_manager() const { return memory_manager_.get(); }
    
    // 流管理
    void* get_default_stream() const { return default_stream_; }
    void* create_stream() { return device_api_->create_stream(); }
    void destroy_stream(void* stream) { device_api_->destroy_stream(stream); }
    
    // 同步
    void synchronize() { device_api_->synchronize(); }
    
    // 错误处理
    void check_error(const std::string& operation);
    
private:
    DeviceType device_type_;
    int device_id_;
    std::unique_ptr<DeviceAPI> device_api_;
    std::unique_ptr<MemoryManager> memory_manager_;
    void* default_stream_ = nullptr;
};

// 运行时环境
class Runtime {
public:
    static Runtime& instance();
    
    // 上下文管理
    ExecutionContext* get_context(DeviceType device_type, int device_id = 0);
    void set_current_context(DeviceType device_type, int device_id = 0);
    ExecutionContext* current_context() const { return current_context_; }
    
    // 模块管理
    void register_module(const std::string& name, ModulePtr module);
    ModulePtr get_module(const std::string& name);
    void unregister_module(const std::string& name);
    
    // 函数调用
    void call_function(const std::string& module_name, const std::string& func_name,
                      const std::vector<Tensor>& args);
    
    // 全局配置
    void set_num_threads(int num_threads);
    int get_num_threads() const { return num_threads_; }
    
    void enable_profiling(bool enable) { profiling_enabled_ = enable; }
    bool is_profiling_enabled() const { return profiling_enabled_; }
    
private:
    std::unordered_map<std::pair<DeviceType, int>, std::unique_ptr<ExecutionContext>,
                      boost::hash<std::pair<DeviceType, int>>> contexts_;
    std::unordered_map<std::string, ModulePtr> modules_;
    ExecutionContext* current_context_ = nullptr;
    
    int num_threads_ = 1;
    bool profiling_enabled_ = false;
    
    Runtime() = default;
};

} // namespace runtime
} // namespace minitvm
```

### CPU运行时实现

```cpp
// include/minitvm/runtime/cpu_runtime.h
#pragma once

#include "runtime_base.h"

namespace minitvm {
namespace runtime {

// CPU设备API实现
class CPUDeviceAPI : public DeviceAPI {
public:
    explicit CPUDeviceAPI(int device_id = 0);
    
    void* allocate(size_t size, size_t alignment = 0) override;
    void deallocate(void* ptr) override;
    void copy(void* dst, const void* src, size_t size) override;
    void copy_async(void* dst, const void* src, size_t size, void* stream = nullptr) override;
    
    void synchronize() override;
    void* create_stream() override;
    void destroy_stream(void* stream) override;
    
    DeviceType device_type() const override { return DeviceType::CPU; }
    int device_id() const override { return device_id_; }
    size_t get_memory_info() const override;
    
private:
    int device_id_;
};

// CPU模块实现
class CPUModule : public Module {
public:
    CPUModule(const std::string& source_code, const std::string& binary_code = "");
    ~CPUModule();
    
    void* get_function(const std::string& name) override;
    void call_function(const std::string& name, const std::vector<Tensor>& args) override;
    
    std::vector<std::string> list_functions() const override;
    std::string get_source() const override { return source_code_; }
    
    size_t get_memory_usage() const override;
    void optimize_memory() override;
    
private:
    std::string source_code_;
    std::string binary_code_;
    void* library_handle_ = nullptr;
    
    std::unordered_map<std::string, void*> function_cache_;
    
    // 动态编译和加载
    void compile_and_load();
    std::string generate_wrapper_code();
    void load_shared_library(const std::string& lib_path);
};

// CPU函数包装器
class CPUFunctionWrapper {
public:
    using FunctionPtr = void(*)(void**);
    
    CPUFunctionWrapper(FunctionPtr func_ptr, const std::vector<std::string>& arg_types);
    
    void call(const std::vector<Tensor>& args);
    
private:
    FunctionPtr func_ptr_;
    std::vector<std::string> arg_types_;
    
    // 参数转换
    void** prepare_arguments(const std::vector<Tensor>& args);
    void cleanup_arguments(void** c_args, size_t num_args);
};

} // namespace runtime
} // namespace minitvm
```

### CUDA运行时实现

```cpp
// include/minitvm/runtime/cuda_runtime.h
#pragma once

#include "runtime_base.h"
#include <cuda_runtime.h>
#include <nvrtc.h>

namespace minitvm {
namespace runtime {

// CUDA设备API实现
class CUDADeviceAPI : public DeviceAPI {
public:
    explicit CUDADeviceAPI(int device_id = 0);
    ~CUDADeviceAPI();
    
    void* allocate(size_t size, size_t alignment = 0) override;
    void deallocate(void* ptr) override;
    void copy(void* dst, const void* src, size_t size) override;
    void copy_async(void* dst, const void* src, size_t size, void* stream = nullptr) override;
    
    void synchronize() override;
    void* create_stream() override;
    void destroy_stream(void* stream) override;
    
    DeviceType device_type() const override { return DeviceType::CUDA; }
    int device_id() const override { return device_id_; }
    size_t get_memory_info() const override;
    
private:
    int device_id_;
    cudaDeviceProp device_prop_;
    
    void check_cuda_error(cudaError_t error, const std::string& operation);
};

// CUDA模块实现
class CUDAModule : public Module {
public:
    CUDAModule(const std::string& source_code, const std::vector<std::string>& compile_options = {});
    ~CUDAModule();
    
    void* get_function(const std::string& name) override;
    void call_function(const std::string& name, const std::vector<Tensor>& args) override;
    
    std::vector<std::string> list_functions() const override;
    std::string get_source() const override { return source_code_; }
    
    size_t get_memory_usage() const override;
    void optimize_memory() override;
    
private:
    std::string source_code_;
    std::vector<std::string> compile_options_;
    
    CUmodule cuda_module_ = nullptr;
    std::unordered_map<std::string, CUfunction> function_cache_;
    
    // NVRTC编译
    void compile_with_nvrtc();
    void load_module_from_ptx(const std::string& ptx);
    
    // CUDA错误检查
    void check_cuda_error(CUresult result, const std::string& operation);
    void check_nvrtc_error(nvrtcResult result, const std::string& operation);
};

// CUDA函数包装器
class CUDAFunctionWrapper {
public:
    CUDAFunctionWrapper(CUfunction func, const std::vector<std::string>& arg_types);
    
    void call(const std::vector<Tensor>& args, dim3 grid_size = dim3(1), 
             dim3 block_size = dim3(256), size_t shared_memory = 0, cudaStream_t stream = 0);
    
    // 自动配置启动参数
    void call_auto_config(const std::vector<Tensor>& args);
    
private:
    CUfunction func_;
    std::vector<std::string> arg_types_;
    
    // 启动参数计算
    std::pair<dim3, dim3> calculate_launch_config(size_t total_elements);
    
    // 参数准备
    void** prepare_kernel_args(const std::vector<Tensor>& args);
    void cleanup_kernel_args(void** args, size_t num_args);
};

} // namespace runtime
} // namespace minitvm
```

---

## Day 22: 执行引擎与JIT

### JIT编译引擎

```cpp
// include/minitvm/jit/jit_engine.h
#pragma once

#include "minitvm/runtime/runtime_base.h"
#include "minitvm/codegen/codegen_base.h"
#include "minitvm/ir/tensor_ir.h"
#include <memory>
#include <future>

namespace minitvm {
namespace jit {

// JIT编译选项
struct JITOptions {
    codegen::Target target = codegen::Target::cpu();
    codegen::CodeGenOptions codegen_options;
    
    bool enable_caching = true;
    std::string cache_directory = ".minitvm_cache";
    
    bool enable_async_compilation = false;
    int compilation_threads = 1;
    
    bool enable_auto_tuning = false;
    int tuning_iterations = 100;
};

// JIT编译缓存
class CompilationCache {
public:
    explicit CompilationCache(const std::string& cache_dir);
    
    // 缓存操作
    bool has_cached_module(const std::string& key) const;
    runtime::ModulePtr get_cached_module(const std::string& key);
    void cache_module(const std::string& key, runtime::ModulePtr module);
    
    // 缓存管理
    void clear_cache();
    void clean_expired_cache(int max_age_days = 7);
    size_t get_cache_size() const;
    
private:
    std::string cache_dir_;
    std::unordered_map<std::string, runtime::ModulePtr> memory_cache_;
    
    std::string get_cache_file_path(const std::string& key) const;
    std::string compute_cache_key(const ir::TensorIRFunction& func, const codegen::Target& target);
};

// JIT编译引擎
class JITEngine {
public:
    explicit JITEngine(const JITOptions& options = {});
    ~JITEngine();
    
    // 编译单个函数
    runtime::ModulePtr compile(const ir::TensorIRFunction& func);
    
    // 异步编译
    std::future<runtime::ModulePtr> compile_async(const ir::TensorIRFunction& func);
    
    // 批量编译
    std::vector<runtime::ModulePtr> compile_batch(const std::vector<ir::TensorIRFunction>& functions);
    
    // 自动调优编译
    runtime::ModulePtr compile_with_auto_tuning(const ir::TensorIRFunction& func,
                                               const std::vector<std::vector<Tensor>>& sample_inputs);
    
    // 配置管理
    const JITOptions& options() const { return options_; }
    void set_options(const JITOptions& options) { options_ = options; }
    
    // 统计信息
    struct Statistics {
        size_t total_compilations = 0;
        size_t cache_hits = 0;
        size_t cache_misses = 0;
        double total_compilation_time = 0.0;
        double average_compilation_time = 0.0;
    };
    
    const Statistics& get_statistics() const { return stats_; }
    void reset_statistics() { stats_ = Statistics{}; }
    
private:
    JITOptions options_;
    std::unique_ptr<CompilationCache> cache_;
    std::unique_ptr<codegen::CodeGenerator> codegen_;
    Statistics stats_;
    
    // 编译实现
    runtime::ModulePtr compile_impl(const ir::TensorIRFunction& func);
    
    // 自动调优
    struct TuningConfig {
        codegen::CodeGenOptions codegen_options;
        double performance_score = 0.0;
    };
    
    TuningConfig auto_tune_function(const ir::TensorIRFunction& func,
                                   const std::vector<std::vector<Tensor>>& sample_inputs);
    
    double evaluate_performance(const runtime::ModulePtr& module, const ir::TensorIRFunction& func,
                               const std::vector<std::vector<Tensor>>& sample_inputs);
};

// 执行引擎 - 整合JIT和运行时
class ExecutionEngine {
public:
    explicit ExecutionEngine(const JITOptions& jit_options = {});
    
    // 函数注册和编译
    void register_function(const std::string& name, const ir::TensorIRFunction& func);
    void compile_function(const std::string& name);
    void compile_all_functions();
    
    // 函数调用
    std::vector<Tensor> call_function(const std::string& name, const std::vector<Tensor>& args);
    
    // 异步执行
    std::future<std::vector<Tensor>> call_function_async(const std::string& name, 
                                                         const std::vector<Tensor>& args);
    
    // 批量执行
    std::vector<std::vector<Tensor>> call_function_batch(const std::string& name,
                                                         const std::vector<std::vector<Tensor>>& batch_args);
    
    // 性能分析
    struct ProfilingResult {
        std::string function_name;
        double execution_time_ms;
        size_t memory_usage_bytes;
        std::unordered_map<std::string, double> detailed_timing;
    };
    
    void enable_profiling(bool enable) { profiling_enabled_ = enable; }
    ProfilingResult get_profiling_result(const std::string& name);
    std::vector<ProfilingResult> get_all_profiling_results();
    
    // 资源管理
    void optimize_memory();
    void clear_cache();
    
private:
    std::unique_ptr<JITEngine> jit_engine_;
    std::unordered_map<std::string, ir::TensorIRFunction> registered_functions_;
    std::unordered_map<std::string, runtime::ModulePtr> compiled_modules_;
    
    bool profiling_enabled_ = false;
    std::unordered_map<std::string, ProfilingResult> profiling_results_;
    
    // 性能分析工具
    class Profiler;
    std::unique_ptr<Profiler> profiler_;
};

// 性能分析器
class ExecutionEngine::Profiler {
public:
    Profiler();
    
    void start_timing(const std::string& name);
    void end_timing(const std::string& name);
    
    void record_memory_usage(const std::string& name, size_t memory_bytes);
    
    ProfilingResult get_result(const std::string& name) const;
    void clear_results();
    
private:
    std::unordered_map<std::string, std::chrono::high_resolution_clock::time_point> start_times_;
    std::unordered_map<std::string, ProfilingResult> results_;
};

} // namespace jit
} // namespace minitvm
```

### 使用示例和测试

```cpp
// tests/integration/test_end_to_end.cpp
#include <gtest/gtest.h>
#include "minitvm/jit/jit_engine.h"
#include "minitvm/core/graph.h"
#include "minitvm/ir/tensor_ir.h"

using namespace minitvm;

class EndToEndTest : public ::testing::Test {
protected:
    void SetUp() override {
        // 配置JIT选项
        jit::JITOptions options;
        options.target = codegen::Target::cpu();
        options.enable_caching = true;
        options.enable_auto_tuning = false;
        
        engine_ = std::make_unique<jit::ExecutionEngine>(options);
        engine_->enable_profiling(true);
    }
    
    // 创建简单的矩阵乘法函数
    ir::TensorIRFunction create_matmul_function() {
        ir::TensorIRFunction func("matmul", {"A", "B"}, 
                                 {{64, 128}, {128, 256}}, 
                                 {DataType::FLOAT32, DataType::FLOAT32});
        
        // 构建TensorIR (简化示例)
        auto A = std::make_shared<ir::Var>("A", DataType::FLOAT32);
        auto B = std::make_shared<ir::Var>("B", DataType::FLOAT32);
        
        // 嵌套循环实现矩阵乘法
        auto i_loop = std::make_shared<ir::For>("i", 
                                               std::make_shared<ir::IntImm>(0),
                                               std::make_shared<ir::IntImm>(64),
                                               ir::For::LoopType::SERIAL, 
                                               nullptr);
        // ... 构建完整的循环嵌套
        
        func.set_body(i_loop);
        return func;
    }
    
    std::unique_ptr<jit::ExecutionEngine> engine_;
};

TEST_F(EndToEndTest, BasicMatMul) {
    // 注册和编译函数
    auto matmul_func = create_matmul_function();
    engine_->register_function("matmul", matmul_func);
    engine_->compile_function("matmul");
    
    // 创建输入数据
    auto A = ones({64, 128}, DataType::FLOAT32);
    auto B = ones({128, 256}, DataType::FLOAT32);
    
    // 执行函数
    auto results = engine_->call_function("matmul", {A, B});
    
    EXPECT_EQ(results.size(), 1);
    EXPECT_EQ(results[0].shape(), Shape({64, 256}));
    
    // 验证结果 (每个元素应该是128.0)
    float* result_data = results[0].data<float>();
    for (int i = 0; i < 64 * 256; ++i) {
        EXPECT_FLOAT_EQ(result_data[i], 128.0f);
    }
    
    // 检查性能分析结果
    auto profiling_result = engine_->get_profiling_result("matmul");
    EXPECT_GT(profiling_result.execution_time_ms, 0.0);
    
    std::cout << "MatMul execution time: " << profiling_result.execution_time_ms << " ms\n";
    std::cout << "Memory usage: " << profiling_result.memory_usage_bytes / 1024.0 / 1024.0 << " MB\n";
}

TEST_F(EndToEndTest, BatchExecution) {
    auto matmul_func = create_matmul_function();
    engine_->register_function("matmul", matmul_func);
    engine_->compile_function("matmul");
    
    // 准备批量输入
    std::vector<std::vector<Tensor>> batch_inputs;
    for (int i = 0; i < 10; ++i) {
        auto A = ones({64, 128}, DataType::FLOAT32);
        auto B = ones({128, 256}, DataType::FLOAT32);
        batch_inputs.push_back({A, B});
    }
    
    // 批量执行
    auto batch_results = engine_->call_function_batch("matmul", batch_inputs);
    
    EXPECT_EQ(batch_results.size(), 10);
    for (const auto& results : batch_results) {
        EXPECT_EQ(results.size(), 1);
        EXPECT_EQ(results[0].shape(), Shape({64, 256}));
    }
}

TEST_F(EndToEndTest, AsyncExecution) {
    auto matmul_func = create_matmul_function();
    engine_->register_function("matmul", matmul_func);
    engine_->compile_function("matmul");
    
    auto A = ones({64, 128}, DataType::FLOAT32);
    auto B = ones({128, 256}, DataType::FLOAT32);
    
    // 异步执行
    auto future_result = engine_->call_function_async("matmul", {A, B});
    
    // 可以做其他工作...
    
    // 获取结果
    auto results = future_result.get();
    
    EXPECT_EQ(results.size(), 1);
    EXPECT_EQ(results[0].shape(), Shape({64, 256}));
}

TEST_F(EndToEndTest, CacheEffectiveness) {
    auto matmul_func = create_matmul_function();
    
    // 第一次编译
    auto start_time = std::chrono::high_resolution_clock::now();
    engine_->register_function("matmul", matmul_func);
    engine_->compile_function("matmul");
    auto first_compile_time = std::chrono::high_resolution_clock::now() - start_time;
    
    // 创建新的引擎实例 (应该从缓存加载)
    jit::JITOptions options;
    options.enable_caching = true;
    auto engine2 = std::make_unique<jit::ExecutionEngine>(options);
    
    start_time = std::chrono::high_resolution_clock::now();
    engine2->register_function("matmul", matmul_func);
    engine2->compile_function("matmul");
    auto second_compile_time = std::chrono::high_resolution_clock::now() - start_time;
    
    // 第二次编译应该更快(从缓存加载)
    EXPECT_LT(second_compile_time.count(), first_compile_time.count());
    
    std::cout << "First compilation: " 
              << std::chrono::duration<double, std::milli>(first_compile_time).count() << " ms\n";
    std::cout << "Second compilation (cached): " 
              << std::chrono::duration<double, std::milli>(second_compile_time).count() << " ms\n";
}
```

---

## 第四阶段总结

### 已完成的功能

1. **多目标代码生成**
   - 统一的代码生成器接口
   - LLVM后端（CPU优化代码）
   - CUDA后端（GPU并行代码）
   - 可扩展到其他目标（OpenCL、Metal等）

2. **运行时系统**
   - 设备抽象层（CPU、CUDA）
   - 内存管理器（内存池、垃圾回收）
   - 执行上下文管理
   - 模块加载和函数调用

3. **JIT编译引擎**
   - 实时编译和缓存
   - 异步编译支持
   - 自动性能调优
   - 执行引擎集成

4. **性能优化特性**
   - 内存池管理
   - 异步执行
   - 批量处理
   - 性能分析和调优

### 学习要点

1. **系统集成**
   - 多组件协同工作
   - 接口设计和抽象
   - 错误处理和资源管理

2. **性能优化**
   - 内存管理策略
   - 并行执行模式
   - 缓存机制设计

3. **工程实践**
   - 模块化设计
   - 测试驱动开发
   - 性能基准测试

### 系统完整性

至此，MiniTVM 深度学习编译器的核心组件已经完成：

1. **前端**：张量系统、操作符、计算图构建
2. **中端**：多级IR、图优化Pass框架
3. **后端**：代码生成、运行时、JIT执行

这构成了一个功能完整的深度学习编译器原型，为深入理解TVM等工业级编译器的设计原理和实现细节提供了坚实的基础。

### 下一步学习建议

1. **扩展功能**：添加更多操作符、优化Pass、目标后端
2. **性能调优**：基准测试、性能分析、算法优化
3. **实际应用**：集成真实模型、对比其他框架
4. **深入TVM**：研究TVM源码、学习高级特性
